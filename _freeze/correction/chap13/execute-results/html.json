{
  "hash": "f02d2d6e05ef57191a3c0919ca5c56c9",
  "result": {
    "markdown": "---\ntitle: \"13 Régularisation de la vraisemblance\"\ntoc: true\n---\n\n\n\n::: {.content-visible when-format=\"html\"}\n::: {.cell}\n\\DeclareMathOperator{\\C}{Cov}\n\\DeclareMathOperator{\\V}{V}\n\\DeclareMathOperator{\\leR}{R^2}\n\n\\newcommand{\\prob}{\\mathbf P}\n\\newcommand{\\argmin}{\\mathop{\\mathrm{argmin}}}\n\\newcommand{\\ind}{\\mathbf 1}\n\\newcommand{\\R}{\\mathbb R}\n\\newcommand{\\E}{\\mathbf E}\n\\newcommand{\\var}{\\mathbf V}\n\\newcommand{\\ps}[2]{\\langle #1,#2\\rangle}\n\\newcommand{\\card}[1]{|{#1}|}\n\\newcommand{\\cov}{\\mathbf{cov}}\n\\newcommand{\\corr}{\\text{corr}}\n\\newcommand{\\AUC}{\\text{AUC}}\n\\newcommand{\\logit}{\\text{logit}}\n:::\n:::\n\n\n::: {#exr-13-1 name=\"Questions de cours\"}\n1.  A, B\n2.  C\n3.  A\n4.  C, D\n:::\n\n::: {#exr-13-2 name=\"Lasso sur des données centrées réduites\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(bestglm)\ndata(SAheart)\nSAheart.X <- model.matrix(chd~.,data=SAheart)[,-1]\nSAheart.Y <- SAheart$chd \nlibrary(glmnet)\nmod.lasso <- glmnet(SAheart.X,SAheart.Y,family=\"binomial\",alpha=1)\n```\n:::\n\n\n1.  \n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    lam.lasso <- mod.lasso$lambda\n    lam <- lam.lasso[50]\n    coef(mod.lasso,s=lam)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    10 x 1 sparse Matrix of class \"dgCMatrix\"\n                             s1\n    (Intercept)    -6.105111733\n    sbp             0.006097928\n    tobacco         0.077763845\n    ldl             0.169776425\n    adiposity       0.012176169\n    famhistPresent  0.902337702\n    typea           0.037527467\n    obesity        -0.050604210\n    alcohol         .          \n    age             0.045476625\n    ```\n    :::\n    :::\n\n\n2.  \n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    mu <- apply(SAheart.X,2,mean)\n    sig <- apply(SAheart.X,2,sd)\n    mu.mat <- matrix(rep(mu,nrow(SAheart.X)),nrow=nrow(SAheart.X),byrow=T)\n    sig.mat <- matrix(rep(sig,nrow(SAheart.X)),nrow=nrow(SAheart.X),byrow=T)\n    SAheart.X.cr <- (SAheart.X-mu.mat)/sig.mat\n    mod.lasso1 <- glmnet(SAheart.X.cr,SAheart.Y,family=\"binomial\",alpha=1)\n    ```\n    :::\n\n\n3.  \n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    lam1 <- mod.lasso1$lambda[50]\n    coef(mod.lasso1,s=lam1)[-1]/sig\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n               sbp        tobacco            ldl      adiposity famhistPresent \n       0.006097928    0.077763845    0.169776425    0.012176169    0.902337702 \n             typea        obesity        alcohol            age \n       0.037527467   -0.050604210    0.000000000    0.045476625 \n    ```\n    :::\n    :::\n\n\n:::\n\n::: {#exr-13-3 name=\"Comparaison de méthodes et courbes ROC\"}\n\n1.  On importe les données et on les sépare en un échantillon d'apprentissage et de test.\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    df <- read.csv(\"../donnees/logit_ridge_lasso.csv\")\n    set.seed(1254)\n    perm <- sample(nrow(df))\n    dapp <- df[perm[1:300],]\n    dtest <- df[perm[301:500],]\n    ```\n    :::\n\n\n2.  On construit les modèles demandés sur les données d'apprentissage uniquement.\n\n\n    ::: {.cell hash='chap13_cache/html/unnamed-chunk-7_df0cbe25b8f7487adc9127d8d25694c6'}\n    \n    ```{.r .cell-code}\n    logit <- glm(Y~.,data=dapp,family=\"binomial\")\n    logit.step <- step(logit,direction=\"backward\",trace=0)\n    ```\n    :::\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    Xapp <- model.matrix(Y~.,data=dapp)[,-1]\n    Xtest <- model.matrix(Y~.,data=dtest)[,-1]\n    Yapp <- dapp$Y\n    Ytest <- dtest$Y\n    ```\n    :::\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    lasso1 <- cv.glmnet(Xapp,Yapp,family=\"binomial\",alpha=1)\n    ridge1 <- cv.glmnet(Xapp,Yapp,family=\"binomial\",alpha=0,lambda=exp(seq(-6,-1,length=100)))\n    lasso2 <- cv.glmnet(Xapp,Yapp,family=\"binomial\",alpha=1,type.measure = \"auc\")\n    ridge2 <- cv.glmnet(Xapp,Yapp,family=\"binomial\",alpha=0,type.measure = \"auc\",lambda=exp(seq(-3,2,length=100)))\n    ```\n    :::\n\n\n\n\n3.  \n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    library(tidyverse)\n    score <- data.frame(logit=predict(logit,newdata=dtest,type=\"response\"),\n                        step=predict(logit.step,newdata=dtest,type=\"response\"),\n                        lasso1=as.vector(predict(lasso1,type=\"response\",newx=Xtest)),\n                        ridge1=as.vector(predict(ridge1,type=\"response\",newx=Xtest)),\n                        lasso2=as.vector(predict(lasso2,type=\"response\",newx=Xtest)),\n                        ridge2=as.vector(predict(ridge2,type=\"response\",newx=Xtest))) %>% \n      mutate(obs=Ytest) %>% gather(key=\"Methode\",value=\"score\",-obs)\n    ```\n    :::\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    library(plotROC)\n    ggplot(score)+aes(m=score,d=obs,color=Methode)+geom_roc()+theme_classic()\n    ```\n    \n    ::: {.cell-output-display}\n    ![](chap13_files/figure-html/unnamed-chunk-11-1.png){width=672}\n    :::\n    :::\n\n\n\n3.  \n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    library(tidymodels)\n    score %>% \n      group_by(Methode) %>% \n      mutate(obs=as.factor(obs)) %>%\n      roc_auc(obs,score,event_level = \"second\") %>%\n      select(Methode,.estimate) %>%\n      arrange(desc(.estimate))\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    # A tibble: 6 × 2\n      Methode .estimate\n      <chr>       <dbl>\n    1 lasso1      0.945\n    2 lasso2      0.944\n    3 ridge2      0.883\n    4 ridge1      0.880\n    5 logit       0.816\n    6 step        0.772\n    ```\n    :::\n    :::\n\n\n:::\n\n::: {#exr-13-4 name=\"Surapprentissage\"}\n1.  \n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    score.app <- data.frame(logit=predict(logit,newdata=dapp,type=\"response\"),\n                        step=predict(logit.step,newdata=dapp,type=\"response\"),\n                        lasso1=as.vector(predict(lasso1,type=\"response\",newx=Xapp)),\n                        ridge1=as.vector(predict(ridge1,type=\"response\",newx=Xapp)),\n                        lasso2=as.vector(predict(lasso2,type=\"response\",newx=Xapp)),\n                        ridge2=as.vector(predict(ridge2,type=\"response\",newx=Xapp))) %>% \n      mutate(obs=Yapp) %>% gather(key=\"Methode\",value=\"score\",-obs) \n    ```\n    :::\n\n\n2.  On prédit 1 si la probabilité que **Y** soit égale à 1 est supérieure ou égale à 0.5 :\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    prev.app <- score.app %>% mutate(prev=round(score))\n    prev.app %>% group_by(Methode) %>% summarise(Err=mean(prev!=obs)) %>% arrange(Err)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    # A tibble: 6 × 2\n      Methode    Err\n      <chr>    <dbl>\n    1 logit   0     \n    2 step    0     \n    3 ridge1  0.0567\n    4 ridge2  0.0767\n    5 lasso1  0.0967\n    6 lasso2  0.11  \n    ```\n    :::\n    :::\n\n\n3.  On fait de même avec l'échantillon test.\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    prev.test <- score %>% mutate(prev=round(score))\n    prev.test %>% group_by(Methode) %>% summarise(Err=mean(prev!=obs)) %>% arrange(Err)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    # A tibble: 6 × 2\n      Methode   Err\n      <chr>   <dbl>\n    1 lasso1  0.12 \n    2 lasso2  0.13 \n    3 ridge1  0.215\n    4 ridge2  0.215\n    5 step    0.23 \n    6 logit   0.24 \n    ```\n    :::\n    :::\n\n\n\n    Sur les données d'apprentissage ce sont les modèles logistiques complets et construits avec **step** qui ont les plus petites erreurs. Ces modèles souffrent de sur-apprentissage : ils ajustent très bien les données d'apprentissage mais ont du mal à bien prédire de nouveaux individus.\n\n\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}