{
  "hash": "da8f8d10e390aec90e8298ead5ca1e1a",
  "result": {
    "markdown": "---\ntitle: \"8 Régularisation des moindre carrés : Ridge, Lasso et elastic net\"\ntoc: true\n---\n\n\n\n::: {.content-visible when-format=\"html\"}\n::: {.cell}\n\\DeclareMathOperator{\\C}{Cov}\n\\DeclareMathOperator{\\V}{V}\n\\DeclareMathOperator{\\leR}{R^2}\n\\DeclareMathOperator{\\trace}{tr}\n\\DeclareMathOperator{\\tr}{tr}\n\n\\newcommand{\\M}{\\mathcal M}\n\\DeclareMathOperator{\\MCO}{MCO}\n\\DeclareMathOperator{\\SCR}{SCR}\n\\DeclareMathOperator{\\SCT}{SCT}\n\\DeclareMathOperator{\\SCE}{SCE}\n\\DeclareMathOperator{\\EQM}{EQM}\n\\DeclareMathOperator{\\diag}{diag}\n\\DeclareMathOperator{\\ro}{R^2_0}\n\\newcommand{\\SC}{\\text{SC}}\n\\DeclareMathOperator{\\vect}{\\mathsf{vect}}\n\\DeclareMathOperator{\\CME}{CME}\n\\DeclareMathOperator{\\CMA}{CMA}\n\\DeclareMathOperator{\\CMB}{CMB}\n\\DeclareMathOperator{\\CMR}{CMR}\n\\DeclareMathOperator{\\Cp}{C_p}\n\\DeclareMathOperator{\\radeux}{R^2_a}\n\n\\newcommand{\\1}{\\mathbf{1}}\n\\newcommand{\\un}{\\mathbf{1}}\n\n\n\\newcommand{\\prob}{\\mathbf P}\n\\newcommand{\\argmin}{\\mathop{\\mathrm{argmin}}}\n\\newcommand{\\ind}{\\mathbf 1}\n\\newcommand{\\R}{\\mathbb R}\n\\newcommand{\\E}{\\mathbf E}\n\\newcommand{\\var}{\\mathbf V}\n\\newcommand{\\ps}[2]{\\langle #1,#2\\rangle}\n\\newcommand{\\card}[1]{|{#1}|}\n\\newcommand{\\cov}{\\mathbf{cov}}\n\\newcommand{\\corr}{\\text{corr}}\n\\newcommand{\\AUC}{\\text{AUC}}\n\\newcommand{\\logit}{\\text{logit}}\n\\newcommand{\\li}[2]{#1_{#2}}\n\\newcommand{\\ssli}[2]{#1_{(#2)}}\n\\newcommand{\\HH}{{\\mathcal{H}}}\n\\newcommand{\\Hz}{{\\mathrm{H_0}}}\n\\newcommand{\\Hu}{{\\mathrm{H_1}}}\n\\newcommand{\\NO}{{\\mathcal{N}}}\n\\newcommand{\\sfrac}[2]{{#1}/{#2}}\n\n\\usepackage{nicematrix}\n\\usepackage{blkarray}\n:::\n:::\n\n\n::: {#exr-8-1 name=\"Questions de cours\"}\nA, B, B, B, A (pour un bon choix de $\\lambda$) et B, A, C et D.\n:::\n\n::: {#exr-8-2 name=\"Projection et régression ridge\"}\n\n:::\n\n::: {#exr-8-3 name=\"Variance des valeurs ajustées avec une régression ridge\"}\n\n:::\n\n::: {#exr-8-4 name=\"Nombre effectif de paramètres de la régression ridge\"}\n1.  Rappelons que pour une valeur $\\kappa$ donnée, le vecteur de coefficients de la régression ridge s'écrit\n$$\n\\hat \\beta_{\\mathrm{ridge}}(\\kappa) = (X'X + \\kappa I)^{-1}X'Y.\n$$\net donc l'ajustement par la régression ridge est\n$$\n\\hat Y_{\\mathrm{ridge}}(\\kappa)=X(X'X + \\kappa I)^{-1}X'Y=H^*(\\kappa)Y\n$$\n\n2.  Soit $U_i$ le vecteur propre de $A$ associé à la valeur propre $d^2_i$. Nous avons donc par définition que\n$$\n\\begin{eqnarray*}\nAU_i&=&d^2_iU_i\\\\\nAU_i+\\lambda U_i&=&d^2_iU_i+\\lambda U_i=(d^2_i+\\lambda) U_i\\\\\n(A+\\lambda I_p)U_i&=&(d^2_i+\\lambda) U_i,\n\\end{eqnarray*}\n$$\nc'est-à-dire que $U_i$ est aussi vecteur propre de $A+\\lambda I_p$ associé à la valeur propre $\\lambda+d^2_i$.\n\n3.  Nous savons que $X=QD P'$ avec $Q$ et $P$ matrices orthogonales et $D=\\diag(d_1,\\dotsc,d_p)$. Puisque $Q$ est orthogonale, nous avons, par définition, $Q'Q=I$. Nous avons donc que $X'X=(QD P')'QD P'=PDQ'QDP'=PD^2P'$. Puisque $P$ est orthogonale $P'P=I_p$ et $P^{-1}=P$.\n$$\n\\begin{eqnarray*}\n\\tr(X(X'X+\\lambda I_p)^{-1}X')&=&\\tr((X'X+\\lambda I_p)^{-1}X'X)\\\\\n&=&\\tr((PD^2P'+\\lambda PP')^{-1}PD^2P')\\\\\n&=&\\tr((P(D+\\lambda I_p )P')^{-1}PD^2P').\n\\end{eqnarray*}\n$$\nNous avons donc\n$$\n\\begin{eqnarray*}\n\\tr(X(X'X+\\lambda I_p)^{-1}X')&=&\\tr( (P')^{-1}(D+\\lambda I_p )^{-1} P^{-1} PD^2P')\\\\\n&=&\\tr( (P')^{-1}(D+\\lambda I_p )^{-1} D^2P')\\\\\n&=&\\tr( (D+\\lambda I_p )^{-1} D^2).\n\\end{eqnarray*}\n$$\nSelon la définition de $H^*(\\kappa)$, nous savons que sa trace vaut donc\n$$\n\\begin{eqnarray*}\n\\tr( (D+\\kappa I_p )^{-1} D^2).\n\\end{eqnarray*}\n$$\nComme $D$ et $I_p$ sont des matrices diagonales, leur somme et produit sont simplement leur somme et produit terme à terme des éléments de la\ndiagonale, et donc cette trace (somme des éléments de la diagonale) vaut\n$$\n\\sum_{i=1}^{p}{\\frac{d_j^2}{d_j^2+\\kappa}}.\n$$\n:::\n\n::: {#exr-8-5 name=\"Estimateurs à rétrecissement - shrinkage\"}\n1.  Soit le modèle de régression\n$$\nY=X\\beta+\\varepsilon.\n$$\nEn le pré-multipliant par $P$, nous avons\n$$\nZ=PY=PX\\beta+P\\varepsilon=DQ\\beta+\\eta=D\\gamma+\\eta.\n$$\nPuisque $\\varepsilon\\sim\\mathcal{N}(0,\\sigma^2 I_n)$ et $P$ fixé, nous avons que $\\eta=P\\varepsilon$ suit une loi normale de moyenne $\\E(\\eta)=P\\E(\\varepsilon)=0$ et de variance $\\V(\\eta)=P\\V(\\varepsilon)P'=\\sigma^2PP'=\\sigma^2I_n$.\n    \n    Par définition, $Z$ vaut $PY$ et nous savons que $Y\\sim\\mathcal{N}(X\\beta,\\sigma^2 I_n)$, donc $Z\\sim\\mathcal{N}(PX\\beta,\\sigma^2 PP')$, c'est-à-dire $Z\\sim\\mathcal{N}(DQ\\beta,\\sigma^2 I_n)$ ou encore $Z\\sim\\mathcal{N}(D\\gamma,\\sigma^2 I_n)$. En utilisant la valeur de $D$ nous avons\n$$    \n\\begin{eqnarray*}\nD\\gamma&=&\n\\begin{pmatrix}\n  \\Delta \\gamma\\\\\n0\n\\end{pmatrix}.\n\\end{eqnarray*}\n$$\nDonc $Z_{1:p}\\sim\\mathcal{N}(\\Delta\\gamma,\\sigma^2I_p)$.\n\n2.  Soit un estimateur de $\\beta$ linéaire en $Y$~: $\\hat \\beta=AY$. Soit l'estimateur de $\\gamma=Q\\beta$ linéaire en $Y$~: $\\hat\\gamma=Q AY$. Pour calculer leur matrice de l'EQM, nous devons calculer leur biais et leur variance. Le biais de $\\hat \\beta$ est\n$$\nB(\\hat \\beta)=\\E(\\hat \\beta)-\\beta=\\E(AY)-\\beta=A\\E(Y)-\\beta=AX\\beta-\\beta.\n$$\nLe biais de $\\hat\\gamma$ s'écrit\n$$\nB(\\hat\\gamma)=\\E(\\hat \\gamma)-\\gamma=\\E(Q\\hat \\beta)-\\gamma=Q\\E(\\hat \\beta)-\\gamma=QAX\\beta-\\gamma.\n$$\nComme $\\gamma=Q\\beta$ et $Q'Q=I_p$ nous avons\n$$\nB(\\hat\\gamma)=QAXQ'\\gamma-\\gamma.\n$$\nLa variance de $\\hat \\beta$ s'écrit\n$$\n\\V(\\hat \\beta)=\\V(AY)=A\\V(Y)A'=\\sigma^2 AA',\n$$\net celle de $\\hat \\gamma$ est\n$$\n\\V(\\hat\\gamma)=\\V(Q\\hat \\beta)=Q\\V(\\hat \\beta)Q'=\\sigma^2 QAA'Q'.\n$$\nNous en déduisons que les matrices des EQM sont respectivement\n$$\n\\begin{eqnarray*}\n\\EQM(\\hat \\beta)&=&(AX\\beta-\\beta)(AX\\beta-\\beta)'+\\sigma^2 AA',\\\\\n\\EQM(\\hat \\gamma)&=&(QAXQ'\\gamma-\\gamma)(QAXQ'\\gamma-\\gamma)' + \\sigma^2 QAA'Q',\n\\end{eqnarray*}\n$$\net enfin les traces de ces matrices s'écrivent\n$$\n\\begin{eqnarray*}\n\\tr(\\EQM(\\hat \\beta))&=&(AX\\beta-\\beta)'(AX\\beta-\\beta)+\\sigma^2\\tr(AA'),\\\\\n\\tr(\\EQM(\\hat \\gamma))&=&(QAXQ'\\gamma-\\gamma)'(QAXQ'\\gamma-\\gamma)+ \\sigma^2\\tr(AA').\\\\\n\\end{eqnarray*}\n$$\nRappelons que $\\gamma=Q\\beta$ et que $Q'Q=I_p$, nous avons donc\n$$\n\\begin{eqnarray*}\n\\tr(\\EQM(\\hat \\gamma))&=&\\gamma'(QAXQ'-I_p)'(QAXQ'-I_p)\\gamma+ \\sigma^2\\tr(AA')\\\\\n&=&\\beta'(QAX - Q)'(QAX - Q)\\beta+ \\sigma^2\\tr(AA')\\\\\n&=&\\beta'(AX-I_p)Q'Q(AX-I_p)\\beta+ \\sigma^2\\tr(AA')\\\\\n&=&\\beta'(AX-I_p)(AX-I_p)\\beta+ \\sigma^2\\tr(AA')=\\tr(\\EQM(\\hat \\beta)).\n\\end{eqnarray*}\n$$\nEn conclusion, que l'on s'intéresse à un estimateur linéaire de $\\beta$ ou à un estimateur linéaire de $\\gamma$, dès que l'on passe de\nl'un à l'autre en multipliant par $Q$ ou $Q'$, matrice orthogonale, la trace de l'EQM est identique, c'est-à-dire que les performances globales des 2 estimateurs sont identiques.\n\n3.  Nous avons le modèle de régression suivant~:\n$$\nZ_{1:p}=\\Delta\\gamma+\\eta_{1:p},\n$$\net donc, par définition de l'estimateur des MC, nous avons\n$$\n\\hat \\gamma_{\\mathrm{MC}}=(\\Delta'\\Delta)^{-1}\\Delta'Z_{1:p}.\n$$\nComme $\\Delta$ est une matrice diagonale, nous avons\n$$\n\\hat \\gamma_{\\mathrm{MC}}=\\Delta^{-2}\\Delta'Z_{1:p}=\\Delta^{-1}Z_{1:p}.\n$$\nCet estimateur est d'expression très simple et il est toujours défini de manière unique, ce qui n'est pas forcément le cas de $\\hat \\beta_{\\mathrm{MC}}$.\n\n\n    Comme $Z_{1:p}\\sim\\mathcal{N}(\\Delta\\gamma,\\sigma^2 I_p)$ nous avons que $\\hat \\gamma_{\\mathrm{MC}}=\\Delta^{-1}Z_{1:p}$ suit une loi normale d'espérance $\\E(\\Delta^{-1}Z_{1:p})=\\Delta^{-1}\\E(Z_{1:p})=\\gamma$ et de variance $\\V(\\hat \\gamma_{\\mathrm{MC}})=\\sigma^2\\Delta^{-2}$. Puisque $\\hat \\gamma_{\\mathrm{MC}}$ est un estimateur des MC, il est sans biais, ce qui est habituel.\n\n4.  L'EQM de $\\hat \\gamma_{\\mathrm{MC}}$, estimateur sans biais, est simplement sa variance. Pour la $i^e$ coordonnée de\n$\\hat \\gamma_{\\mathrm{MC}}$, l'EQM est égal à l'élément $i,i$ de la matrice de variance $\\V(\\hat \\gamma_{\\mathrm{MC}})$, c'est-à-dire\n$\\sigma^2/\\delta_i^2$. La trace de l'EQM est alors simplement la somme, sur toutes les coordonnées $i$, de cet EQM obtenu.\n\n5.  Par définition $\\hat \\gamma(c)=\\diag(c_i)Z_{1:p}$ et nous savons que $Z_{1:p}\\sim\\mathcal{N}(\\Delta\\gamma,\\sigma^2 I_p).$ Nous obtenons que $\\hat \\gamma(c)$ suit une loi normale d'espérance $\\E(\\diag(c_i)Z_{1:p})=\\diag(c_i)\\Delta\\gamma$ et de variance\n$$\n\\V(\\hat \\gamma(c))= \\diag(c_i)\\V(Z_{1:p})\\diag(c_i)'= \\sigma^2\\diag(c_i^2).\n$$\nLa loi de $\\hat \\gamma(c)$ étant une loi normale de matrice de variance diagonale, nous en déduisons que les coordonnées\n  de $\\hat \\gamma(c)$ sont indépendantes entre elles.\n  \n6.  Calculons l'EQM de la $i^e$ coordonnée de $\\hat \\gamma(c)$\n$$\n\\EQM(\\hat \\gamma(c)_i)=\\E(\\hat \\gamma(c)_i -\\gamma)^2=\\E(\\hat \\gamma(c)_i^2)+\n\\E(\\gamma_i^2)-2\\E(\\hat \\gamma(c)_i \\gamma_i).\n$$\nComme $\\gamma_i$ et que $\\E(\\hat \\gamma(c)_i^2)=\\V(\\hat \\gamma(c)_i^2)+\\{\\E(\\hat \\gamma(c)_i^2)\\}^2$, nous avons\n$$\n\\begin{align*}\n\\EQM(\\hat \\gamma(c)_i)&=\\sigma^2 c_i^2+(c_i\\delta_i\\gamma_i)^2+\\gamma_i^2-2\\gamma_i\\E(\\hat \\gamma(c)_i)\\\\\n&=\\sigma^2 c_i^2+(c_i\\delta_i\\gamma_i)^2+\\gamma_i^2-2\\sigma^2 c_i\\delta_i\\gamma_i= \\sigma^2c_i^2+\\gamma_i^2(c_i\\delta_i -1)^2.\n\\end{align*}\n$$\n7.  De manière évidente si $\\gamma_i^2$ diminue, alors l'EQM de $\\hat \\gamma(c)_i$ diminue aussi. Calculons la valeur de l'EQM quand\n$\\gamma_i^2=\\frac{\\sigma^2}{\\delta_i^2}\\frac{(1/\\delta_i)+c_i}{(1/\\delta_i)-c_i}$. Nous avons, grâce à la question précédente,\n$$\n\\begin{eqnarray*}\n\\EQM(\\hat \\gamma(c)_i)&=&\\sigma^2 c_i^2+(c_i\\delta_i -1)^2\\frac{\\sigma^2}{\\delta_i^2}\\frac{(1/\\delta_i)+c_i}{(1/\\delta_i)-c_i}\\\\\n&=&\\sigma^2 c_i^2+\\frac{\\sigma^2}{\\delta_i^2}(1 - c_i\\delta_i)^2\\frac{1+\\delta_ic_i}{1-\\delta_ic_i}\\\\\n&=&\\sigma^2 c_i^2+\\frac{\\sigma^2}{\\delta_i^2}(1 - c_i\\delta_i)(1+\\delta_ic_i)\\\\\n&=&\\sigma^2 c_i^2+\\frac{\\sigma^2}{\\delta_i^2}(1-\\delta_i^2c_i^2)\\\\\n&=&\\sigma^2 c_i^2+\\frac{\\sigma^2}{\\delta_i^2}-\\sigma^2c_i^2=\\frac{\\sigma^2}{\\delta_i^2}\\\\\n&=&\\EQM(\\hat \\gamma_{\\mathrm{MC}}),\n\\end{eqnarray*}\n$$\nd'où la conclusion demandée.\n\n8.  Par définition de $\\hat \\gamma(c)$, nous avons\n$$\n\\begin{eqnarray*}\n\\hat \\gamma(c)&=&\\diag(c_i)Z_{1:p}=\\diag(\\frac{\\delta_i}{\\delta_i^2+\\kappa})Z_{1:p}\\\\\n&=&(\\Delta'\\Delta + \\kappa I_p)^{-1}\\Delta'Z_{1:p},\n\\end{eqnarray*}\n$$\npuisque $\\Delta$ est diagonale. De plus nous avons\n$$\nD =\n\\bigl( \\begin{smallmatrix}\n  \\Delta\\\\\n0\n\\end{smallmatrix}\\bigr),\n$$\nce qui entraîne que $D'D=\\Delta'\\Delta$ et $D'Z=\\Delta' Z_{1:p}$.\nNous obtenons donc\n$$\n\\hat \\gamma(c)=(D'D+\\kappa I_p)^{-1}D'Z.\n$$\nRappelons que $D=PXQ'$ avec $P$ et $Q$ matrices orthogonales, nous avons\nalors\n$$\n\\begin{eqnarray*}\n\\hat \\gamma(c)&=&(QX'P'PXQ' + \\kappa I_p)^{-1} D'Z=(QX'XQ' + \\kappa QQ')^{-1}D'Z\\\\\n&=&(Q(X'X  + \\kappa I_p)Q')^{-1}D'Z=(Q')^{-1}(X'X  + \\kappa I_p)^{-1}(Q)^{-1}D'Z\\\\\n&=&Q(X'X  + \\kappa I_p)^{-1}Q'D'Z.\n\\end{eqnarray*}\n$$\nComme $Z=PY$ et $D=PXQ'$, nous avons\n$$\n\\hat \\gamma(c)=Q(X'X  + \\kappa I_p)^{-1}Q' QX'P' PY=Q(X'X  + \\kappa I_p)^{-1}XY.\n$$\nEnfin, nous savons que $Q\\hat\\gamma=\\hat \\beta$, nous en déduisons que $\\hat\\gamma=Q'\\hat \\beta$ et donc que dans le cas particulier où $c_i=\\frac{\\delta_i}{\\delta_i^2+\\kappa}$ nous obtenons\n$$\n\\hat \\beta=Q\\hat \\gamma(c)=(X'X  + \\kappa I_p)^{-1}XY,\n$$\nc'est-à-dire l'estimateur de la régression ridge.\n:::\n\n::: {#exr-8-6 name=\"Coefficient constant et régression sous contraintes\"}\n\n:::\n\n::: {#exr-8-7 name=\"Unicité pour la régression lasso, Giraud (2014)\"}\n\n:::\n\n::: {#exr-8-8}\n1.  \n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    library(tidyverse)\n    signal <- read_csv(\"../donnees/courbe_lasso.csv\")\n    donnees <- read_csv(\"../donnees/echan_lasso.csv\")\n    ggplot(signal)+aes(x=x,y=y)+geom_line()+\n      geom_point(data=donnees,aes(x=X,y=Y))\n    ```\n    \n    ::: {.cell-output-display}\n    ![](chap8_files/figure-html/unnamed-chunk-2-1.png){width=672}\n    :::\n    :::\n\n\n2.  Nous cherchons à reconstruire le signal à partir de l'échantillon. Bien entendu, vu la forme du signal, un modèle linéaire de la forme \n$$\ny_i=\\beta_0+\\beta_1x_i+\\varepsilon_i\n$$ \nn'est pas approprié. De nombreuses approches en **traitement du signal** proposent d'utiliser une `base` ou `dictionnaire` représentée par une collection de fonctions $\\{\\psi_j(x)\\}_{j=1,\\dots,K}$ et de décomposer le signal dans cette base :\n$$\nm(x)\\approx \\sum_{j=1}^K \\beta_j\\psi_j(x).\n$$\nPour un dictionnaire donné, on peut alors considérer un **modèle linéaire**\n$$\n  y_i=\\sum_{j=1}^K \\beta_j\\psi_j(x_i)+\\varepsilon_i.\n$$ {#eq-mod-lin-signal}\nLe problème est toujours d'estimer les paramètres $\\beta_j$ mais les variables sont maintenant définies par les éléments du dictionnaire. Il existe différents types de dictionnaire, dans cet exercice nous proposons de considérer la base de Fourier définie par\n$$\n\\psi_0(x)=1,\\quad \\psi_{2j-1}(x)=\\cos(2j\\pi x)\\quad\\text{et}\\quad \\psi_{2j}(x)=\\sin(2j\\pi x),\\quad j=1,\\dots,K.\n$$\n\n3.  \n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    mat.dict <- function(K,x){\n    \tres <- matrix(0,nrow=length(x),ncol=2*K) |> as_tibble()\n    \tfor (j in 1:K){\n    \t  res[,2*j-1] <- cos(2*j*pi*x)\n    \t  res[,2*j] <- sin(2*j*pi*x)\n    \t}\n    \treturn(res)\n    }\n    ```\n    :::\n\n\n4.  Il suffit d'ajuster le modèle linéaire où les variables explicatives sont données par le dictionnaire :\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    D25 <- mat.dict(25,donnees$X) |> mutate(Y=donnees$Y)\n    mod.lin <- lm(Y~.,data=D25)\n    S25 <- mat.dict(25,signal$x)\n    prev.MCO <- predict(mod.lin,newdata = S25)\n    signal1 <- signal |> mutate(MCO=prev.MCO) |> rename(signal=y)\n    signal2 <- signal1 |> pivot_longer(-x,names_to=\"meth\",values_to=\"y\")\n    ggplot(signal2)+aes(x=x,y=y)+geom_line(aes(color=meth))+\n      scale_y_continuous(limits = c(-2,2))+geom_point(data=donnees,aes(x=X,y=Y))\n    ```\n    \n    ::: {.cell-output-display}\n    ![](chap8_files/figure-html/unnamed-chunk-4-1.png){width=672}\n    :::\n    :::\n\n    Le signal estimé a tendance à surajuster les données. Cela vient du fait qu'on estime 51 paramètres avec seulement 60 observations.\n\n5.  On regarde tout d'abord le `chemin de régularisation` des estimateurs **lasso**\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    library(glmnet)\n    X.25 <- model.matrix(Y~.,data=D25)[,-1]\n    lasso1 <- glmnet(X.25,D25$Y,alpha=1)\n    plot(lasso1)\n    ```\n    \n    ::: {.cell-output-display}\n    ![](chap8_files/figure-html/unnamed-chunk-5-1.png){width=672}\n    :::\n    :::\n\n    Il semble que quelques coefficients quittent la valeur 0 bien avant les autres. On effectue maintenant la validation croisée pour sélectionner le paramètre $\\lambda$.\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    set.seed(1234)\n    lasso.cv <- cv.glmnet(X.25,D25$Y,alpha=1)\n    plot(lasso.cv)\n    ```\n    \n    ::: {.cell-output-display}\n    ![](chap8_files/figure-html/unnamed-chunk-6-1.png){width=672}\n    :::\n    :::\n\n    On calcule les prévisions et on trace le signal.\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    prev.lasso <- as.vector(predict(lasso.cv,newx=as.matrix(S25)))\n    signal1$lasso <- prev.lasso\n    signal2 <- signal1 |> pivot_longer(-x,names_to=\"meth\",values_to=\"y\")\n    ggplot(signal2)+aes(x=x,y=y)+geom_line(aes(color=meth))+\n      scale_y_continuous(limits = c(-2,2))+geom_point(data=donnees,aes(x=X,y=Y))\n    ```\n    \n    ::: {.cell-output-display}\n    ![](chap8_files/figure-html/unnamed-chunk-7-1.png){width=672}\n    :::\n    :::\n\n    L'algorithme **lasso** a permis de corriger le problème de sur-apprentissage. Les coefficients lasso non nuls sont les suivants\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    v.sel <- which(coef(lasso.cv)!=0)\n    v.sel\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n     [1]  1  2  4  5  6  8 21 28 30 38 40\n    ```\n    :::\n    :::\n\n\n\n\n:::\n",
    "supporting": [
      "chap8_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}