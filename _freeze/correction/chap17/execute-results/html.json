{
  "hash": "5c43683a6fa953003ab6247d9c8de6ac",
  "result": {
    "markdown": "---\ntitle: \"17 Estimateurs à noyau et $k$ plus proches voisins\"\ntoc: true\n---\n\n\n\n::: {.content-visible when-format=\"html\"}\n::: {.cell}\n\\DeclareMathOperator{\\C}{Cov}\n\\DeclareMathOperator{\\V}{V}\n\\DeclareMathOperator{\\leR}{R^2}\n\n\\newcommand{\\prob}{\\mathbf P}\n\\newcommand{\\argmin}{\\mathop{\\mathrm{argmin}}}\n\\newcommand{\\ind}{\\mathbf 1}\n\\newcommand{\\R}{\\mathbb R}\n\\newcommand{\\E}{\\mathbf E}\n\\newcommand{\\var}{\\mathbf V}\n\\newcommand{\\ps}[2]{\\langle #1,#2\\rangle}\n\\newcommand{\\card}[1]{|{#1}|}\n\\newcommand{\\cov}{\\mathbf{cov}}\n\\newcommand{\\corr}{\\text{corr}}\n\\newcommand{\\AUC}{\\text{AUC}}\n\\newcommand{\\logit}{\\text{logit}}\n:::\n:::\n\n\n::: {#exr-17-1 name=\"Questions de cours\"}\n1.  A\n2.  B\n3.  A\n:::\n\n::: {#exr-17-2 name=\"Estimateur de Nadaraya-Watson\"}\nIl suffit de dériver la quantité à minimiser par rapport à $\\beta_1$ et la valeur de $\\beta_1$ qui annule cette dérivée :\n$$-2\\sum_{i=1}^n(y_i-\\beta_1)p_i(x)=0\\quad\\Longleftrightarrow \\quad\\widehat\\beta_1(x)=\\frac{\\sum_{i=1}^ny_ip_i(x)}{\\sum_{i=1}^np_i(x)}.$$\n:::\n\n::: {#exr-17-3 name=\"Polynômes locaux\"}\n\n:::\n\n::: {#exr-17-4 name=\"Estimateur à noyau uniforme dans $\\mathbb R^p$\"}\n1.  En annulant la dérivée par rapport à $a$, on obtient\n$$\\widehat m(x)=\\frac{\\sum_{i=1}^ny_iK\\left(\\frac{\\|x_i-x\\|}{h}\\right)}{\\sum_{i=1}^nK\\left(\\frac{\\|x_i-x\\|}{h}\\right)}.$$\n\n2.  On a\n$$\\V[\\widehat m(x)]= \\frac{\\sigma^2}{\\sum_{i=1}^nK\\left(\\frac{\\|x_i-x\\|}{h}\\right)}$$\net\n$$\\E[\\widehat m(x)]-m(x)=\\frac{\\sum_{i=1}^n(m(x_i)-m(x))K\\left(\\frac{\\|x_i-x\\|}{h}\\right)}{\\sum_{i=1}^nK\\left(\\frac{\\|x_i-x\\|}{h}\\right)}.$$\n\n3.  On a maintenant $|m(x_i)-m(x)|\\leq L\\|x_i-x\\|$. Or \n$$K\\left(\\frac{\\|x_i-x\\|}{h}\\right)$$\nest non nul si et seulement si $\\|x_i-x\\|\\leq h$. Donc pour tout $i=1,\\dots,n$\n$$L\\|x_i-x\\|K\\left(\\frac{\\|x_i-x\\|}{h}\\right)\\leq Lh K\\left(\\frac{\\|x_i-x\\|}{h}\\right).$$\nD'où le résultat.\n\n4.  On a\n$$\\V[\\widehat m(x)]= \\frac{\\sigma^2}{\\sum_{i=1}^nK\\left(\\frac{\\|x_i-x\\|}{h}\\right)}=\\frac{\\sigma^2}{\\sum_{i=1}^n\\ind_{B_h}(x_i-x)}.$$\nOr \n$$\\sum_{i=1}^n\\ind_{B_h}(x_i-x)\\geq C_1n\\textrm{Vol}(B_h)\\geq C_1\\gamma_dnh^d$$\noù $\\gamma_d=\\pi^{d/2}/\\Gamma(d/2+1)$. On a donc\n$$\\V[\\widehat m(x)]\\leq \\frac{\\sigma^2}{C_1\\gamma_dnh^d}=\\frac{C_2\\sigma^2}{nh^d}$$\navec $C_2=1/(C_1\\gamma_d)$.\n\n5.  On déduit\n$$\\E[(\\widehat m(x)-m(x))^2]\\leq L^2h^2+\\frac{C_2\\sigma^2}{nh^d}.$$\n\n6.  Soit $M(h)$ le majorant ci-dessus. On a\n$$M(h)'=2hL^2-\\frac{C_2\\sigma^2d}{n}h^{-d-1}.$$\nLa dérivée s'annule pour \n$$h_{opt}=\\frac{2L^2}{C_2\\sigma^2d}n^{-\\frac{1}{d+2}}.$$\nLorsque $h=h_{opt}$ l'erreur quadratique vérifie\n$$\\E[(\\hat m(x)-m(x))^2]=\\mathrm{O}\\left(n^{-\\frac{2}{d+2}}\\right).$$\nLa vitesse diminue lorsque la dimension $d$ augmente, c'est le **fléau de la dimension**.\n\n:::\n\n::: {#exr-17-5 name=\"Vitesse de la régression univariée en design equi-espacé\"}\n1.  $\\widehat\\beta$ minimise $\\sum_{i=1}^n(Y_i-\\beta x_i)^2$. On a donc\n$$\\widehat\\beta=\\frac{\\sum_{i=1}^nx_iY_i}{\\sum_{i=1}^nx_i^2}.$$\n\n2.  On déduit\n$$\\E[\\widehat\\beta]=\\beta\\quad\\textrm{et}\\quad\\V(\\widehat\\beta)=\\frac{\\sigma^2}{\\sum_{i=1}^nx_i^2}.$$\n\n3.  Comme \n$$\\sum_{i=1}^nx_i^2=\\frac{(n+1)(2n+1)}{6n},$$\non obtient le résultat demandé.\n\n:::\n\n::: {#exr-17-6 name=\"Critère LOO\"}\n1.  On désigne par $\\widehat F_h$ le vecteur $(\\widehat f_h(x_i),i=1,\\dots,n)$, $\\widehat F_k$ le vecteur $(\\widehat f_k(x_i),i=1,\\dots,n)$ et $\\mathbb Y=(y_1,\\dots,y_n)$. On voit facilement que\n$$\\widehat F_h=S_h\\mathbb Y\\quad\\text{et}\\quad \\widehat F_k=S_k\\mathbb Y$$\noù $S_h$ et $S_k$ sont des matrices $n\\times n$ dont le terme général est défini par\n$$S_{ij,h}=\\frac{K((x_i-x_j)/h)}{\\sum_l K((x_i-x_l)/h)}\n\\quad\\text{et}\\quad\nS_{ij,k}=\n\\left\\{\n\\begin{array}{ll}\n  1/k&\\text{ si $x_j$ est parmi les $k$-ppv de $x_i$} \\\\\n  0 & \\text{ sinon}.\n  \\end{array}\n\\right.$$\n\n\n2.  Pour simplifier on note $K_{ij}=K((x_i-x_j)/h)$ On a\n$$\\widehat f_h^i(x_i)=\\frac{\\sum_{j\\neq i}K_{ij}y_j}{\\sum_{j\\neq i}K_{ij}}.$$\nPar conséquent\n$$\\widehat f_h^i(x_i)\\left[\\sum_{j=1}^nK_{ij}-K_{ii}\\right]=\\sum_{j\\neq i}K_{ij}y_j.$$\nOn obtient le résultat demandé en divisant tout $\\sum_{j=1}^nK_{ij}$. Pour l'estimateur de plus proches voisins, on remarque que, si on enlève la $i$ème observation alors l'estimateur des $k$ plus proches voisins de $x_i$ s'obtient à partir ce celui des $k+1$ plus proches voisins avec la $i$ème observation de la façon suivante :\n$$\\widehat f_k^i(x_i)=\\frac{k+1}{k}\\sum_{j\\neq i}S_{ij,k+1}y_j.$$\nOn obtient le résultat demandé on observant que $S_{ii,k+1}=1/(k+1)$ et donc\n$$\\frac{1}{1-S_{ii,k+1}}=\\frac{k+1}{k}.$$\n\n3.  On obtient pour l'estimateur à noyau\n\\begin{align*}\nLOO(\\widehat f_h)= & \\frac{1}{n}\\sum_{i=1}^n\\left(y_i-\\widehat f_h^i(x_i)\\right)^2 \\\\\n= & \\frac{1}{n}\\sum_{i=1}^n\\left(\\frac{y_i-S_{ii,h}y_i-\\sum_{j\\neq i}S_{ij,h}y_{j}}{1-S_{ii,h}}\\right)^2 \\\\\n= & \\frac{1}{n}\\sum_{i=1}^n\\left(\\frac{y_i-\\widehat f_h(x_i)}{1-S_{ii,h}}\\right)^2.\n\\end{align*}\nLe calcul est similaire pour l'estimateur des plus proches voisins.\n:::\n\n::: {#exr-17-7 name=\"Caret et kppv\"}\nOn importe les données et on ne garde que les deux variables demandées :\n\n::: {.cell}\n\n```{.r .cell-code}\nozone <- read.table(\"../donnees/ozone.txt\",header=TRUE,sep=\";\")\ndf <- ozone[,c(\"O3\",\"T12\")]\n```\n:::\n\n\nOn construit la grille de plus proches voisins candidats :\n\n::: {.cell}\n\n```{.r .cell-code}\ngrille <- data.frame(k=1:40)\n```\n:::\n\n\nOn indique à **caret** qu'on veut faire de la validation croisée 10 blocs :\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(caret)\nctrl <- trainControl(method=\"cv\")\n```\n:::\n\n\nOn lance la validation croisée avec la fonction **train** :\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\nsel.k <- train(O3~.,data=df,method=\"knn\",trControl=ctrl,tuneGrid=grille)\nsel.k\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nk-Nearest Neighbors \n\n50 samples\n 1 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 44, 46, 45, 44, 45, 45, ... \nResampling results across tuning parameters:\n\n  k   RMSE      Rsquared   MAE     \n   1  25.39941  0.2777479  21.83800\n   2  20.89184  0.3784027  17.05972\n   3  17.81455  0.5039548  14.88541\n   4  16.49216  0.5636676  14.01838\n   5  16.75867  0.5579746  14.33716\n   6  16.76845  0.5384320  13.92737\n   7  16.11270  0.5792998  13.77474\n   8  15.96855  0.5992164  13.64931\n   9  16.48290  0.5679445  14.13226\n  10  16.89146  0.5572318  14.62217\n  11  17.14669  0.5541549  14.68934\n  12  17.69184  0.5469553  15.27732\n  13  17.41791  0.5668045  15.10278\n  14  17.53775  0.5484378  15.17445\n  15  17.61846  0.5669393  15.35179\n  16  17.89693  0.5550383  15.60799\n  17  18.08559  0.5579067  15.72184\n  18  18.25932  0.5655324  15.89826\n  19  18.84244  0.5402343  16.27675\n  20  19.08747  0.5429639  16.30312\n  21  19.44070  0.5266746  16.57676\n  22  19.66410  0.5196443  16.65315\n  23  19.69539  0.5339187  16.60436\n  24  19.94851  0.5245251  16.73931\n  25  20.04650  0.5072557  16.78093\n  26  20.14871  0.5013264  16.78305\n  27  20.28257  0.5036138  16.89089\n  28  20.45060  0.4988594  16.89900\n  29  20.54389  0.5105147  16.92002\n  30  20.79567  0.5080200  17.12472\n  31  20.96099  0.5014461  17.25705\n  32  21.14524  0.4891512  17.23304\n  33  21.38173  0.4875148  17.38980\n  34  21.59260  0.4810615  17.43883\n  35  21.83266  0.4707654  17.67392\n  36  22.06004  0.4448406  17.74640\n  37  22.34462  0.4236828  17.95419\n  38  22.54304  0.3999950  18.11507\n  39  22.61034  0.4147744  18.15091\n  40  22.68049  0.4296545  18.23871\n\nRMSE was used to select the optimal model using the smallest value.\nThe final value used for the model was k = 8.\n```\n:::\n:::\n\n\nOn sélectionnera \n\n::: {.cell}\n\n```{.r .cell-code}\nsel.k$bestTune\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  k\n8 8\n```\n:::\n:::\n\nplus proches voisins.\n\n:::\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}