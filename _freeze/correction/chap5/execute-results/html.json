{
  "hash": "6478e67d069b3d5b2fccfe104cbcf142",
  "result": {
    "markdown": "---\ntitle: \"5 Inférence dans le modèle gaussien\"\ntoc: true\n---\n\n\n\n::: {.content-visible when-format=\"html\"}\n::: {.cell}\n\\DeclareMathOperator{\\C}{Cov}\n\\DeclareMathOperator{\\V}{V}\n\\DeclareMathOperator{\\leR}{R^2}\n\\DeclareMathOperator{\\trace}{tr}\n\\DeclareMathOperator{\\tr}{tr}\n\n\\newcommand{\\M}{\\mathcal M}\n\\DeclareMathOperator{\\MCO}{MCO}\n\\DeclareMathOperator{\\SCR}{SCR}\n\\DeclareMathOperator{\\SCT}{SCT}\n\\DeclareMathOperator{\\SCE}{SCE}\n\\DeclareMathOperator{\\EQM}{EQM}\n\\DeclareMathOperator{\\diag}{diag}\n\\DeclareMathOperator{\\ro}{R^2_0}\n\n\\newcommand{\\1}{\\mathbf{1}}\n\\newcommand{\\un}{\\mathbf{1}}\n\n\n\\newcommand{\\prob}{\\mathbf P}\n\\newcommand{\\argmin}{\\mathop{\\mathrm{argmin}}}\n\\newcommand{\\ind}{\\mathbf 1}\n\\newcommand{\\R}{\\mathbb R}\n\\newcommand{\\E}{\\mathbf E}\n\\newcommand{\\var}{\\mathbf V}\n\\newcommand{\\ps}[2]{\\langle #1,#2\\rangle}\n\\newcommand{\\card}[1]{|{#1}|}\n\\newcommand{\\cov}{\\mathbf{cov}}\n\\newcommand{\\corr}{\\text{corr}}\n\\newcommand{\\AUC}{\\text{AUC}}\n\\newcommand{\\logit}{\\text{logit}}\n\\newcommand{\\li}[2]{#1_{#2}}\n\\newcommand{\\ssli}[2]{#1_{(#2)}}\n\\newcommand{\\HH}{{\\mathcal{H}}}\n\\newcommand{\\Hz}{{\\mathrm{H_0}}}\n\\newcommand{\\Hu}{{\\mathrm{H_1}}}\n\\newcommand{\\NO}{{\\mathcal{N}}}\n:::\n:::\n\n\n::: {#exr-5-1 name=\"Questions de cours\"}\nA, C, A, B, B.\n:::\n\n::: {#exr-5-2 name=\"Théorème 5.1\"}\nL'IC (i) découle de la propriété (i) de la proposition 5.3.\nLa propriété (ii) donnant un IC pour $\\sigma^2$ découle de la loi de \n$\\hat \\sigma^2$.\nEnfin, la propriété (iii) est une conséquence de la loi obtenue propriété \n(ii) de la proposition 5.3.\n:::\n\n::: {#exr-5-3 name=\"Test et $R^2$\"}\nEn utilisant l'orthogonalité des sous-espaces (figure 5.3 page 99) et le théorème de Pythagore, nous \navons \n\\begin{eqnarray*}\n\\|\\hat Y_0-\\hat Y\\|^2 &=& \\|\\hat \\varepsilon_0\\|^2- \\| \\hat \\varepsilon\\|^2.\n\\end{eqnarray*}\nNous pouvons le démontrer de la manière suivante :\n\\begin{eqnarray*}\n\\|\\hat Y_0-\\hat Y\\|^2 &=& \\|\\hat Y_0-Y+Y-\\hat Y\\|^2\\\\\n&=& \\|\\hat \\varepsilon_0\\|^2+ \\| \\hat \\varepsilon\\|^2\n+2\\langle \\hat Y_0-Y,Y-\\hat Y\\rangle\\\\\n&=& \\|\\hat \\varepsilon_0\\|^2+ \\| \\hat \\varepsilon\\|^2\n-2\\langle Y-\\hat Y_0,Y-\\hat Y\\rangle\\\\\n&=& \\|\\hat \\varepsilon_0\\|^2+ \\| \\hat \\varepsilon\\|^2\n-2\\langle P_{X_0^\\perp}Y,P_{X^\\perp}Y\\rangle\\\\\n&=& \\|\\hat \\varepsilon_0\\|^2+ \\| \\hat \\varepsilon\\|^2\n-2\\langle (P_{X^\\perp}+P_{X})P_{X_0^\\perp}Y,P_{X^\\perp}Y\\rangle.\n\\end{eqnarray*}\nOr $\\Im(X_0) \\subset \\Im(X)$, nous avons \ndonc $P_{X^\\perp}P_{X_0^\\perp}=P_{X^\\perp}$. \nDe plus,\n$\\hat \\varepsilon=P_{X^\\perp}Y$, cela donne\n\\begin{eqnarray*}\n\\langle (P_{X^\\perp}+P_{X})P_{X_0^\\perp}Y,P_{X^\\perp}Y\\rangle &=&\n\\langle P_{X^\\perp}Y,P_{X^\\perp}Y\\rangle \n+\\langle P_{X}P_{X_0^\\perp}Y,P_{X^\\perp}Y\\rangle \\\\\n&=& \\|\\hat \\varepsilon\\|^2 + 0.\n\\end{eqnarray*}\nLe résultat est démontré, revenons à la statistique de test.\nIntroduisons les différentes écritures du $\\leR$\n\\begin{eqnarray*}\n\\leR2 = \\frac{\\|\\hat Y - \\bar Y\\|^2}{\\|Y - \\bar Y\\|^2}=1 -\n\\frac{\\|\\hat \\varepsilon\\|^2}{\\|Y - \\bar Y\\|^2}.\n\\end{eqnarray*}\nLa statistique de test vaut\n\\begin{eqnarray*}\nF&=&\\frac{\\|\\hat \\varepsilon_0\\|^2- \\| \\hat \\varepsilon\\|^2}\n{\\| Y-\\hat Y\\|^2}\\frac{n-p}{p-p_0}\\\\\n&=&\\frac{\\|\\hat \\varepsilon_0\\|^2/\\|Y-\\bar Y\\|^2- \n\\| \\hat \\varepsilon\\|^2/\\|Y-\\bar Y\\|^2}\n{\\| Y-\\hat Y\\|^2/\\|Y-\\bar Y\\|^2}\\frac{n-p}{p-p_0},\n\\end{eqnarray*}\nnous obtenons\n\\begin{eqnarray*}\nF&=&\\frac{\\leR-\\ro}{1-\\leR}\\frac{n-p}{p-p_0},\n\\end{eqnarray*}\nsoit le résultat annoncé. Cette dernière quantité est toujours \npositive car $\\ro \\leq \\leR$ et nous avons là un moyen de\ntester des modèles emboîtés *via* le coefficient de détermination.\n:::\n\n::: {#exr-5-4 name=\"Test et $R^2$ et constante dans le modèle\"}\nà corriger ;).\n:::\n\n::: {#exr-5-5 name=\"Ozone\"}\n1.  Les résultats sont dans l'ordre\n    \\begin{eqnarray*}\n    6.2, 0.8, 6.66, -1.5, -1, 50, 5, 124.\n    \\end{eqnarray*}\n\n2.  La statistique de test de nullité du paramètre se trouve dans la troisième colonne, nous conservons $\\Hz$ pour les\nparamètres associés à Ne9 et Ne12, et la rejetons pour les autres.\n\n3.  La statistique de test de nullité simultanée des paramètres autres que la constante vaut 50. Nous rejetons $\\Hz$.\n    \n4.  Nous connaissons\n    \\begin{align*}\n    \\hat y^{p}_{n+1} &= x'_{n+1}\\hat \\beta,\\\\\n    x'_{n+1}&=(1, 10, 20, 0, 0, 1)\\\\\n    \\hat \\beta&=(62, -4, 5, -1.5, -0.5, 0.8)'\n    \\end{align*}\n    et donc la prévision est $\\hat y^{p}_{n+1} =122.8$. Pour l'intervalle de confiance il nous faut $\\hat\\sigma=16$ mais aussi la matrice $X'X$ (donc toutes les données) ce que nous n'avons pas ici. On ne peut donc faire d'intervalle de confiance.\n\n5.  Nous sommes en présence de modèles emboîtés, nous pouvons appliquer la formule adaptée (voir l'exercice précédent) :\n    \\begin{eqnarray*}\n    F&=& \\frac{\\leR2-\\ro2}{1-\\leR2}\\frac{n-p}{p-p_0}\\\\\n    &=& \\frac{0.66-0.5}{1-0.66}\\frac{124}{2}= 29.\n    \\end{eqnarray*}\n    Nous conservons $\\Hz$, c'est-à-dire le modèle le plus simple.\n:::\n\n::: {#exr-5-6 name=\"Équivalence du test T et du test F\"}\nRécrivons la statistique de test $F$, en se rappelant que $X_0$ est\nla matrice $X$ privée de sa $j^e$ colonne, celle correspondant au\ncoefficient que l'on teste :\n \\begin{eqnarray*}\n F&=&\\frac{\\|X\\hat \\beta-P_{X_0}X\\hat \\beta\\|^2}{\\hat \\sigma^2}\n  =\\frac{\\|X_j\\hat \\beta_j-\\hat \\beta_jP_{X_0}X_j\\|^2}{\\hat \\sigma^2}\n=\\frac{\\hat\\beta_j^2}{\\hat \\sigma^2}X_j'(I-P_{X_0})X_j.\n \\end{eqnarray*}\nRécrivons maintenant le carré de la statistique $T$ en explicitant $\\hat\n \\sigma^2_{\\hat \\beta_j}$ :\n\\begin{eqnarray*}\nT^2&=&\\frac{\\hat \\beta_j^2}{\\hat \\sigma^2 [(X'X)^{-1}]_{jj}},\n\\end{eqnarray*}\noù $[(X'X)^{-1}]_{jj}$ est le $j^e$ élément diagonal de la \nmatrice $(X'X)^{-1}$. Afin de calculer ce terme, nous utilisons \nla formule permettant d'obtenir l'inverse d'une matrice bloc, formule \ndonnée en annexe A.2 page 416.\nPour appliquer facilement cette formule, en changeant\nl'ordre des variables, la matrice $X$ devient $(X_0|X_j)$ et\n$X'X$ s'écrit alors\n\\begin{eqnarray*}\nX'X&=&\\left(\n\\begin{array}{c|c}\nX'_0X_0&X'_0X_j\\\\\\hline\nX'_jX_0&X'_jX_j\n\\end{array}\\right).\n\\end{eqnarray*}\nSon inverse, en utilisant la formule d'inverse de matrice bloc, est \n\\begin{eqnarray*}\n[(X'X)^{-1}]_{jj}&=&\\left(X'_jX_j-X'_jX_0(X'_0X_0)^{-1}X'_0X_j\\right)^{-1}\n=\\left(X_j'(I-P_{X_0})X_j\\right)^{-1}.\n\\end{eqnarray*}\nNous avons donc $T^2=F$. \nAu niveau des lois, l'égalité est aussi\nvalable et nous avons que le carré d'un Student à\n$(n-p)$ ddl est une loi de Fisher à $(1,n-p)$ ddl. Bien entendu, le\nquantile $(1-\\alpha)$ d'une loi de Fisher correspond au quantile\n$1-\\alpha/2$ d'une loi de Student. La loi $\\mathcal{T}$ est symétrique\nautour de 0 et donc, lorsqu'elle est élevée au carré, les valeurs plus\nfaibles que $t_{n-p}(\\alpha/2)$, qui ont une probabilité sous $\\Hz$ de\n$\\alpha/2$ d'apparaître, et celles plus fortes que\n$t_{n-p}(1-\\alpha/2)$, qui ont une probabilité sous $\\Hz$ de\n$\\alpha/2$ d'apparaître, deviennent toutes plus grandes que\n$t^2_{n-p}(1-\\alpha/2)$. La probabilité que ces valeurs dépassent ce\nseuil sous $\\Hz$ est de $\\alpha$ et correspond donc bien par définition à\n$f_{1,n-p}(1-\\alpha)$.\n:::\n\n::: {#exr-5-7 name=\"Équivalence du test F et du test de VM\"}\nNous avons noté la vraisemblance en début du chapitre par \n\\begin{eqnarray*}\n\\mathcal{L}(Y,\\beta,\\sigma^2) &=& \\prod_{i=1}^n f_{Y}(y_i) \n= \\left(\\frac{1}{2\\pi\\sigma^2}\\right)^{n/2}\\exp{\\left[-\\frac{1}{2 \\sigma^2}\n\\sum_{i=1}^n \\left(y_i- \\sum_{j=1}^p\\beta_jx_{ij}\\right)^2\\right]}\\\\\n&=& \\left(\\frac{1}{2\\pi\\sigma^2}\\right)^{n/2}\\exp{\\left[-\\frac{1}{2 \\sigma^2}\n\\|Y-X\\beta\\|^2\\right]}.\n\\end{eqnarray*}\nCette vraisemblance est maximale lorsque $\\hat \\beta$ est l'estimateur \ndes MC et que $\\hat \\sigma^2 = \\|Y-X\\hat \\beta\\|^2/n$. Nous avons alors\n\\begin{eqnarray*}\n\\max_{\\beta,\\sigma^2} \\mathcal{L}(Y,\\beta,\\sigma^2)&=&\n\\left(\\frac{n}{2\\pi\\|Y-X\\hat \\beta\\|^2 }\\right)^{n/2}\\exp{\\left(-\\frac{n}{2}\\right)}\\\\\n&=&\\left(\\frac{n}{2\\pi \\SCR}\\right)^{n/2}\\exp{\\left(-\\frac{n}{2}\\right)}\n=\\mathcal{L}(Y,\\hat \\beta,\\hat \\sigma^2),\n\\end{eqnarray*}\noù $\\SCR=\\|Y-X\\hat \\beta\\|^2$.\\index{Somme des carrés!résiduelle}\n\n\nSous l'hypothèse \n$\\Hz$ nous obtenons de façon évidente le résultat suivant : \n\\begin{eqnarray*}\n\\max_{\\beta,\\sigma^2} \\mathcal{L}_0(Y,\\beta_0,\\sigma^2) \n=\\left(\\frac{n}{2\\pi \\SCR_0}\\right)^{n/2}\\exp{\\left(-\\frac{n}{2}\\right)}\n=\\mathcal{L}_0(Y,\\hat \\beta_0,\\hat \\sigma^2_0),\n\\end{eqnarray*}\noù $\\SCR_0$ correspond à la somme des carrés résiduels sous $\\Hz$, c'est-à-dire \n$\\SCR_0=\\|Y-X_0\\hat \\beta_0\\|^2$.\nOn définit le test du rapport de vraisemblance maximale (VM) par la \nrégion critique \\citep{leh59} suivante :\n\\begin{eqnarray*}\n\\mathcal{D}_\\alpha = \\left\\{\nY \\in \\R^n : \\lambda=\\frac{\\mathcal{L}_0(Y,\\hat \\beta_0,\\hat \\sigma^2)}\n{\\mathcal{L}(Y,\\hat \\beta,\\hat \\sigma^2)} < \\lambda_0 \n\\right\\}.\n\\end{eqnarray*}\nLa statistique du rapport de vraisemblance maximale vaut ici\n\\begin{eqnarray*}\n\\lambda = \\left(\\frac{\\SCR}{\\SCR_0}\\right)^{n/2} = \n\\left(\\frac{\\SCR_0}{\\SCR}\\right)^{-n/2}.\n\\end{eqnarray*}\nLe test du rapport de VM rejette $\\Hz$ lorsque la statistique $\\lambda$ \nest inférieure à une valeur $\\lambda_0$ définie de façon à avoir le \nniveau du test égal à $\\alpha$. Le problème qui reste à étudier est de\nconnaître la distribution (au moins sous $\\Hz$) de $\\lambda$.\nDéfinissons, pour $\\lambda$ positif, la fonction bijective $g$ suivante :\n\\begin{eqnarray*}\ng(\\lambda) = \\lambda^{-2/n}-1.\n\\end{eqnarray*}\nLa fonction $g$ est décroissante (sa dérivée est toujours négative), \ndonc $\\lambda<\\lambda_0$ si et seulement si \n$g(\\lambda)>g(\\lambda_0)$. Cette fonction $g$ va nous permettre de nous \nramener à des statistiques dont la loi est connue. Nous avons alors \n\\begin{eqnarray*}\ng(\\lambda)&>&g(\\lambda_0)\\\\\n\\frac{\\SCR_0-\\SCR}{\\SCR}&>&g(\\lambda_0)\\\\\n\\frac{n-p}{p-p_0}\\frac{\\SCR_0-\\SCR}{\\SCR}&>&f_0\n\\end{eqnarray*}\noù $f_0$ est déterminée par \n\\begin{eqnarray*}\nP_{\\Hz}\\left(\\frac{n-p}{p-p_0}\\frac{\\SCR_0-\\SCR}{\\SCR}>f_0\n\\right)=\\alpha,\n\\end{eqnarray*}\navec la loi de cette statistique qui est une loi $\\mathcal{F}_{p-p_0,n-p}$\n(cf.~section précédente).  Le test du rapport de VM est donc équivalent\nau test qui rejette $\\Hz$ lorsque la statistique\n\\begin{eqnarray*}\nF=\\frac{n-p}{p-p_0}\\frac{\\SCR_0-\\SCR}{\\SCR}\n\\end{eqnarray*}\nest supérieure à $f_0$, où $f_0$ est la valeur du fractile $\\alpha$ \nde la loi de Fisher à $(p-p_0,n-p)$ degrés de liberté.\n:::\n\n::: {#exr-5-8 name=\"Test de Fisher pour une hypothèse linéaire quelconque\"}\nNous pouvons toujours traduire l'hypothèse $\\Hz$ : \n$R\\beta=r$ en terme de sous-espace de $\\M_X$. Lorsque $r=0$, \nnous avons un sous-espace vectoriel de $\\M_X$ et lorsque \n$r\\neq 0$ nous avons un sous-espace affine de $\\M_X$. Dans \nles deux cas, nous noterons ce sous-espace $\\M_0$ et \n$\\M_0 \\subset \\M_X$. Cependant nous ne pourrons \nplus le visualiser facilement comme nous l'avons fait précédemment \navec $\\M_{X_0}$ où nous avions enlevé des colonnes à la \nmatrice $X$. Nous allons décomposer l'espace $\\M_X$ en deux \nsous-espaces orthogonaux\n\\begin{eqnarray*}\n\\M_X = \\M_0 \\stackrel{\\perp}{\\oplus} ( \\M_0^\\perp \\cap \\M_X ).\n\\end{eqnarray*}\n\\noindent\nSous $\\Hz$, l'estimation des moindres carrés donne $\\hat Y_0$\nprojection orthogonale de $Y$ sur $\\M_0$ et nous appliquons la même\ndémarche pour construire la statistique de test. La démonstration \nest donc la même que celle du théorème 5.2. C'est-à-dire \nque nous regardons si $\\hat Y_0$ est proche de $\\hat Y$ et nous avons donc \n\\begin{eqnarray*}\nF&=&\\frac{\\|\\hat Y -\\hat Y_0\\|^2/\\dim(\\M_0^{\\perp}  \\cap \\M_X)}{\\|Y - \\hat Y\\|^2/\n\\dim(\\M_{X^{\\perp}})}\\\\ \n&=&\\frac{n-p}{q} \\frac{\\|Y-\\hat Y_0\\|^2-\\|Y-\\hat Y\\|^2}{ \\|Y-\\hat Y\\|^2}\\\\\n&=& \\frac{n-p}{q}\\frac{\\SCR_0-\\SCR}{\\SCR}\\sim \\mathcal{F}_{q,n-p}.\n\\end{eqnarray*}\nLe problème du test réside dans le calcul de $\\hat Y_0$. Dans la\npartie précédente, il était facile de calculer $\\hat Y_0$ car\nnous avions la forme explicite du projecteur sur $\\M_0$. \nUne première façon de procéder revient à trouver la forme du\nprojecteur sur $\\M_0$. Une autre façon de faire est de récrire le\nproblème de minimisation sous la contrainte $R\\beta=r$. Ces deux\nmanières d'opérer sont présentées en détail dans la correction \nde l'exercice 2.13. Dans tous les cas \nl'estimateur des MC contraints \npar  $R\\beta=r$ est défini par\n\\begin{eqnarray*}\n\\hat \\beta_0&=&\\hat \\beta + (X'X)^{-1}R'[R(X'X)^{-1}R']^{-1}(r-R\\hat \\beta).\n\\end{eqnarray*}\n:::\n\n::: {#exr-5-9 name=\"Généralisation de la régression ridge\"}\n  Soit la fonction à minimiser\n  \\begin{align*}\n    R(\\beta)&=\\|Y-X\\beta\\|^2 -\\sum_{j=1}^{p}\\delta_j(\\beta_j^2) \\\\\n    &= (Y-X\\beta)'(Y-X\\beta) - \\beta' \\Delta \\beta\n  \\end{align*}\n  avec $\\delta_{1}, \\dotsc, \\delta_{p}$ des réels positifs ou nuls.\n\n  Sachant que $\\frac{\\partial \\beta' A\\beta}{\\partial \\beta}=2A\\beta$ (avec $A$ symétrique) et que $\\frac{\\partial X\\beta}{\\partial \\beta}=X'$ nous avons la dérivée partielle suivante\n  \\begin{align*}\n    \\frac{\\partial R}{\\partial \\beta}&=-2X'(Y-X\\beta)  + 2\\Delta \\beta\n  \\end{align*}\n  En annulant cette dérivée nous avons\n  \\begin{align*}\n    -2X'(Y-X\\hat\\beta_{\\mathrm{RG}}) + 2\\Delta \\hat\\beta_{\\mathrm{RG}}&=0\\\\\n         (X'X + \\Delta) \\hat\\beta_{\\mathrm{RG}} &=  X'Y\n  \\end{align*}\n  donc en prémultipliant par $(X'X-\\Delta)^{-1}$ nous obtenons\n  \\begin{align*}\n    \\hat\\beta_{\\mathrm{RG}}=(X'X-\\Delta)^{-1}X'Y.\n  \\end{align*}\n  En régression multiple le nombre de paramètres est $p=\\tr(P_{X})$\n  avec $P_{X}$ la matrice de l'endomorphisme qui permet d'obtenir\n  $\\hat Y$ à partir de $Y$. Dans cette régression ridge, nous avons\n  que\n  \\begin{align*}\n    \\hat Y_{\\mathrm{RG}}&=X\\hat\\beta_{\\mathrm{RG}}=X(X'X-\\Delta)^{-1}X'Y\n  \\end{align*}\n  donc la matrice de l'endomorphisme est ici $X(X'X-\\Delta)^{-1}X'$ et\n  le nombre équivalent de paramètres est $\\tr(X(X'X-\\Delta)^{-1}X')$.\n:::\n\n::: {#exr-5-10 name=\"IC pour la régression ridge\"}\n1.  Loi de $\\hat \\beta$ : $\\NO(\\beta, \\sigma^{2}(X'X)^{-1})$ grâce au modèle et à $\\HH_3$.\n\n2.  Loi de $\\hat \\beta_{\\mathrm{ridge}}(\\tilde\\kappa) $. Comme $\\hat \\beta_{\\mathrm{ridge}}(\\tilde\\kappa)= (X'X-\\tilde\\kappa I)^{-1}X'Y$ avec $A=(X'X-\\tilde\\kappa I)^{-1}X'$ qui est une matrice fixe. Avec $\\HH_{3}$ et le modèle de régression multiple on a que $Y\\sim\\NO(X\\beta, \\sigma^{2}I)$.\n\n    Puisque $Y$ est un vecteur gaussien, il en est de même de $\\hat \\beta_{\\mathrm{ridge}}(\\tilde\\kappa)=AY$. Calculons son espérance\n    \\begin{align*}\n    \\E(\\hat \\beta_{\\mathrm{ridge}}(\\tilde\\kappa))&=\\E(AY)=A\\E(Y)=AX\\beta\\\\\n    &=(X'X-\\tilde\\kappa I)^{-1}X'X\\beta\n    \\end{align*}\n    et sa variance\n    \\begin{align*}\n    \\V(\\hat \\beta_{\\mathrm{ridge}}(\\tilde\\kappa))&=\\V(AY)=A\\V(Y)A'=A\\sigma^{2}I A' = \\sigma^{2} A A'\\\\\n    &=\\sigma^{2}(X'X-\\tilde\\kappa I)^{-1}X'X(X'X-\\tilde\\kappa I)^{-1}.\n    \\end{align*}\n    \n3.  Calculons le produit scalaire de $Y-\\hat Y_{\\mathrm{ridge}}$ et $\\hat Y_{MC}:$\n    \\begin{align*}\n    <Y-\\hat Y_{\\mathrm{ridge}};\\hat Y_{MC}>&=<Y-\\hat Y_{MC} + \\hat Y_{MC} -\\hat Y_{\\mathrm{ridge}};\\hat Y_{MC}>  \\\\\n    & =  <Y-\\hat Y_{MC}; \\hat Y_{MC}> + <   \\hat Y_{MC} -\\hat Y_{\\mathrm{ridge}} ;  \\hat Y_{MC}>\\\\\n    &= 0 + <   \\hat Y_{MC} -\\hat Y_{\\mathrm{ridge}} ;  \\hat Y_{MC}>\n    \\end{align*}\n    Or $\\hat Y_{\\mathrm{ridge}} = X\\beta_{\\mathrm{ridge}}(\\tilde\\kappa)$ donc il appartient au sous espace vectoriel $\\Im(X)$, de même que $\\hat Y_{MC}=P_{X}Y$. Sauf si $\\tilde\\kappa=0$ on a que $\\hat Y_{\\mathrm{ridge}}\\neq \\hat Y_{MC}$ donc $\\hat Y_{MC} -\\hat Y_{\\mathrm{ridge}}$ est un vecteur non nul de $\\Im(X)$ et donc son produit scalaire avec $\\hat Y_{MC}\\in \\Im(X)$ est non nul.\n\n4.  Il faut pouvoir démontrer l'indépendance de $\\hat\\sigma_{\\mathrm{ridge}}$ et $\\hat \\beta_{\\mathrm{ridge}}$. Pour le théorème 5.1, on montre l'indépendance entre$\\hat \\beta$ et $\\hat \\sigma$ en considérant les 2 vecteurs $\\hat\\beta$ et $\\hat \\varepsilon=(Y-\\hat Y)$. Comme nous pouvons écrire $\\hat \\beta=(X'X)^{-1}X'P_XY$, $\\hat \\beta$ est donc une fonction fixe (dépendante uniquement des $X$) de $P_XY$. De plus, $\\hat \\varepsilon=P_{X^\\perp}Y$ est orthogonal à $P_XY$. Ces 2 vecteurs suivent des lois normales et sont donc indépendants. Il en résulte que $\\hat \\beta$ et $Y-\\hat Y$ sont indépendants et de même pour $\\hat \\beta$ et $\\hat \\sigma$.\n\n    Ici, $\\hat\\sigma_{\\mathrm{ridge}}$ est une fonction de $Y-\\hat Y_{\\mathrm{ridge}}$. Le vecteur $\\hat\\beta_{\\mathrm{ridge}}=(X'X+\\tilde\\kappa I_p)^{-1}X'Y=(X'X+\\tilde\\kappa I_p)^{-1}X'P_XY$ est une fonction fixe ($\\tilde \\kappa$ est considéré comme fixé) de $P_XY$. Par contre, $P_XY$ n'est pas orthogonal à $(Y-\\hat Y_{\\mathrm{ridge}})$, comme nous l'avons montré, nous ne pouvons donc montrer l'indépendance de $\\hat\\beta_{\\mathrm{ridge}}$ et $\\hat\\sigma_{\\mathrm{ridge}}$.\n\n    Une autre idée serait d'utiliser $\\hat\\sigma$ mais en général si l'on utilise la régression ridge c'est que l'on se doute que $\\hat Y$ n'est pas un bon estimateur de $X\\beta$ et donc \\textit{a priori} $\\hat\\sigma$ qui est une fonction de $Y-\\hat Y$ risque de ne pas être un bon estimateur de $\\sigma$. L'estimateur $\\hat\\sigma$ peut même être nul, ce qui pratiquement peut arriver quand $p>n$.\n\n5.  En général quand $X$ est fixe pour un bootstrap en régression on estime $\\hat \\beta$ puis on déduit les $\\{\\hat \\epsilon_{i}\\}$. De cet ensemble sont tirés de manière équiprobable avec remise $n$ résidus $\\{\\hat \\epsilon_{i}^{*}\\}$. Ces nouveaux résidus sont additionnés à $X\\beta$ pour faire un nouveau vecteur $Y^{*}$ et avoir un échantillon bootstap $Y^{*}, X$.\n\n    Ici l'estimation de $\\hat \\beta$ sera mauvaise (et c'est pour cela que l'on utilise la régression ridge) et plutôt que d'estimer de mauvais résidus nous allons retirer avec remise parmi les $Y_{i}, X_{i.}$ ce qui est la procédure adaptée au $X$ aléatoire mais ici nous avons peu de choix\n\n\n    **Entrées** : $\\tilde \\kappa$ fixé, $\\alpha$ fixé, $B$ choisi. <br>\n    **Sorties** : IC, au niveau $\\alpha$, coordonnée par coordonnée de $\\beta$.\n    1.  Estimer $\\beta_{\\mathrm{ridge}}(\\tilde \\kappa)$ .\n    2.  En déduire $\\hat \\varepsilon_{\\mathrm{ridge}}=Y-X\\hat \\beta_{\\mathrm{ridge}}$.\n    3.  Pour $k=1$ à $B$\n        -   tirer avec remise $n$ résidus estimés parmi les $n$ coordonnées de $\\hat \\varepsilon_{\\mathrm{ridge}}$ ;\n        -   on note ces résidus (réunis dans 1 vecteur) $\\hat \\varepsilon_{\\mathrm{ridge}}^{(k)}$ ;\n        -   construire 1 échantillon $Y^{(k)}=X\\beta_{\\mathrm{ridge}}(\\tilde \\kappa)+\\hat \\varepsilon_{\\mathrm{ridge}}^{(k)}$ ;\n        -   $\\tilde \\kappa^{(k)} \\leftarrow \\tilde \\kappa$ ;\n        -   estimer le vecteur de paramètre $\\beta_{\\mathrm{ridge}}^{(k)}(\\tilde \\kappa^{(k)})=(X'X+\\tilde\\kappa^{(k)} I_p)^{-1}X'Y^{(k)}$ ;\n    4.  Pour $j=1$ à $p$\n        -   calculer les quantiles empiriques de niveau $\\alpha/2$ et $1-\\alpha/2$    pour la coordonnée $j$, sur tous les vecteurs $\\{\\beta_{\\mathrm{ridge}}^{(k)}(\\tilde \\kappa)\\}$ ;\n        \n        \n6.  L'algorithme est presque le même. Cependant comme $\\tilde \\kappa$ n'est pas fixé, pour estimer $\\beta_{\\mathrm{ridge}}(\\tilde \\kappa)$ il faut déterminer $\\tilde \\kappa$ par une méthode choisie. Ensuite, à chaque estimation de $\\beta_{\\mathrm{ridge}}^{(k)}(\\tilde \\kappa^{(k)})$, il est nécessaire au préalable de déterminer $\\tilde \\kappa^{(k)}$ par la même méthode que celle utilisée pour déterminer $\\tilde \\kappa$.\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}