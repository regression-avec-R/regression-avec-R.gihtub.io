{
  "hash": "e706a9acc4030679ef1f25d43802b009",
  "result": {
    "markdown": "---\ntitle: \"6 Variables qualitatives : ANCOVA et ANOVA\"\ntoc: true\n---\n\n\n\n::: {.content-visible when-format=\"html\"}\n::: {.cell}\n\\DeclareMathOperator{\\C}{Cov}\n\\DeclareMathOperator{\\V}{V}\n\\DeclareMathOperator{\\leR}{R^2}\n\\DeclareMathOperator{\\trace}{tr}\n\\DeclareMathOperator{\\tr}{tr}\n\n\\newcommand{\\M}{\\mathcal M}\n\\DeclareMathOperator{\\MCO}{MCO}\n\\DeclareMathOperator{\\SCR}{SCR}\n\\DeclareMathOperator{\\SCT}{SCT}\n\\DeclareMathOperator{\\SCE}{SCE}\n\\DeclareMathOperator{\\EQM}{EQM}\n\\DeclareMathOperator{\\diag}{diag}\n\\DeclareMathOperator{\\ro}{R^2_0}\n\\newcommand{\\SC}{\\text{SC}}\n\\DeclareMathOperator{\\vect}{\\mathsf{vect}}\n\\DeclareMathOperator{\\CME}{CME}\n\\DeclareMathOperator{\\CMA}{CMA}\n\\DeclareMathOperator{\\CMB}{CMB}\n\\DeclareMathOperator{\\CMR}{CMR}\n\n\\newcommand{\\1}{\\mathbf{1}}\n\\newcommand{\\un}{\\mathbf{1}}\n\n\n\\newcommand{\\prob}{\\mathbf P}\n\\newcommand{\\argmin}{\\mathop{\\mathrm{argmin}}}\n\\newcommand{\\ind}{\\mathbf 1}\n\\newcommand{\\R}{\\mathbb R}\n\\newcommand{\\E}{\\mathbf E}\n\\newcommand{\\var}{\\mathbf V}\n\\newcommand{\\ps}[2]{\\langle #1,#2\\rangle}\n\\newcommand{\\card}[1]{|{#1}|}\n\\newcommand{\\cov}{\\mathbf{cov}}\n\\newcommand{\\corr}{\\text{corr}}\n\\newcommand{\\AUC}{\\text{AUC}}\n\\newcommand{\\logit}{\\text{logit}}\n\\newcommand{\\li}[2]{#1_{#2}}\n\\newcommand{\\ssli}[2]{#1_{(#2)}}\n\\newcommand{\\HH}{{\\mathcal{H}}}\n\\newcommand{\\Hz}{{\\mathrm{H_0}}}\n\\newcommand{\\Hu}{{\\mathrm{H_1}}}\n\\newcommand{\\NO}{{\\mathcal{N}}}\n\n\\usepackage{nicematrix}\n\\usepackage{blkarray}\n:::\n:::\n\n\n::: {#exr-6-1 name=\"Questions de cours\"}\nA, A, C, B.\n:::\n\n::: {#exr-6-2 name=\"Analyse de la covariance\"}\n\n1.  Nous avons pour le modèle complet la matrice suivante :\n$$\nX=\\begin{bmatrix}\n1&\\cdots&0     &x_{11}&\\cdots&0\\\\\n\\vdots&\\cdots&\\vdots&\\vdots&\\vdots&\\vdots\\\\\n1&\\cdots&      0&x_{1n_1}&\\cdots&0\\\\\n\\cdots&\\cdots&\\cdots&\\cdots&\\cdots&\\cdots\\\\\n0&\\cdots&1&0&\\cdots&x_{I1}\\\\\n\\vdots&\\vdots&\\vdots&\\vdots&\\vdots&\\vdots\\\\\n0&\\cdots&1&0&\\cdots&x_{In_I}\n\\end{bmatrix}\n$$\net pour les deux sous-modèles, nous avons les matrices suivantes :\n$$\nX=\\begin{bmatrix}\n1&\\cdots&0     &x_{11}\\\\\n\\vdots &\\cdots&\\vdots&\\vdots\\\\\n1&\\cdots&      0&x_{1n_1}\\\\\n\\cdots&\\cdots&\\cdots&\\cdots\\\\\n0&\\cdots&1&x_{I1}\\\\\n\\vdots&\\vdots&\\vdots&\\vdots\\\\\n0&\\cdots&1&x_{In_I}\n\\end{bmatrix}\n\\quad\nX=\\begin{bmatrix}\n1&x_{11}&\\cdots&0\\\\\n\\vdots &\\vdots&\\vdots&\\vdots\\\\\n1&x_{1n_1}&\\cdots&0\\\\\n\\cdots&\\cdots&\\cdots&\\cdots\\\\\n1&0&\\cdots&x_{I1}\\\\\n\\vdots&\\vdots&\\vdots&\\vdots\\\\\n1&0&\\cdots&x_{In_I}\n\\end{bmatrix}\n$$\n\n2.  Dans le modèle complet, nous obtenons par le calcul\n$$\nX'X = \\begin{bmatrix}\nn_1&0&\\cdots&\\sum x_{i1}&0&\\cdots\\\\\n&\\ddots&&&\\ddots&\\\\\n0&\\cdots&n_I&0&\\cdots&\\sum x_{iI}\\\\\n\\sum x_{i1}&0&\\cdots&\\sum x^2_{i1}&0&\\cdots\\\\\n&\\ddots&&&\\ddots&\\\\\n0&\\cdots&\\sum x_{iI}&0&\\cdots&\\sum x^2_{iI}\\\\\n\\end{bmatrix}\n  \\quad\nX'Y = \\begin{bmatrix}\n\\sum y_{i1}\\\\\n\\vdots\\\\\n\\sum y_{iI}\\\\\n\\sum x_{i1}y_{i1}\\\\\n\\vdots\\\\\n\\sum x_{iI}y_{iI}\\\\\n\\end{bmatrix}\n$$\nUne inversion par bloc de $X'X$ et un calcul matriciel donnent le résultat indiqué.\n\n3.  Une autre façon de voir le problème est de partir du\nproblème de minimisation\n\\begin{eqnarray*}\n&&\\min \\sum_{i=1}^I\\sum_{j=1}^{n_i}\\left(y_{ij}-\\alpha_{j}-\\beta_{j}x_{ij}\\right)^2\\\\\n&=& \\min \\sum_{j=1}^{n_i}\\left(y_{j1}-\\alpha_1-\\beta_{1}x_{j1}\\right)^2+\\cdots\n+\\sum_{j=1}^{n_I}\\left(y_{jI}-\\alpha_I-\\beta_{I}x_{JI}\\right)^2.\n\\end{eqnarray*}\nCela revient donc à calculer les estimateurs des MC pour chaque modalité de la variable\n    qualitative. Attention tout de même, des régressions pour chaque modalité donnent bien les mêmes coefficients $\\alpha_{i}, \\beta_{i}$ mais les écarts-types estimés seront différents: un par modalité dans le cas des régressions pour chaque modalité, un seul écart-type estimé dans le cas de l'ANCOVA.\n\n:::\n\n::: {#exr-6-3 name=\"Estimateurs des MC et ANOVA à 1 facteur\"}\nLa preuve de cette proposition est relativement longue et peu \ndifficile. Nous avons toujours $Y$ un vecteur de $\\R^n$ à expliquer.\nNous projetons $Y$ sur le sous-espace engendré par les colonnes \nde $A_c$, noté $\\M_{A_c}$, de dimension I, et obtenons \nun unique $\\hat Y$. Cependant, en fonction des contraintes utilisées, \nle repère de $\\M_{A_c}$ va changer. \n\nLe cas le plus facile se retrouve lorsque $\\mu=0$. Nous avons alors \n\\begin{eqnarray*}\n(A_c'A_c) = \n\\begin{bmatrix}\nn_1&0&\\cdots&0 \\\\\n0&n_2&0&\\cdots \\\\\n\\vdots&\\vdots&\\vdots&\\vdots\\\\\n0& \\cdots & 0 & n_I\n\\end{bmatrix}\n\\quad \n(A_c'Y)=\\begin{bmatrix}\n\\sum_{j=1}^{n_1} y_{1j}\\\\\n\\sum_{j=1}^{n_2} y_{2j}\\\\\n\\vdots\\\\\n\\sum_{j=1}^{n_I} y_{Ij}\n\\end{bmatrix}\n\\end{eqnarray*}\nd'où le résultat.\nLa variance de $\\hat \\alpha$ vaut $\\sigma^2 (A_c'A_c)^{-1}$ et\ncette matrice est bien diagonale.\n\nPour les autres contraintes, nous utilisons le vecteur $\\vec{e}_{ij}$ \nde $\\R^n$ dont toutes les coordonnées sont nulles sauf celle \nrepérée par le couple $(i,j)$ qui vaut 1 pour repérer un individu.\nNous notons $\\vec{e}_{i}$ le vecteur de $\\R^n$ dont toutes les coordonnées \nsont nulles sauf celles repérées par les indices $i,j$ pour $j=1,\\cdots,n_{i}$ \nqui valent 1. En fait, ce vecteur repère donc les individus qui admettent \nla modalité $i$. La somme des $\\vec{e}_{i}$ vaut le vecteur $\\1$.\nLes vecteurs colonnes de la matrice $A_c$ valent donc \n$\\vec{e}_{1},\\cdots,\\vec{e}_{I}$.\n\n\nConsidérons le modèle \n\\begin{eqnarray*}\nY=\\mu \\1 + \\alpha_1 \\vec{e_1}+ \\alpha_2 \\vec{e_2} + \n\\cdots + \\alpha_I\\vec{e_I} + \\varepsilon.\n\\end{eqnarray*}\nVoyons comment nous pouvons récrire ce modèle lorsque les \ncontraintes sont satisfaites.\n\n1.  $\\alpha_1=0$, le modèle devient alors \n\\begin{eqnarray*}\nY &=&\\mu \\1 + 0 \\vec{e_1} + \\alpha_2 \\vec{e_2} + \\cdots + \n\\alpha_I\\vec{e_I} + \\varepsilon\\\\\n&=&\\mu \\1 + \\alpha_2 \\vec{e_2} + \\cdots + \\alpha_I\\vec{e_I} + \\varepsilon\\\\\n&=& [\\1, \\vec{e_2}, \\cdots, \\vec{e_I}] \\beta + \\varepsilon\\\\\n&=& X_{[\\alpha_1=0]} \\beta_{[\\alpha_1=0]} + \\varepsilon.\n\\end{eqnarray*}\n\n2.  $\\sum n_i \\alpha_i = 0$ cela veut dire que $\\alpha_I= - \\sum_{j=1}^{I-1} n_j\\alpha_j/n_I$, le modèle devient\n    $$\n    \\begin{eqnarray*}\n    Y &=&\\mu \\1+ \\alpha_1 \\vec{e_1} + \\cdots +\\alpha_{I-1} \\vec{e_{I-1}}   \n    - \\sum_{j=1}^{I-1}  \\frac{n_j\\alpha_j}{n_I}\\vec{e_I} + \\varepsilon\\\\\n    &=& \\mu \\1 + \\alpha_1(\\vec{e}_{1}-\\frac{n_1}{n_I}\\vec{e}_{I}) + \\cdots \n    + \\alpha_{I-1} (\\vec{e}_{I-1}-\\frac{n_{I-1}}{n_I}\\vec{e}_{I})+\\varepsilon\\\\\n    &=&\\mu \\1 + \\alpha_1\\vec{v}_{1}+ \\cdots + \\alpha_{I-1} \\vec{v}_{I-1} \n    + \\varepsilon \\quad \\hbox{où} \\quad \\vec{v}_{i}= (\\vec{e}_{i}-\\frac{n_i}{n_I}\\vec{e}_{I})\\\\\n    &=& X_{[\\sum  n_i \\alpha_i=0]} \\beta_{[\\sum  n_i\\alpha_i=0]} + \\varepsilon.\n    \\end{eqnarray*}\n    $$\n\n3.  $\\sum  \\alpha_i = 0$ cela veut dire que $\\alpha_I= - \\sum_{j=1}^{I-1} \\alpha_j$, le modèle devient\n    $$\n    \\begin{eqnarray*}\n    Y &=&\\mu \\1 + \\alpha_1 \\vec{e_1} + \\cdots + \\alpha_{I-1} \\vec{e_{I-1}} -\n    \\sum_{j=1}^{I-1}  \\alpha_j \\vec{e_I} + \\varepsilon\\\\\n    &=& \\mu \\1 + \\alpha_1(\\vec{e}_{1}-\\vec{e}_{I}) + \\cdots \n    + \\alpha_{I-1} (\\vec{e}_{I-1}-\\vec{e}_{I})+\\varepsilon\\\\\n    &=&\\mu \\1 + \\alpha_1\\vec{u}_{1}+ \\cdots + \\alpha_{I-1} \\vec{u}_{I-1}\n    + \\varepsilon \\quad \\hbox{où} \\quad \\vec{u}_{i}= (\\vec{e}_{i}-\\vec{e}_{I})\\\\\n    &=& X_{[\\sum  \\alpha_i=0]} \\beta_{[\\sum  \\alpha_i=0]} + \\varepsilon.\n    \\end{eqnarray*}\n    $$\n  \nDans tous les cas, la matrice $X$ est de taille $n \\times I$, et de rang $I$. La matrice $X'X$ est donc inversible. Nous pouvons calculer l'estimateur $\\hat \\beta$ des MC de $\\beta$ par la formule $\\hat \\beta = (X'X)^{-1}X'Y$ et obtenir les valeurs des estimateurs. Cependant ce calcul n'est pas toujours simple et il est plus facile de démontrer les résultats *via* les projections.\n\nLes différentes matrices $X$ et la matrice $A$ engendrent le même \nsous-espace, donc la projection de $Y$, notée $\\hat Y$ dans ce sous-espace, \nest toujours la même. La proposition 6.2 indique que\n\\begin{eqnarray*}\n\\hat Y = \\bar{y_1} \\vec{e_1} + \\cdots + \\bar{y_I} \\vec{e_I}. \n\\end{eqnarray*}\nAvec les différentes contraintes, nous avons les 3 cas suivants :\n\n1.  $\\alpha_1=0$, la projection s'écrit\n    $$\n    \\begin{eqnarray*}\n    \\hat Y &=&\\hat \\mu \\1 + \\hat \\alpha_2 \\vec{e_2} + \\cdots + \n    \\hat \\alpha_I\\vec{e_I}.\n    \\end{eqnarray*}\n    $$\n\n2.  $\\sum n_i \\alpha_i = 0$, la projection s'écrit\n    $$\n    \\begin{eqnarray*}\n    \\hat Y &=&\\hat \\mu \\1 + \\hat \\alpha_1 \\vec{e_1} + \\cdots + \\hat \\alpha_{I-1} \n    \\vec{e_{I-1}}   - \\sum_{j=1}^{I-1}  \\frac{n_j \\hat \\alpha_j}{n_I}\\vec{e_I}.\n    \\end{eqnarray*}\n    $$\n\n3.  $\\sum  \\alpha_i = 0$, la projection s'écrit\n    $$\n    \\begin{eqnarray*}\n    \\hat Y &=& \\hat \\mu \\1 + \\hat \\alpha_1 \\vec{e_1} + \\cdots + \n    \\hat \\alpha_{I-1} \\vec{e_{I-1}} - \\sum_{j=1}^{I-1} \\hat  \\alpha_j \\vec{e_I}.\n    \\end{eqnarray*}\n    $$\n\nIl suffit maintenant d'écrire que la projection est identique dans chaque cas et de remarquer que le vecteur $\\1$ est la somme des vecteurs $\\vec{e_i}$ pour $i$ variant de 1~à~$I$. Cela donne\n\n1.  $\\alpha_1=0$\n    $$\n    \\begin{eqnarray*}\n    &&\\bar{y_1} \\vec{e_1} + \\cdots + \\bar{y_I} \\vec{e_I} \\\\\n    &=&\\hat \\mu\\1+\\hat\\alpha_2\\vec{e_2}+\\cdots+\\hat \\alpha_I\\vec{e_I}\\\\\n    &=& \\hat \\mu\\vec{e_1}+(\\hat \\mu+\\hat \\alpha_2)\\vec{e_2}\n    \\cdots (\\hat \\mu+\\hat \\alpha_I)\\vec{e_I}.\n    \\end{eqnarray*}\n    $$\n\n2.  $\\sum n_i \\alpha_i = 0$\n    $$\n    \\begin{eqnarray*}\n    &&\\bar{y_1} \\vec{e_1} + \\cdots + \\bar{y_I} \\vec{e_I} \\\\\n    &=& \\hat \\mu \\1 + \\hat \\alpha_1 \\vec{e_1} + \\cdots + \\hat \\alpha_{I-1} \n    \\vec{e_{I-1}}   - \\sum_{j=1}^{I-1}  \\frac{n_j \\hat \\alpha_j}{n_I}\\vec{e_I}\\\\\n    &=& (\\hat \\mu + \\hat \\alpha_1) \\vec{e_1} + \\cdots +\n    (\\hat \\mu  + \\hat \\alpha_{I-1}) \\vec{e_{I-1}} +\n    (\\hat \\mu  - \\sum_{i=1}^{I-1} \\frac{n_i}{n_I}\\hat \\alpha_i) \\vec{e_I}.\n    \\end{eqnarray*}\n    $$\n\n3.  $\\sum \\alpha_i = 0$\n    $$\n    \\begin{eqnarray*}\n    &&\\bar{y_1} \\vec{e_1} + \\cdots + \\bar{y_I} \\vec{e_I} \\\\\n    &=&\\hat \\mu \\1 + \\hat \\alpha_1 \\vec{e_1} + \\cdots + \n    \\hat \\alpha_{I-1} \\vec{e_{I-1}} - \\sum_{j=1}^{I-1} \\hat  \\alpha_j \\vec{e_I}\\\\\n    &=& (\\hat \\mu + \\hat \\alpha_1) \\vec{e_1} + \\cdots +\n    (\\hat \\mu  + \\hat \\alpha_{I-1})\\vec{e_{I-1}} +\n    (\\hat \\mu - \\sum_{i=1}^{I-1} \\hat \\alpha_i) \\vec{e_I}.\n    \\end{eqnarray*}\n    $$\n    \nEn identifiant les différents termes, nous obtenons le résultat annoncé.\n\n:::\n\n::: {#exr-6-4 name=\"Estimateurs des MC et ANOVA à deux facteurs\"}\nNous notons $\\vec{e}_{ijk}$ le vecteur de $\\R^n$ dont toutes les \ncoordonnées sont nulles sauf celle indicée par $ijk$ qui vaut 1.\nSous les contraintes de type analyse par cellule, le modèle \ndevient \n\\begin{eqnarray*}\ny_{ijk} &=& \\gamma_{ij} + \\varepsilon_{ijk},\n\\end{eqnarray*}\net donc matriciellement \n\\begin{eqnarray*}\nY= X \\beta +\\varepsilon \\quad \\quad X=(\\vec{e_{11}},\\vec{e_{12}},\\ldots,\\vec{e_{IJ}}),\n\\end{eqnarray*}\noù le vecteur $\\vec{e}_{ij}= \\sum_{k} \\vec{e}_{ijk}$. Les vecteurs \ncolonnes de la matrice $X$ sont orthogonaux entre eux. Le \ncalcul matriciel $(X'X)^{-1}X'Y$ donne alors le résultat annoncé. \n:::\n\n::: {#exr-6-5 name=\"Estimateurs des MC et ANOVA à deux facteurs, suite\"}\nNous notons $\\vec{e}_{ijk}$ le vecteur de $\\R^n$ dont toutes les \ncoordonnées sont nulles sauf celle indicée par $ijk$ qui vaut 1.\nNous définissons ensuite les vecteurs suivants~: \n\\begin{eqnarray*}\n\\vec{e}_{ij} = \\sum_{k} \\vec{e}_{ijk} \\quad\n\\vec{e}_{i.} = \\sum_{j} \\vec{e}_{ij}  \\quad \n\\vec{e}_{.j} = \\sum_{i} \\vec{e}_{ij}  \\quad\n\\vec{e} = \\sum_{i,j,k} \\vec{e}_{ijk}.\n\\end{eqnarray*}\nAfin d'effectuer cet exercice, nous définissons les sous-espaces suivants~:\n\\begin{eqnarray*}\nE_1&\\!\\!:=\\!\\!&\\{m \\vec{e},\\ m \\hbox{ quelconque} \\}\\\\\nE_2&\\!\\!:=\\!\\!&\\{\\sum_i a_i \\vec{e}_{i.},\\ \\sum_i a_i=0\\}\\\\\nE_3&\\!\\!:=\\!\\!&\\{\\sum_j b_j \\vec{e}_{.j},\\ \\sum_j b_j=0\\}\\\\\nE_4&\\!\\!:=\\!\\!&\\{\\sum_{ij} c_{ij} \\vec{e}_{ij},\n\\ \\forall j \\sum_{i} c_{ij}=0 \\hbox{ et } \\forall i \\sum_{j} c_{ij}=0\\}.\n\\end{eqnarray*}\nCes espaces $E_1$, $E_2$, $E_3$ et $E_4$ sont de dimension respective \n1, $I-1$, $J-1$ et $(I-1)(J-1)$. \nLorsque le plan est équilibré, tous \nces sous-espaces sont orthogonaux. Nous avons la décomposition suivante~:\n\\begin{eqnarray*}\nE = E_1 \\stackrel{\\perp}{\\oplus} E_2 \\stackrel{\\perp}{\\oplus} E_3 \n\\stackrel{\\perp}{\\oplus} E_4.\n\\end{eqnarray*}\n\nLa projection sur $E$ peut se décomposer en une partie sur $E_1,\\cdots,E_4$ \net l'estimateur des MC est obtenu par projection de $Y$ sur $E$. Notons \n$P_{E^\\perp}$, $P_{E},$ $P_{E_1},$ $P_{E_2},$ $P_{E_3}$ et $P_{E_4}$ les \nprojections orthogonales sur les sous-espaces  $E^\\perp$, $E$, $E_1$, \n$E_2$, $E_3$ et $E_4$, nous avons alors \n\\begin{eqnarray*}\nP_{E_1} Y &=& \\bar{y} \\1 ,\n\\end{eqnarray*}\npuis, en remarquant que projeter sur le sous-espace engendré par les \ncolonnes de $A=[\\vec{e}_{1.},\\cdots,\\vec{e}_{I.}]$ est identique \nà la projection sur $E_1 \\stackrel{\\perp}{\\oplus} E_2$, nous avons \nalors avec $\\1 = \\sum_i \\vec{e}_{i.}$,\n\\begin{eqnarray*}\nP_{A} Y = \\sum_i \\bar{y}_{i.} \\vec{e}_{i.} \\quad \\hbox{donc}\n\\quad P_{E_2} Y =\\sum_i (\\bar{y}_{i.} - \\bar{y})\\ \\vec{e}_{i.}.\\\\\n\\end{eqnarray*}\nDe la même façon, nous obtenons\n\\begin{eqnarray*}\nP_{E_3}(Y)&=&\\sum_j (\\bar{y}_{.j} - \\bar{y})\\ \\vec{e}_{.j},\\\\\nP_{E_4}(Y)&=&\\sum_{ij} (\\bar{y}_{ij}-\\bar{y}_{i.}-\\bar{y}_{.j}+\\bar{y})\\  \\vec{e}_{i.},\\\\\nP_{E^\\perp}(Y)  &=&\\sum_{ijk} (y_{ijk}-\\bar{y}_{ij})\\ \\vec{e}_{ijk}, \n\\end{eqnarray*}\noù $\\vec{e}_{ijk}$ est le vecteur dont toutes les coordonnées sont nulles \nsauf celle indicée par ${ijk}$ qui vaut 1. En identifiant terme à terme,\nnous retrouvons le résultat énoncé.\n\n:::\n\n::: {#exr-6-6 name=\"Tableau d'ANOVA à 2 facteurs équilibrés\"}\nLorsque le plan est équilibré, nous avons démontré, \nque les sous-espaces $E_1$, $E_2$, $E_3$ et $E_4$\nsont orthogonaux (cf. exercice précédent) deux à deux. Nous avons alors\n\\begin{eqnarray*}\nY &=& P_{E_1}(Y) + P_{E_2}(Y) + P_{E_3}(Y) + P_{E_4}(Y) + P_{E^\\perp}(Y).\n\\end{eqnarray*}\nNous obtenons ensuite par le théorème de Pythagore\n\\begin{eqnarray*}\n\\begin{array}{ccccccccccc}\n\\|Y - \\bar Y \\|^2 &=&  \\| P_{E_2}(Y)\\|^2 &+& \n\\|P_{E_3}(Y)\\|^2 &+& \\|P_{E_4}(Y)\\|^2 &+& \\|P_{E^\\perp}(Y)\\|^2\\\\\n\\SCT &=& \\SC_A &+& \\SC_B &+& \\SC_{AB} &+& \\SCR,\n\\end{array}\n\\end{eqnarray*}\noù \n\\begin{eqnarray*}\n\\SCT &=& \\sum_i \\sum_j \\sum k (y_{ijk} - \\bar y)^2\\\\\n\\SC_A &=& Jr \\sum_i (y_{i..}-\\bar y)^2\\\\\n\\SC_B &=& Ir \\sum_j (y_{.j.} - \\bar y)^2\\\\\n\\SC_{AB} &=& r \\sum_i \\sum_j (y_{ij.} - y_{i..} - y_{.j.} +\\bar y)^2\\\\\n\\SCR &=& \\sum_i \\sum_j \\sum_k (y_{ijk}- \\bar{y_{ij}})^2.\n\\end{eqnarray*}\n\nAfin  de bien visualiser les vecteurs voici un exemple avec $I=2$, $J=3$ et $r=2$ en remplaçant les $0$ par $.$ :\n\n$$\n\\begin{array}{*{12}c}\n      \\vec{e}&\\vec{e}_{1.}&\\vec{e}_{2.}&\n      \\vec{e}_{.1}&\\vec{e}_{.2}&\\vec{e}_{.3}&\n      \\vec{e_{11}}&\\vec{e}_{12}&\\vec{e}_{13}&\n      \\vec{e}_{21}&\\vec{e}_{22}&\\vec{e}_{23}&\\\\\n1&1&.&1&.&.&1&.&.&.&.&.& \\\\\n1&1&.&1&.&.&1&.&.&.&.&.& \\\\\n1&1&.&.&1&.&.&1&.&.&.&.& \\\\\n1&1&.&.&1&.&.&1&.&.&.&.& \\\\\n1&1&.&.&.&1&.&.&1&.&.&.& \\\\\n1&1&.&.&.&1&.&.&1&.&.&.& \\\\\n%%\n1&.&1&1&.&.&.&.&.&1&.&.& \\\\\n1&.&1&1&.&.&.&.&.&1&.&.& \\\\\n1&.&1&.&1&.&.&.&.&.&1&.& \\\\\n1&.&1&.&1&.&.&.&.&.&1&.& \\\\\n1&.&1&.&.&1&.&.&.&.&.&1& \\\\\n1&.&1&.&.&1&.&.&.&.&.&1& \\\\\n\\end{array}\n$$\n\n1.  En écrivant les vecteurs dans le cadre général et en faisant la somme ci-dessous\n    $$\n    \\vec{Y}=\\mu \\vec{e}+\\sum_{i} \\alpha_i \\vec{e}_{i.}+\\sum_{j} \\beta_j \\vec{e}_{.j}\n    +\\sum_{ij} (\\alpha\\beta)_{ij} \\vec{e}_{ij} + \\vec{\\varepsilon},\n    $${#eq-anova2somme}\n    on a  bien que la ligne $ijk$ vaut\n    $$\n    y_{ijk} = \\mu +\\alpha_i+\\beta_j+(\\alpha\\beta)_{ij}+ \\varepsilon_{ijk}.\n    $$\n\n2.  Montrons que $E_1 \\perp E_2$. Pour cela prenons deux vecteurs quelconques de $E_1$ et $E_2 $, ils s'écrivent $m\\vec{e}$ et\n$\\sum_{i=1}^I a_{i} \\vec{e}_{i.}$ (avec $\\sum_{i=1}^I a_{i}=0$) et leur produit scalaire vaut\n$$\n<m\\vec{e};\\sum_{i=1}^I a_{i} \\vec{e}_{i.}>=m\\sum_{i=1}^I a_{i}<\\vec{e};\\vec{e}_{i.}> = m I \\sum_{i=1}^I a_{i} =0\n$$\n    De même avec $E_1 \\perp E_3$.\n\n    Montrons que $E_1 \\perp E_4$. Pour cela prenons deux vecteurs quelconques de $E_1$ et $E_4$, ils s'écrivent $m\\vec{e}$ et\n$\\sum_{i=1}^I\\sum_{j=1}^J (ab)_{ij} \\vec{e}_{ij}$ (avec pour tout $i$ $\\sum_j (ab)_{ij}=0$ et pour tout $j$\n$\\sum_i (ab)_{ij}=0$). Leur produit scalaire vaut\n\\begin{align*}\n  <m\\vec{e};\\sum_{i=1}^I\\sum_{j=1}^J (ab)_{ij}\\vec{e}_{ij}>\n  &=m\\sum_{i=1}^I \\sum_{j=1}^J (ab)_{ij}<\\vec{e};\\vec{e}_{ij}> = m r \\sum_{i=1}^I \\sum_{j=1}^J (ab)_{ij} =0\n\\end{align*}\n\n    Montrons que $E_2 \\perp E_4$. Pour cela prenons deux vecteurs quelconques de $E_2$ et $E_4$, ils s'écrivent $\\sum_{l=1}^I a_{l} \\vec{e}_{l}$ (avec $\\sum_{l=1}^I a_{l}=0$) et\n$\\sum_{i=1}^I\\sum_{j=1}^J (ab)_{ij} \\vec{e}_{ij}$ (avec $i$ $\\sum_j (ab)_{ij}=0$ et pour tout $j$\n$\\sum_i (ab)_{ij}=0$. Leur produit scalaire vaut\n\\begin{align*}\n  <\\sum_{l=1}^I a_{l} \\vec{e}_{l.};\\sum_{i=1}^I\\sum_{j=1}^J (ab)_{ij}\\vec{e}_{ij}>\n  &=\\sum_{l=1}^I a_{l}\\sum_{i=1}^I \\sum_{j=1}^J (ab)_{ij}<\\vec{e}_{l.};\\vec{e}_{ij}> \\\\\n  &=\\sum_{l=1}^I a_{l} r \\sum_{i=1}^I \\sum_{j=1}^J (ab)_{ij} =0\n\\end{align*}\n    De même avec $E_3 \\perp E_4$.\n    \n3.  La dimension de $E_{1}$ vaut 1 (car $\\vec{e}$ est non nul). Le sous-espace $E_{2}$ est engendré par les $I$ vecteurs non nuls et orthogonaux deux à deux $\\{\\vec{e}_{i.}\\}_{i=1}^{I}$ donc le sous espace engendré est au moins de dimension $I$. Cependant on ajoute une contrainte linéaire donc $dim(E_{2})=I-1$. De même pour $E_{2}$ dont la dimension est donc $J-1$. Enfin sous-espace $E_{4}$ est engendré par les $IJ$ vecteurs non nuls et orthogonaux deux à deux $\\{\\vec{e}_{ij}\\}$ (donc le sous espace engendré est de dimension $IJ$) mais auquel on ajoute plusieurs contraintes linéaires. Il faut donc compter le nombre de contrainte linéaires indépendantes.\n\n    Montrons que les $I+J$ contraintes $ \\forall i \\sum_{j} (ab)_{ij}=0$ et $\\forall j \\sum_{i} (ab)_{ij}=0$ ne sont pas indépendantes. En effet quand $I+J-1$ contraintes sont vérifiées, la dernière restante l'est aussi.\n$$\n  \\begin{array}{*{5}c}\n  (ab)_{11}&(ab)_{12}&\\ldots&(ab)_{1J-1}&(ab)_{1J}&=0\\\\\n  (ab)_{21}&(ab)_{22}&\\ldots&(ab)_{2J-1}&(ab)_{2J}&=0\\\\\n  \\vdots&\\vdots& &\\vdots&\\vdots&\\vdots\\\\\n  (ab)_{I1}&(ab)_{I2}&\\ldots&(ab)_{IJ-1}&(ab)_{IJ}&=0\\\\\n  =0&=0&\\ldots&=0&c=?&\n\\end{array}\n$$\n    Posons que $I+J-1$ contraintes sont vérifiées~: $I$ en ligne et $J-1$ en colonnes (voir ci-dessus). En sommant toute la matrice on sait (somme en ligne) que cela vaut zéro et donc la somme en colonne vaut elle aussi 0 et donc la dernière somme $c$ vaut  0 (voir ci-dessus). Nous avons donc que la dimension de $E_{4}$ est $IJ-(I +J -1)=(I-1)(J-1)$\n    \n4.  \n    -   Calculons $P_{1}Y$\n        \\begin{align*}\n        P_{1}Y &= \\un (\\un' \\un)^{-1} \\un'Y=\\un (IJr)^{-1} \\sum_{ijk}Y_{ijk} = \\frac{1}{IJr} Y_{...} \\un\\\\\n              &=\\bar Y_{...} \\vec{e}\n        \\end{align*}\n    -   Calculons $P_{2}Y$. On sait que $F_{2}=E_1 \\stackrel{\\perp}{\\bigoplus} E_2$ et que $F_{2}$ est de dimension $I$. Il est donc engendré par les $I$ vecteurs orthogonaux $\\{\\vec{e}_{i.}\\}_{i=1}^{I}$ qui en forme une base. Du fait de la décomposition on a\n        $$\n        \\begin{align}\n        P_{F_{2}}Y &= P_{1}Y  + P_{2}Y\n        \\end{align}\n        $${#eq-pf2}\n        Calculons maintenant directement la projection sur $F_{2}$. Ce sous-espace est engendré par les $I$ vecteurs orthogonaux $\\{\\vec{e}_{i.}\\}_{i=1}^{I}$ en posant la matrice concaténant les (coordonnées des) vecteurs:\n        \\begin{align*}\n        F_{2}&=(\\vec{e}_{1.}|\\vec{e}_{2}|\\cdots|\\vec{e}_{I.})\n        \\end{align*}\n        on a le projecteur\n        \\begin{align*}\n        P_{F_{2}}&=F_{2}(F_{2}'F_{2})^{-1}F'_{2}\n        \\end{align*}\n        En effectuant le calcul matriciel on a\n        \\begin{align*}\n        (F_{2}'F_{2})&=\\diag(I, I, \\dotsc, I)\n        \\end{align*}\n        et par  calcul matriciel direct on trouve\n        $$\n        \\begin{align}\n        P_{F_{2}}Y&=F_{2}\\diag(1/I, 1/I, \\dotsc, 1/I)F_{2}'Y\\nonumber\\\\\n        &=F_{2}\n        \\begin{pmatrix}\n        \\bar Y_{1..}\\\\\n        \\bar Y_{2..}\\\\\n        \\vdots\\\\\n        \\bar Y_{I..}\\\\\n        \\end{pmatrix}\n        = \\bar Y_{1..} \\vec{e}_{1.} + \\bar Y_{2..}\\vec{e}_{2.}\n        + \\dotsc \\bar Y_{I..}\\vec{e}_{I.}\n        \\end{align}\n        $${#eq-pf2bis}\n        En utilisant les équations @eq-pf2 et @eq-pf2bis on trouve\n        \\begin{align*}\n        P_{2}Y&=\\bar Y_{1..} \\vec{e}_{1.} + \\bar Y_{2..}\\vec{e}_{2.}\n        + \\dotsc + \\bar Y_{I..}\\vec{e}_{I.} - \\bar Y_{...} \\vec{e}.\n        \\end{align*}\n        Remarquons que $\\vec{e}=\\vec{e}_{1.} + \\dotsc +\\vec{e}_{I.}$ et en remplaçant cela dans l'équation précédente nous avons\n        $$\n        \\begin{align*}\n        P_{2}Y&=(\\bar Y_{1..} -\\bar Y_{...})  \\vec{e}_{1.} + (\\bar Y_{2..} - \\bar Y_{...}) \\vec{e}_{2.}\n        + \\dotsc + (\\bar Y_{I..} -\\bar Y_{...}) \\vec{e}_{I.}.\n        \\end{align*}\n        $$\n    -   En calquant ces calculs pour $E_{3}$ on trouve\n        \\begin{align*}\n        P_{3}Y&=(\\bar Y_{.1.}-\\bar Y_{...}) \\vec{e}_{.1} + (\\bar Y_{.2.}-\\bar Y_{...})\\vec{e}_{.2}\n        + \\dotsc +(\\bar Y_{.J.}-\\bar Y_{...})\\vec{e}_{.J}.\n        \\end{align*}\n    -   Enfin pour $E_{4}$, remarquons que $F_{4}=E=\\vect(\\vec{e}_{11}, \\dotsc, \\vec{e}_{IJ})$. La projection sur $E$ identifié à sa matrice $(\\vec{e}_{11}| \\dotsc |\\vec{e}_{IJ})$ peut être calculée de manière directe comme\n        \\begin{align}\n        P_{E}Y&=E\\diag(1/r, 1/r, \\cdots, 1/r)E'Y\\nonumber\\\\\n        &=E\n        \\begin{pmatrix}\n        \\bar Y_{11.}\\\\\n        \\bar Y_{21.}\\\\\n        \\vdots\\\\\n        \\bar Y_{IJ.}\\\\\n        \\end{pmatrix}\n        = \\bar Y_{11.} \\vec{e}_{11} + \\dotsc \\bar Y_{IJ.}\\vec{e}_{IJ}\\label{eq:pebis}\n        \\end{align}\n        En se servant de la décomposition on a\n        \\begin{align}\n        P_{E}Y&=P_{1}Y + P_{2}Y  + P_{3}Y  + P_{4}Y\n        \\end{align}\n        Et en identifiant les deux calculs (avec $\\vec{e}_{i.}=\\vec{e}_{i1} + \\dotsc +\\vec{e}_{iJ}$ et $\\vec{e}_{.j}=\\vec{e}_{1j} + \\dotsc +\\vec{e}_{Ij}$ )\n        \\begin{align*}\n        P_{4}Y&= (\\bar Y_{11.} - \\bar Y_{1..} - \\bar Y_{.1.} + \\bar Y_{...})\\vec{e}_{11}+ \\dotsc + (\\bar Y_{IJ} - \\bar Y_{I..} - \\bar Y_{.J.} + \\bar Y_{...})\\vec{e}_{IJ}.\n        \\end{align*}\n    -   La dernière projection s'obtient comme\n        \\begin{align*}\n        QY&=Y- P_{E}Y = Y -  \\bar Y_{11.} \\vec{e}_{11} + \\dotsc \\bar Y_{IJ.}\\vec{e}_{IJ}\n        \\end{align*}\n\n5.  En reprenant la décomposition en sous-espace orthogonaux suivante\n    $$\n    \\begin{align}\n    \\R^{n}&= E  \\stackrel{\\perp}{\\bigoplus} E^{\\perp} =E  \\stackrel{\\perp}{\\bigoplus} Q\\\\\n    &= E_1 \\stackrel{\\perp}{\\bigoplus} E_2 \\stackrel{\\perp}{\\bigoplus} E_3 \\stackrel{\\perp}{\\bigoplus} E_4 \\stackrel{\\perp}{\\bigoplus} Q\n    \\end{align}\n    $${#eq-decompEanova2}\n    On a donc que\n    $$\n    \\begin{align*}\n    Y &= P_{1}Y + P_{2}Y + P_{3}Y + P_{4}Y + P_{Q}Y.\n    \\end{align*}\n    $${#eq-decompPanova2}\n    En utilisant toutes les définitions de la question précédente on a\n    $$\n    \\begin{split}\n    Y&=\\bar Y_{...} \\vec{e}\\\\\n    & \\ \\  +  (\\bar Y_{1..} - \\bar Y_{...}) \\vec{e}_{1.} + \\dotsc +\\bar (Y_{I..}- \\bar Y_{...})\\vec{e}_{I.}\\\\\n    & \\ \\    + (\\bar Y_{.1.}- \\bar Y_{...}) \\vec{e}_{.1} +  \\dotsc +\\bar (Y_{.J.}- \\bar Y_{...})\\vec{e}_{.J} \\\\\n    & \\ \\  + (\\bar Y_{11.} - \\bar Y_{1..} - \\bar Y_{.1.} + \\bar Y_{...})\\vec{e}_{11}+ \\dotsc + (\\bar Y_{IJ} - \\bar Y_{I..} - \\bar Y_{.J.} + \\bar Y_{...})\\vec{e}_{IJ} \\\\\n    & \\ \\ + P_{Q}Y.\n    \\end{split}\n    $${#eq-decompPCoefanova2}\n    En utilisant l'@eq-anova2somme on identifie terme à terme et nous obtenons les paramètres du modèle :\n    \\begin{align*}\n    \\hat \\mu&=\\bar Y_{...}\\\\\n    \\hat \\alpha_{i}&=(\\bar Y_{i..} - \\bar Y_{...})\\\\\n    \\hat \\beta_{j}&=(\\bar Y_{.j.} - \\bar Y_{...})\\\\\n    (\\widehat{\\alpha\\beta})_{ij}&=(\\bar Y_{ij.} - \\bar Y_{i..} - \\bar Y_{.j.} + \\bar Y_{...})\\\\\n    \\end{align*}\n    \n6.  En utilisant l'@eq-decompPanova2 et en se rappelant de l'orthogonalité (@eq-decompEanova2) on a\n    $$\n    \\begin{align}\n    \\|Y -\\bar Y_{...} \\vec{e}\\|^{2} &=\\| P_{1}Y + P_{2}Y + P_{3}Y + P_{4}Y + P_{Q}Y\\|^{2}\\nonumber\\\\\n    &=\\|P_{1}Y\\|^{2} + \\|P_{2}Y\\|^{2} + \\|P_{3}Y\\|^{2} + \\|P_{4}Y\\|^{2} + \\|P_{Q}Y\\|^{2}\n    \\end{align}\n    $${#eq-decompositonSC1}\n    et remplaçant les projections par leur expression (voir par exemple @eq-decompPCoefanova2) et calculant les normes on a\n    $$\n    \\begin{split}\n    \\sum_{i=1}^I \\sum_{j=1}^J\\sum_{k=1}^r (Y_{ijk} - \\bar Y_{...})^{2}\n    &= rJ\\sum_{i=1}^I(\\bar Y_{i..} - \\bar Y_{...})^{2}\n    + rI\\sum_{j=1}^J(\\bar Y_{.j.} - \\bar Y_{...})^{2}\\\\\n    & \\ \\  + r\\sum_{i=1}^I\\sum_{j=1}^J\n    (\\bar Y_{ij} - \\bar Y_{i..} - \\bar Y_{.j.} + \\bar Y_{...})^{2}\\\\\n    & \\ \\  + \\sum_{i=1}^I\\sum_{j=1}^J\\sum_{k=1}^r\n    (Y_{ijk} -  \\bar Y_{ij})^{2}.\n    \\end{split}\n    $${#eq-decompositonSC2}\n    En multipliant par $1/n$ l'équation ci-dessus nous obtenons la décomposition de la variance.\n\n7.  Le vecteur $Y$ grâce à $\\HH_{3}$ est un vecteur gaussien de moyenne $\\vec{m}=\\mu +\\alpha_i+\\beta_j+(\\alpha\\beta)_{ij}$ et de variance $\\sigma^{2}I_{n}$. On sait que\n    \\begin{align*}\n    P_{E}Y &= P_{1}Y + P_{2}Y + P_{3}Y + P_{4}Y\n    \\end{align*}\n    Grâce au théorème de Cochran on a que\n    $$\n    \\frac{\\|P_{i}Y -P_{i}\\vec{m} \\|^{2}}{\\sigma^{2}} \\sim \\chi^{2} (dim(E_{i}))\n    $$\n    ou encore que $\\frac{\\|P_{i}Y\\|^{2}}{\\sigma^{2}}$ suit un $\\chi^{2} (dim(E_{i}))$ décentré de paramètre de décentrage $\\|P_{i}\\vec{m} \\|^{2}$. Pour $Q=E^{\\perp}$ on a que $P_{Q}\\vec{m}=0$ et il n'y a pas de décentrage.\n    \n8.  Reprenons l'@eq-decompositonSC1 qui s'écrit aussi avec des sommes (@eq-decompositonSC2) ou encore\n    \\begin{align*}\n    \\SCT\n    &= \\SCE_{a} + \\SCE_{b} + \\SCE_{ab}  + \\SCR\n    \\end{align*}\n    On a donc que\n    \\begin{align*}\n    \\SCE_{a}&=\\|P_{2}Y\\|^{2}\\sim \\sigma^{2}\\chi^{2} (I-1), \\|P_{2}\\vec{m} \\|^{2}),\\\\\n    \\SCE_{b}&=\\|P_{3}Y\\|^{2}\\sim \\sigma^{2}\\chi^{2} (J-1), \\|P_{3}\\vec{m} \\|^{2}),\\\\\n    \\SCE_{ab}&=\\|P_{4}Y\\|^{2}\\sim \\sigma^{2}\\chi^{2} ((I-1)(J-1)), \\|P_{4}\\vec{m} \\|^{2}),\\\\\n    \\SCE_{ab}&=\\|P_{Q}Y\\|^{2}\\sim \\sigma^{2}\\chi^{2} (n - IJ).\n    \\end{align*}\n    Nous voyons donc que chaque terme est la norme carrée d'une projection du vecteur gaussien $Y$ dans un sous-espace et que ces sous-espaces sont orthogonaux 2 à 2. Les vecteurs gaussiens projetés sont donc indépendants ainsi que leur norme au carré. Les lois de ces normes carrées sont donc à $\\sigma^{2}$ près des $\\chi^{2}$ décentrés (sauf pour $Q$) qui sont indépendants.\n    \n9.  Notons $\\CME_{ab} = \\SCE_{ab}/((I-1)(J-1))$ et $\\CMR = \\SCR/(n-IJ)$ nous avons donc\n    \\begin{align*}\n    \\frac{\\CME_{ab}}{\\CMR}&=\\frac{\\frac{\\SCE_{ab}}{\\sigma^{2}(I-1)(J-1)}}{\\frac{\\SCR}{\\sigma^{2}(n-IJ)}}\n    \\end{align*}\n    qui est le rapport de deux $\\chi^{2}$ indépendants ramenés à leur degrés de liberté et dont le numérateur est décentré. Nous avons donc une loi de Fisher de paramètres $(I-1)(J-1), n -IJ, \\|P_{4}\\vec{m} \\|^{2}$. Sous $\\Hz:$ « il n'y a pas d'interaction » (ou $P_{4}\\vec{m}=0$) alors la loi se simplifie et le paramètre de décentrage inconnu (qui dépend de  $\\vec{m}$) disparaît et la loi est $F((I-1)(J-1), n -IJ)$.\n:::\n\n",
    "supporting": [
      "chap6_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}