{
  "hash": "25c0dcda199a36c47caf9af4624b87a7",
  "result": {
    "markdown": "---\ntitle: \"1 La régression linéaire simple\"\ntoc: true\n---\n\n\n\n::: {.content-visible when-format=\"html\"}\n::: {.cell}\n\\DeclareMathOperator{\\C}{Cov}\n\\DeclareMathOperator{\\V}{V}\n\\DeclareMathOperator{\\leR}{R^2}\n\\DeclareMathOperator{\\trace}{tr}\n\\DeclareMathOperator{\\tr}{tr}\n\n\\newcommand{\\M}{\\mathcal M}\n\\DeclareMathOperator{\\MCO}{MCO}\n\\DeclareMathOperator{\\SCR}{SCR}\n\\DeclareMathOperator{\\SCT}{SCT}\n\\DeclareMathOperator{\\SCE}{SCE}\n\\DeclareMathOperator{\\EQM}{EQM}\n\\DeclareMathOperator{\\diag}{diag}\n\n\\newcommand{\\1}{\\mathbf{1}}\n\\newcommand{\\un}{\\mathbf{1}}\n\n\n\\newcommand{\\prob}{\\mathbf P}\n\\newcommand{\\argmin}{\\mathop{\\mathrm{argmin}}}\n\\newcommand{\\ind}{\\mathbf 1}\n\\newcommand{\\R}{\\mathbb R}\n\\newcommand{\\E}{\\mathbf E}\n\\newcommand{\\var}{\\mathbf V}\n\\newcommand{\\ps}[2]{\\langle #1,#2\\rangle}\n\\newcommand{\\card}[1]{|{#1}|}\n\\newcommand{\\cov}{\\mathbf{cov}}\n\\newcommand{\\corr}{\\text{corr}}\n\\newcommand{\\AUC}{\\text{AUC}}\n\\newcommand{\\logit}{\\text{logit}}\n\\newcommand{\\li}[2]{#1_{#2}}\n\\newcommand{\\ssli}[2]{#1_{(#2)}}\n\\newcommand{\\HH}{{\\mathcal{H}}}\n\\newcommand{\\Hz}{{\\mathrm{H_0}}}\n\\newcommand{\\Hu}{{\\mathrm{H_1}}}\n:::\n:::\n\n\n::: {#exr-1-1 name=\"Questions de cours\"}\nB, A, B, A.\n:::\n\n::: {#exr-1-2 name=\"Biais des estimateurs\"}\nLes $\\hat \\beta_j$ sont fonctions de $Y$ (aléatoire), ce sont donc des \nvariables aléatoires. Une autre façon d'écrire $\\hat \\beta_2$ \nen fonction de $\\beta_2$ consiste à remplacer $y_i$ \npar sa valeur soit \n\\begin{eqnarray*}\n\\hat \\beta_2 &=&\\frac{\\sum (x_i - \\bar x) y_i}{\\sum(x_i-\\bar x)^2}\n=\\frac{\\beta_1\\sum (x_i - \\bar x)+\\beta_2\\sum x_i(x_i - \\bar x)+\n\\sum (x_i - \\bar x) \\varepsilon_i }{\\sum(x_i-\\bar x)^2} \\\\\n&=& \\beta_2 + \\frac{\\sum (x_i - \\bar x) \\varepsilon_i}{\\sum(x_i-\\bar x)^2}.\n\\end{eqnarray*}\nPar hypothèse $\\E(\\varepsilon_i)=0$, les autres termes ne sont \npas aléatoires, le résultat est démontré.\n\n\nLe résultat est identique pour $\\hat \\beta_1$ car $\\E (\\hat \\beta_1) =\n\\E (\\bar y) -\\bar x \\E (\\hat \\beta_2)= \\beta_1 +\\bar x \\beta_2 - \\bar\nx \\beta_2=\\beta_1$, le résultat est démontré.\n\n:::\n\n::: {#exr-1-3 name=\"Variance des estimateurs\"}\nNous avons\n\\begin{eqnarray*}\n\\V(\\hat \\beta_2) \n&=& \n\\V\\left(\\beta_2+\\frac{\\sum(x_i-\\bar x)\\varepsilon_i}\n{\\sum(x_i-\\bar x)^2}\\right).\n\\end{eqnarray*}\nOr $\\beta_2$ est inconnu mais pas aléatoire et les $x_i$ ne sont pas\naléatoires donc \n\\begin{eqnarray*}\n\\V(\\hat \\beta_2) \n&=&\\V\\left(\\frac{\\sum(x_i-\\bar x)\\varepsilon_i}{\\sum(x_i-\\bar x)^2}\\right)\n=\\frac{\\V\\left(\\sum(x_i-\\bar x)\\varepsilon_i\\right)}{\\left[\\sum(x_i-\\bar x)^2\\right]^2}\\\\\n&=&\\frac{\\sum_{i,j}(x_i-\\bar x)(x_j-\\bar x)\\C(\\varepsilon_i,\\varepsilon_j)}{\\left[\\sum(x_i-\\bar x)^2\\right]^2}.\n\\end{eqnarray*}\nOr $\\C(\\varepsilon_i,\\varepsilon_j)=\\delta_{ij}\\sigma^2$ donc\n\\begin{eqnarray*}\n\\V(\\hat \\beta_2) \n&=&\\frac{\\sum_i(x_i-\\bar x)^2\\sigma^2}{\\left[\\sum_i(x_i-\\bar x)^2\\right]^2}\n=\\frac{\\sigma^2}{\\sum(x_i-\\bar x)^2}.\n\\end{eqnarray*}\nPlus les mesures $x_i$ sont dispersées autour de leur moyenne, plus\n$\\V(\\hat \\beta_2)$ est faible et plus l'estimation est\nprécise. Bien sûr, plus $\\sigma^2$ est faible, c'est-à-dire plus les\n$y_i$ sont proches de la droite inconnue, plus l'estimation est précise.\\\\\nPuisque $\\hat \\beta_1=\\bar y - \\hat \\beta_2 \\bar x$, nous avons\n\\begin{eqnarray*}\n\\V(\\hat \\beta_1) \n&=& \n\\V \\left(\\bar y-\\hat \\beta_2 \\bar x \\right)=\\V \\left(\\bar y\\right)+V(\\bar x \\hat \\beta_2)-2\\C(\\bar y,\\hat \\beta_2 \\bar x)\\\\\n&=&\\V \\left(\\frac{\\sum y_i}{n}\\right)+\\bar x^2\\frac{\\sigma^2}{\\sum(x_i-\\bar x)^2}-2 \\bar x\\C(\\bar y,\\hat \\beta_2)\\\\\n&=&\\frac{\\sigma^2}{n}+\\bar x^2\\frac{\\sigma^2}{\\sum(x_i-\\bar x)^2}-2 \\bar x\\sum_i\\C(\\bar y,\\hat \\beta_2).\n\\end{eqnarray*}\n\nCalculons \n\\begin{eqnarray*}\n\\C(\\bar y, \\hat \\beta_2) \n&=& \\frac{1}{n}\n\\C\\left(\\sum_{i}\\left(\\beta_1+\\beta_2 x_i+\\varepsilon_i\\right),\\frac{\\sum_j(x_j-\\bar x)\\varepsilon_j}\n{\\sum_j(x_j-\\bar x)^2}\\right)\\\\\n&=&\\frac{1}{n}\\sum_{i}\\C\\left(\\varepsilon_i,\\frac{\\sum_j(x_j-\\bar x)\\varepsilon_j}\n{\\sum_j(x_j-\\bar x)^2}\\right)\\\\\n&=&\\frac{1}{\\sum_j(x_j-\\bar x)^2}\n\\sum_{i}\\frac{1}{n}\\C\\left(\\varepsilon_i,\\sum_j(x_j-\\bar x)\\varepsilon_j\\right)\\\\\n&=&\\frac{\\sigma^2 \\frac{1}{n}\\sum_{i}(x_i-\\bar x)}{\\sum_j(x_j-\\bar x)^2}=0.\n\\end{eqnarray*}\nNous avons donc \n\\begin{eqnarray*}\n\\V(\\hat \\beta_1) \n&=& \\frac{\\sigma^2}{n}+\\bar x^2\\frac{\\sigma^2}{\\sum(x_i-\\bar x)^2}\n=\\frac{\\sigma^2 \\sum x_i^2}{n\\sum(x_i-\\bar x)^2}.\n\\end{eqnarray*}\nLà encore, plus $\\sigma^2$ est faible, c'est-à-dire plus les\n$y_i$ sont proches de la droite inconnue, plus l'estimation est précise.\nPlus les valeurs $x_i$ sont dispersées autour de leur moyenne, plus\nla variance de l'estimateur sera faible. De même, une faible moyenne $\\bar x$\nen valeur absolue contribue à bien estimer $\\beta_1$. \n\n:::\n\n::: {#exr-1-4 name=\"Covariance de $\\hat\\beta_1$ et $\\hat\\beta_2$\"}\nNous avons\n\\begin{eqnarray*}\n\\C(\\hat \\beta_1,\\hat \\beta_2) &=&\\C(\\bar y-\\hat \\beta_2 \\bar x,\\hat \\beta_2)\n= \\C(\\bar y,\\hat \\beta_2)- \\bar x \\V(\\hat \\beta_2)=\n -\\frac{\\sigma^2 \\bar x}{\\sum(x_i-\\bar x)^2}.\n\\end{eqnarray*}\nLa covariance entre $\\beta_1$ et $\\beta_2$ est négative.  L'équation\n$\\bar y=\\hat \\beta_1+\\hat \\beta_2\\bar x$ indique que la droite des MC\npasse par le centre de gravité du nuage $(\\bar x, \\bar y)$. Supposons\n$\\bar x$ positif, nous voyons bien que, si nous augmentons la pente,\nl'ordonnée à l'origine va diminuer et vice versa. Nous retrouvons donc\nle signe négatif pour la covariance entre $\\hat \\beta_1$ et $\\hat\n\\beta_2$.\n:::\n\n::: {#exr-1-5 name=\"Théorème de Gauss-Markov\"}\nL'estimateur des MC s'écrit $\\hat \\beta_2 = \\sum_{i=1}^n p_i y_i,$\navec $p_i=(x_i-\\bar x)/\\sum(x_i -\\bar x)^2$. \n\nConsidérons un autre estimateur $\\tilde{\\beta_2}$ linéaire en\n$y_i$ et sans biais, c'est-à-dire  \n$$\\tilde{\\beta_2} =\\sum_{i=1}^n \\lambda_i y_i.$$ \n\n\nMontrons que $\\sum \\lambda_i=0$ et $\\sum \\lambda_i x_i=1$. \nL'égalité $\\E (\\tilde{\\beta_2}) = \\beta_1 \\sum \\lambda_i +\n\\beta_2 \\sum \\lambda_i x_i +  \\sum \\lambda_i \\E (\\varepsilon_i)$ \nest vraie pour tout $\\beta_2$ et $\\tilde \\beta_2$ est sans biais\ndonc $\\E(\\tilde \\beta_2)=\\beta_2$ pour tout $\\beta_2$, c'est-à-dire \nque $\\sum \\lambda_i=0$ et $\\sum \\lambda_i x_i=1$. \n\n\nMontrons que $\\V(\\tilde{\\beta_2}) \\geq \\V(\\hat\n\\beta_2)$.\n\\begin{eqnarray*}\n\\V(\\tilde{\\beta_2}) = \\V(\\tilde{\\beta_2}- \\hat \\beta_2 + \\hat \\beta_2)\n=\\V(\\tilde{\\beta_2}- \\hat \\beta_2)+\\V(\\hat \\beta_2)+\n2\\C(\\tilde{\\beta_2}- \\hat \\beta_2,\\hat \\beta_2).\n\\end{eqnarray*}\n\\begin{eqnarray*}\n\\C(\\tilde{\\beta_2}- \\hat \\beta_2,\\hat \\beta_2)\n\\!=\\!\\C(\\tilde{\\beta_2},\\hat \\beta_2) -\\V(-\\hat \\beta_2)\n\\!=\\!\\frac{\\sigma^2\\sum \\lambda_i(x_i-\\bar x)}{\\sum (x_i-\\bar x)^2} - \n\\frac{\\sigma^2}{\\sum (x_i-\\bar x)^2} \n\\!=\\!0,\n\\end{eqnarray*}\net donc \n\\begin{eqnarray*}\n\\V(\\tilde{\\beta_2}) =\n\\V(\\tilde{\\beta_2}- \\hat \\beta_2)+\\V(\\hat \\beta_2).\n\\end{eqnarray*}\nUne variance est toujours positive et donc\n\\begin{eqnarray*}\n\\V(\\tilde{\\beta_2}) \\geq \\V(\\hat \\beta_2).\n\\end{eqnarray*}\nLe résultat est démontré. On obtiendrait la même chose pour \n$\\hat \\beta_1$.\n:::\n\n::: {#exr-1-6 name=\"Somme des résidus\"}\nIl suffit de remplacer les résidus par leur \ndéfinition et de remplacer $\\hat \\beta_1$ par son expression \n\\begin{eqnarray*}\n\\sum_i \\hat \\varepsilon_i \n= \\sum_i (y_i - \\bar y + \\hat \\beta_2 \\bar x - \\hat \\beta_2 x_i)\n= \\sum_i (y_i-\\bar y) - \\hat \\beta_2 \\sum_i (x_i - \\bar x)= 0.\n\\end{eqnarray*}\n:::\n\n::: {#exr-1-7 name=\"Estimateur de la variance du bruit\"}\nRécrivons les résidus en constatant que $\\hat \\beta_1= \\bar y- \\hat \\beta_2 \\bar x$ et $\\beta_1=\\bar y - \\beta_2 \\bar x - \\bar \\varepsilon$,\n\\begin{eqnarray*}\n\\hat \\varepsilon_i&=& \\beta_1 + \\beta_2 x_i +\\varepsilon_i - \\hat \\beta_1 - \\hat \\beta_2x_i\\\\\n&=& \\bar{y} - \\beta_2 \\bar{x} - \\bar{\\varepsilon} + \\beta_2 x_i +\\varepsilon_i -\\bar{y} + \\hat \\beta_2 \\bar{x} - \\hat \\beta_2x_i\\\\\n&=& (\\beta_2-\\hat \\beta_2)(x_i -\\bar{x}) + (\\varepsilon_i - \\bar{\\varepsilon}).\n\\end{eqnarray*}\nEn développant et en nous servant de l'écriture de $\\hat \\beta_2$ donnée \ndans la solution de l'@exr-1-2, nous avons \n\\begin{eqnarray*}\n\\sum \\hat \\varepsilon_i^2& \\!=\\!& (\\beta_2-\\hat \\beta_2)^2 \\!\\sum  \\!(x_i \\!-\\!\\bar{x})^2\n\\!+\\!\\sum \\!(\\varepsilon_i \\!-\\! \\bar{\\varepsilon})^2\\!+\\!2 \\!(\\beta_2\\!-\\!\\hat \\beta_2)\n\\!\\sum \\!(x_i \\!-\\!\\bar{x})(\\varepsilon_i \\!- \\!\\bar{\\varepsilon})\\\\\n&=& (\\beta_2-\\hat \\beta_2)^2 \\sum  (x_i -\\bar{x})^2\n+\\sum (\\varepsilon_i - \\bar{\\varepsilon})^2 - 2 (\\beta_2-\\hat \\beta_2)^2 \\sum  (x_i -\\bar{x})^2.\n\\end{eqnarray*}\nPrenons en l'espérance\n\\begin{eqnarray*}\n\\E \\left( \\sum \\hat{\\varepsilon_i}^2\\right)= \\E  \\left(\\sum (\\varepsilon_i - \\bar{\\varepsilon})^2\\right) -\n\\sum  (x_i -\\bar{x})^2 \\V(\\hat \\beta_2)\n= (n-2) \\sigma^2.\n\\end{eqnarray*}\n:::\n\n::: {#exr-1-8 name=\"Prévision\"}\nCalculons la variance \n\\begin{eqnarray*}\n\\V\\left(\\hat y^p_{n+1}\\right)\n&=&\\V\\left(\\hat \\beta_1 + \\hat \\beta_2x_{n+1}\\right)\n=\\V(\\hat \\beta_1)+x_{n+1}^2 \\V(\\hat \\beta_2)\n+2 x_{n+1} \\C\\left(\\hat \\beta_1,\\hat \\beta_2\\right)\\\\\n&=& \\frac{\\sigma^2}{\\sum(x_i-\\bar x)^2}\\left(\n\\frac{\\sum x_i^2}{n}+x_{n+1}^2-2 x_{n+1}\\bar x \\right)\\\\\n&=& \\frac{\\sigma^2}{\\sum(x_i-\\bar x)^2}\\left(\n\\frac{\\sum (x_i-\\bar x)^2}{n}+\\bar x^2 + x_{n+1}^2-2 x_{n+1}\\bar x \\right)\\\\\n&=& \\sigma^2\\left(\\frac{1}{n}+\n\\frac{(x_{n+1}-\\bar x)^2}{\\sum (x_i-\\bar x)^2}\\right).\n\\end{eqnarray*}\nPlus la valeur à prévoir s'éloigne du centre de gravité, plus la\nvaleur prévue sera variable (i.e. de variance élevée).\n\n\n\\textbullet Variance de l'erreur de prévision\\\\\nNous obtenons la variance de l'erreur de \nprévision en nous servant du fait que $y_{n+1}$ est fonction de \n$\\varepsilon_{n+1}$ seulement, alors que $\\hat y^p_{n+1}$ est fonction \ndes autres $\\varepsilon_i$, $i=1,\\cdots,n$. Les deux quantités \nne sont pas corrélées. Nous avons alors\n\\begin{eqnarray*}\n\\V(\\hat \\varepsilon_{n+1}^p) \\!=\\! \\V\\left(y_{n+1} \\!-\\! \\hat y_{n+1}^p\\right) \n\\!=\\! \\V(y_{n+1})\\!+\\!\\V(\\hat y_{n+1}^p)\\!=\\! \\sigma^2\\left(1+\\frac{1}{n}\n\\!+\\!\\frac{(x_{n+1}-\\bar x)^2}{\\sum (x_i-\\bar x)^2}\\right).\n\\end{eqnarray*}\n\n:::\n\n::: {#exr-1-9 name=\"$R^2$ et coefficient de corrélation\"}\nLe coefficient $\\leR$ s'écrit\n\\begin{eqnarray*}\n\\leR&=&%\\frac{\\|\\hat{Y} -\\bar{y}\\1\\|^2}{\\|Y-\\bar{y}\\1\\|^2}=\n\\frac{\\sum_{i=1}^{n}{\\left(\\hat \\beta_1 + \\hat \\beta_2 x_i - \\bar{y}\\right)^2}}{\n\\sum_{i=1}^{n}{\\left(y_i-\\bar{y}\\right)^2}}=\n\\frac{\\sum_{i=1}^{n}{\\left( \\bar{y}-\\hat \\beta_2 \\bar{x}+ \\hat \\beta_2 x_i - \\bar{y}\\right)^2}}{\n{\\sum_{i=1}^{n}(y_i-\\bar{y})^2}}\\\\\n&=&\n\\frac{\\hat \\beta_2^2\\sum_{i=1}^{n}{\\left( x_i - \\bar{x}\\right)^2}}{\n\\sum_{i=1}^{n}\\left(y_i-\\bar{y}\\right)^2}\n=\\frac{\n\\left[\\sum_{i=1}^{n}{( x_i - \\bar{x})(y_i-\\bar{y})}\\right]^2\n\\sum_{i=1}^{n}{( x_i - \\bar{x})^2}}{\n\\left[\\sum_{i=1}^{n}{( x_i - \\bar{x})^2}\\right]^2\n\\sum_{i=1}^{n}(y_i-\\bar{y})^2}\\\\\n&=&\\frac{\\left[\n\\sum_{i=1}^{n}{( x_i - \\bar{x})(y_i-\\bar{y})}\\right]^2}{\n\\sum_{i=1}^{n}{( x_i - \\bar{x})^2}\\sum_{i=1}^{n}{(y_i-\\bar{y})^2}}=\\rho^2(X,Y).\n\\end{eqnarray*}\n:::\n\n::: {#exr-1-10 name=\"Les arbres\"}\nLe calcul donne \n\\begin{eqnarray*}\n\\hat \\beta_1 =\\frac{6.26}{28.29}=0.22 \\quad \\quad \n\\hat \\beta_0 = 18.34-0.22 \\times 34.9=10.662.\n\\end{eqnarray*}\nNous nous servons de la propriété $\\sum_{i=1}^n \\hat \\varepsilon_i=0$\npour obtenir\n\\begin{eqnarray*}\n\\leR2 &=& \\frac{\\sum_{i=1}^{20} (\\hat y_i - \\bar y)^2}\n{\\sum_{i=1}^{20}(y_i - \\bar y)^2}=\\frac{\\sum_{i=1}^{20} \n(\\hat \\beta_1 x_i - \\hat \\beta_1 \\bar x)^2}\n{\\sum_{i=1}^{20}(y_i - \\bar y)^2}=0.22^2 \\times \\frac{28.29}{2.85}=0.48.\n\\end{eqnarray*}\nLes statistiques de test valent 5.59 pour $\\beta_0$ et 4.11 pour \n$\\beta_1$. Elles sont à comparer à un fractile de la loi de Student admettant \n18 ddl, soit 2.1. Nous rejetons dans les deux cas l'hypothèse de nullité\ndu coefficient. Nous avons modélisé la hauteur par une fonction \naffine de la circonférence, il semblerait évident que la droite passe \npar l'origine (un arbre admettant un diamètre proche de zéro doit \nêtre petit), or nous rejetons l'hypothèse $\\beta_0=0$. Les données \nmesurées indiquent des arbres dont la circonférence varie de 26 à \n43 cm, les estimations des paramètres du modèle sont valides pour \ndes données proches de $[26;43]$.\n\n:::\n\n::: {#exr-1-11 name=\"Modèle quadratique\"}\nLes modèles sont \n\\begin{eqnarray*}\n\\mathtt{O3} &=& \\beta_1 + \\beta_2 \\mathtt{T12} +\\varepsilon \\quad \n\\hbox{modèle classique,}\\\\\n\\mathtt{O3} &=& \\gamma_1 + \\gamma_2 \\mathtt{T12}^2 +\\varepsilon \\quad\n\\hbox{modèle demandé}.\n\\end{eqnarray*}\nL'estimation des paramètres donne \n\\begin{eqnarray*}\n\\widehat{\\mathtt{O3}} &=& 31.41 + 2.7 \\ \\mathtt{T12} \\quad\\quad \\leR2=0.28 \\quad\n\\hbox{modèle classique,}\\\\\n\\widehat{\\mathtt{O3}} &=& 53.74 + 0.075 \\ \\mathtt{T12}^2 \\quad \\leR2=0.35 \\quad\n\\hbox{modèle demandé}.\n\\end{eqnarray*}\nLes deux modèles ont le même nombre de paramètres, nous préférons \nle modèle quadratique car le $\\leR$ est plus élevé.\n\n:::\n",
    "supporting": [
      "chap1_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}