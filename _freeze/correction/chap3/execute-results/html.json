{
  "hash": "07048cde25ebcb8f98706f70b2353740",
  "result": {
    "markdown": "---\ntitle: \"3 Validation du modèle\"\ntoc: true\n---\n\n\n\n::: {.content-visible when-format=\"html\"}\n::: {.cell}\n\\DeclareMathOperator{\\C}{Cov}\n\\DeclareMathOperator{\\V}{V}\n\\DeclareMathOperator{\\leR}{R^2}\n\\DeclareMathOperator{\\trace}{tr}\n\\DeclareMathOperator{\\tr}{tr}\n\n\\newcommand{\\M}{\\mathcal M}\n\\DeclareMathOperator{\\MCO}{MCO}\n\\DeclareMathOperator{\\SCR}{SCR}\n\\DeclareMathOperator{\\SCT}{SCT}\n\\DeclareMathOperator{\\SCE}{SCE}\n\\DeclareMathOperator{\\EQM}{EQM}\n\\DeclareMathOperator{\\diag}{diag}\n\n\\newcommand{\\1}{\\mathbf{1}}\n\\newcommand{\\un}{\\mathbf{1}}\n\n\n\\newcommand{\\prob}{\\mathbf P}\n\\newcommand{\\argmin}{\\mathop{\\mathrm{argmin}}}\n\\newcommand{\\ind}{\\mathbf 1}\n\\newcommand{\\R}{\\mathbb R}\n\\newcommand{\\E}{\\mathbf E}\n\\newcommand{\\var}{\\mathbf V}\n\\newcommand{\\ps}[2]{\\langle #1,#2\\rangle}\n\\newcommand{\\card}[1]{|{#1}|}\n\\newcommand{\\cov}{\\mathbf{cov}}\n\\newcommand{\\corr}{\\text{corr}}\n\\newcommand{\\AUC}{\\text{AUC}}\n\\newcommand{\\logit}{\\text{logit}}\n\\newcommand{\\li}[2]{#1_{#2}}\n\\newcommand{\\ssli}[2]{#1_{(#2)}}\n\\newcommand{\\HH}{{\\mathcal{H}}}\n\\newcommand{\\Hz}{{\\mathrm{H_0}}}\n\\newcommand{\\Hu}{{\\mathrm{H_1}}}\n:::\n:::\n\n\n::: {#exr-3-1 name=\"Questions de cours\"}\nC si $\\1$ fait partie des variables ou si $\\1 \\in \\Im(X)$, A, C, C, A.\n:::\n\n::: {#exr-3-2 name=\"Propriétés d'une matrice de projection\"}\nLa trace d'un projecteur vaut la dimension de l'espace sur lequel s'effectue \nla projection, donc $\\tr(P_X)=p$. Le second point découle de la propriété \n$P^2=P$.\n\nLes matrices $P_X$ et $P_XP_X$ sont égales, nous avons que \n$(P_X)_{ii}$ vaut $(P_XP_X)_{ii}$. Cela s'écrit \n\\begin{eqnarray*}\nh_{ii} &=&  \\sum_{k=1}^n h_{ik} h_{ki}\\\\\n&=& h_{ii}^2 + \\sum_{k=1, k \\neq i}^n h_{ik}^2\\\\\nh_{ii}(1-h_{ii}) &=& \\sum_{k=1, k \\neq i}^n h_{ik}^2.\n\\end{eqnarray*}\nLa dernière quantité de droite de l'égalité est positive et \ndonc le troisième point est démontré. En nous servant de cet\nécriture les deux derniers points sont aussi démontrés.\n\nNous pouvons écrire \n\\begin{eqnarray*}\nh_{ii}(1-h_{ii}) &=& h_{ij}^2 + \\sum_{k=1, k \\neq i ,j }^n h_{ik}^2.\n\\end{eqnarray*}\nLa quantité de gauche est maximum lorsque $h_{ii}=0.5$ et vaut \nalors $0.25$. Le quatrième point est démontré.\n:::\n\n::: {#exr-3-3 name=\"Lemme d'invertion matricielle\"}\nCommençons par effectuer les calculs en notant que la quantité \n$u'M^{-1}v$ est un scalaire que nous noterons $k$. Nous avons\n\\begin{eqnarray*}\n\\left(M+uv'\\right)\\left(M^{-1}-\\frac{M^{-1}uv'M^{-1}}{1+u'M^{-1}v}\\right)\n&=&MM^{-1}-\\frac{MM^{-1}uv'M^{-1}}{1+k}+uv'M^{-1}\n-\\frac{uv'M^{-1}uv'M^{-1}}{1+k}\\\\\n&=&I+\\frac{-uv'M^{-1}+uv'M^{-1}+kuv'M^{-1}-ukv'M^{-1}}{1+k}.\n\\end{eqnarray*}\nLe résultat est démontré.\n:::\n\n::: {#exr-3-4 name=\"Résidus studentisés\"}\n\n2.  Il suffit d'utiliser la définition du produit matriciel et de la somme matricielle et d'identifier les 2 membres des égalités.\n\n3.  En utilisant maintenant l'égalité de l'exercice précédent sur les \ninverses, avec $u=-x_i$ et $v=x_i'$, nous avons \n\\begin{eqnarray*}\n(\\ssli{X'}{i}\\ssli{X}{i})^{-1}=(X'X-\\li{x}{i}\\li{x}{i}')^{-1}=(X'X)^{-1}+\n\\frac{(X'X)^{-1}\\li{x}{i}\\li{x}{i}'(X'X)^{-1}}\n{1-\\li{x}{i}'(X'X)^{-1}\\li{x}{i}}.%\\label{eq:hiieta}\n\\end{eqnarray*}\n    La définition de $h_{ii}=\\li{x}{i}'(X'X)^{-1}\\li{x}{i}$\ndonne le résultat.\n\n4.  Calculons la prévision où $\\hat \\beta_{(i)}$ est l'estimateur de \n$\\beta$ obtenu sans la $i^e$ observation\n\n\\begin{eqnarray*}\n\\hat y_i^p \n= \\li{x}{i}'\\hat \\beta_{(i)} \n&=& \\li{x}{i}' (\\ssli{X'}{i}\\ssli{X}{i})^{-1}\\ssli{X'}{i}Y_{(i)}\\\\\n&=& \\li{x}{i}'\\left[(X'X)^{-1} \n+ \\frac{(X'X)^{-1}\\li{x}{i}\\li{x}{i}'(X'X)^{-1}}{1-h_{ii}}\n\\right]\\left(X'Y-\\li{x}{i}'y_i\\right)\\\\\n&=& \\li{x}{i}' \\hat \\beta + \\frac{h_{ii}}{1-h_{ii}}\\li{x}{i}' \\hat \\beta \n- h_{ii}y_i -\\frac{h_{ii}^2}{1-h_{ii}}y_i\\\\\n&=& \\frac{1}{1-h_{ii}}\\hat y_i - \\frac{h_{ii}}{1-h_{ii}}y_i.\n\\end{eqnarray*}\n\n5.  Ce dernier résultat donne\n\\begin{eqnarray*}\n\\hat \\varepsilon_i = (1-h_{ii})(y_i-\\hat y^p_i).\n\\end{eqnarray*}\nNous avons alors\n\\begin{eqnarray*}\nt^*_i &=& \\frac{\\hat \\varepsilon_i}{\\hat \\sigma_{(i)}\\sqrt{1-h_{ii}}}\\\\\n&=&\\frac{\\sqrt{(1-h_{ii})}(y_i - \\hat y_i^p)}{\\hat \\sigma_{(i)}}.\n\\end{eqnarray*}\nPour terminer, remarquons qu'en multipliant l'égalité de la question 3 \nà gauche par $\\li{x}{i}'$ et à droite par $\\li{x}{i}$ \n\\begin{eqnarray*}\n\\li{x}{i}'(\\ssli{X'}{i}\\ssli{X}{i})^{-1}\\li{x}{i} \n&=& h_{ii}+ \\frac{h_{ii}^2}{1-h_{ii}}.\\\\\n1+\\li{x}{i}'(\\ssli{X'}{i}\\ssli{X}{i})^{-1}\\li{x}{i} \n&=& 1 +\\frac{h_{ii}}{1-h_{ii}}=\\frac{h_{ii}}{1-h_{ii}}.\n\\end{eqnarray*}\n\n6.   Utilisons l'expression\n\\begin{eqnarray*}\nt^*_i=\\frac{y_i-\\hat y_i^p }\n{\\hat \\sigma_{(i)}\\sqrt{1+\\li{x}{i}'(\\ssli{X'}{i}\\ssli{X}{i})^{-1}\\li{x}{i}}}.\n\\end{eqnarray*}\nNous pouvons alors appliquer la preuve de la proposition 5.4\npage 97, en constatant que la $i^e$ observation \nest une nouvelle observation. Nous avons donc $n-1$ observations \npour estimer les paramètres, cela donne donc un Student à $n-1-p$ \nparamètres.\n:::\n\n::: {#exr-3-5 name=\"Distance de Cook\"}\n1.  Nous reprenons une partie des calculs de l'exercice précédent :\n\\begin{eqnarray*}\n\\hat \\beta_{(i)} &=& (\\ssli{X'}{i}\\ssli{X}{i})^{-1}\\ssli{X'}{i}\\ssli{Y}{i}\\\\\n&=& (X'X)^{-1}[X'Y-\\li{x}{i}y_i]+\\frac{1}{1-h_{ii}}\n(X'X)^{-1}\\li{x}{i}\\li{x}{i}'(X'X)^{-1}[X'Y-\\li{x}{i}y_i]\\\\\n&=& \\hat \\beta - (X'X)^{-1}\\li{x}{i}y_i + \\frac{1}{1-h_{ii}}\n(X'X)^{-1}\\li{x}{i}\\li{x}{i}'\\hat \\beta - \\frac{h_{ii}}{1-h_{ii}}\n(X'X)^{-1}\\li{x}{i}y_i,\n\\end{eqnarray*}\n    d'où le résultat. \n    \n2.  Pour obtenir la seconde écriture de la distance de Cook, nous écrivons d'abord que \n\\begin{eqnarray*}\n\\hat \\beta_{(i)} - \\hat \\beta = \\frac{- \\hat \\varepsilon_i}{1-h_{ii}}\n(X'X)^{-1}\\li{x}{i}.\n\\end{eqnarray*}\n    Puis nous développons\n\\begin{eqnarray*}\nC_i &=& \\frac{1}{p \\hat \\sigma^2}(\\hat \\beta_{[i]}-\\hat \\beta)'\nX'X(\\hat \\beta_{(i)}-\\hat \\beta)\\\\\n&=& \\frac{1}{p \\hat \\sigma^2} \\left(\\frac{- \\hat \\varepsilon_i}{1-h_{ii}}\\right)^2 \\li{x}{i}' (X'X)^{-1}(X'X)(X'X)^{-1}\\li{x}{i}.\n\\end{eqnarray*}\n    Le résultat est démontré.\n:::\n\n::: {#exr-3-6 name=\"Régression partielle\"}\nNous avons le modèle suivant :\n\\begin{eqnarray*}\nP_{X_{\\bar{j}}^\\perp} Y&=&\\beta_jP_{X_{\\bar{j}}^\\perp} X_j + \\eta.\n\\end{eqnarray*}\nL'estimateur des moindres carrés $\\tilde\\beta_j$ issu de ce \nmodèle vaut\n\\begin{eqnarray*}\n\\tilde \\beta_j = (X'_j P_{X_{\\bar{j}}^\\perp} X_j)^{-1}\nX'_j P_{X_{\\bar{j}}^\\perp} Y.\n\\end{eqnarray*}\nLa projection de $Y$ sur $\\Im(X_{\\bar{j}})$ (i.e. la prévision par \nle modèle sans la variable $X_j$) peut s'écrire comme la projection $Y$ \nsur $\\Im(X)$ qui est ensuite projetée sur $\\Im(X_{\\bar{j}})$, \npuisque $\\Im(X_{\\bar{j}})\\subset \\Im(X)$. Ceci s'écrit\n\\begin{eqnarray*}\nP_{X_{\\bar{j}}}Y&=&P_{X_{\\bar{j}}}P_{X}Y=P_{X_{\\bar{j}}}X\\hat{\\beta}\n=P_{X_{\\bar{j}}}(X_{\\bar{j}}\\hat\\beta_{\\bar{j}}+\\hat\\beta_jX_j)\n=X_{\\bar{j}}\\hat\\beta_{\\bar{j}}+\\hat\\beta_jP_{X_{\\bar{j}}}X_j,\n\\end{eqnarray*}\net donc\n\\begin{eqnarray*}\nX_{\\bar{j}}\\hat\\beta_{\\bar{j}} = P_{X_{\\bar{j}}} Y - \n\\hat\\beta_jP_{X_{\\bar{j}}}X_j.\n\\end{eqnarray*}\nRécrivons les résidus \n\\begin{eqnarray*}\n\\hat{\\varepsilon}&=&P_{X^\\perp} Y=Y-X\\hat\\beta\n=Y-X_{\\bar{j}}\\hat\\beta_{\\bar{j}}-\\hat\\beta_jX_j\\nonumber\\\\\n&=&Y-P_{X_{\\bar{j}}}Y + \\hat\\beta_jP_{X_{\\bar{j}}}X_j  -\\hat\\beta_j X_j\\nonumber\\\\\n&=&(I-P_{X_{\\bar{j}}})Y - \\hat\\beta_j(I-P_{X_{\\bar{j}}})X_j\\nonumber\\\\\n&=&P_{X_{\\bar{j}}^\\perp} Y-\\hat\\beta_jP_{X_{\\bar{j}}^\\perp} X_j.%\\label{eq:origine:residpartiel}\n\\end{eqnarray*}\nEn réordonnant cette dernière égalité, nous pouvons écrire\n\\begin{eqnarray}\nP_{X_{\\bar{j}}^\\perp} Y&=&\\hat\\beta_jP_{X_{\\bar{j}}^\\perp} X_j+\\hat{\\varepsilon}.\\nonumber\n\\end{eqnarray}\nNous avons alors\n\\begin{eqnarray*}\n\\tilde\\beta_j &=& (X'_j P_{X_{\\bar{j}}^\\perp} X_j)^{-1}\nX'_j P_{X_{\\bar{j}}^\\perp} Y\\\\\n&=& (X'_j P_{X_{\\bar{j}}^\\perp} X_j)^{-1}\nX'_j(\\hat\\beta_jP_{X_{\\bar{j}}^\\perp} X_j+\\hat{\\varepsilon})\\\\\n&=& \\hat\\beta_j +(X'_j P_{X_{\\bar{j}}^\\perp} X_j)^{-1} X'_j\\hat{\\varepsilon}).\n\\end{eqnarray*}\nLe produit scalaire $X'_j\\hat{\\varepsilon} = \\langle X_j,\\hat{\\varepsilon} \\rangle$ \nest nul car les deux vecteurs appartiennent à des sous-espaces orthogonaux, d'où le \nrésultat.\n\n:::\n\n::: {#exr-3-7 name=\"TP : Résidus partiels\"}\n\n1.  Importation\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    don <- read.table(\"../donnees/tprespartiel.dta\", header=TRUE, sep=\";\")\n    ```\n    :::\n\n\n2.  Estimation du modèle\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    mod <- lm(Y~X1+X2+X3+X4, data=don)\n    ```\n    :::\n\n\n3.  Analyse des résidus partiels. Commençons par calculer les résidus partiels (matrice $n\\times p$)\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    rpartiel <- residuals(mod, type=\"partial\")\n    ```\n    :::\n\n\n    Créons un data-frame à 3 colonnes et $np$ lignes: la colonne de nom de la variable (`X1` répété $n$ fois, `X2` répété $n$ fois, `X3` répété $n$ fois  et `X4` répété $n$ fois), les variables `X1`, `X2`,  `X3`  et `X4` et les résidus partiels.\n    \n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    noms <- rep(names(don)[1:4], each=nrow(don))\n    X <-  as.vector(data.matrix(don[,1:4]))\n    donlong <- cbind.data.frame(noms, X, rpartiel=as.vector(rpartiel))\n    ```\n    :::\n\n              \n    La représentation avec \\pkg{ggplot2} est donnée par\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    library(ggplot2)\n    ggplot(donlong, aes(X, rpartiel)) + geom_point() +\n      facet_wrap(vars(noms), scale=\"free\")\n    ```\n    \n    ::: {.cell-output-display}\n    ![](chap3_files/figure-html/unnamed-chunk-6-1.png){width=672}\n    :::\n    :::\n\n\n    Les 3 premières variables montrent des tendances linéaires (ou aucune pour la troisième) alors que la troisième semble montrer plutôt une tendance quadratique.\n\n4.  Refaisons le modèle avec `X5` :\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    don <- cbind.data.frame(don, X5=don$X4^{2})\n    mod2 <- lm(Y~X1+X2+X3+X5, data=don)\n    rpartiel <- residuals(mod2, type=\"partial\")\n    noms <- rep(names(don)[c(1:3,6)], each=nrow(don))\n    X <-  as.vector(data.matrix(don[,c(1:3,6)]))\n    donlong2 <- cbind.data.frame(noms, X, rpartiel=as.vector(rpartiel))\n    ggplot(donlong2, aes(X, rpartiel)) + geom_point() +\n      facet_wrap(vars(noms), scale=\"free\")\n    ```\n    \n    ::: {.cell-output-display}\n    ![](chap3_files/figure-html/unnamed-chunk-7-1.png){width=672}\n    :::\n    :::\n\n\n    et nous constatons que les résidus partiels sont tous à tendance linéaire. Les 2 modèles ayant le même nombre de variables nous pouvons les comparer via leur $\\leR$ qui valent 0.986 et 0.9966. La seconde modélisation est la meilleure tant pour la qualité globale que pour l'analyse des résidus.\n\n5.  Avec le second jeu de données\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    donbis <- read.table(\"../donnees/tpbisrespartiel.dta\", header=TRUE, sep=\";\")\n    modbis <- lm(Y~X1+X2+X3+X4, data=donbis)\n    rpartiel <- residuals(modbis, type=\"partial\")\n    noms <- rep(names(donbis)[1:4], each=nrow(donbis))\n    X <-  as.vector(data.matrix(donbis[,1:4]))\n    donlongbis <- cbind.data.frame(noms, X, rpartiel=as.vector(rpartiel))\n    ggplot(donlongbis, aes(X, rpartiel)) + geom_point() +\n      facet_wrap(vars(noms), scale=\"free\")\n    ```\n    \n    ::: {.cell-output-display}\n    ![](chap3_files/figure-html/unnamed-chunk-8-1.png){width=672}\n    :::\n    :::\n\n\n\n    Nous voyons clairement une sinusoïde de type $\\sin(-2\\pi X_4)$ sur le dernier graphique. Changeons `X4`\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    donbis <- cbind.data.frame(donbis, X5=sin(-2*pi*donbis$X4))\n    modbis2 <- lm(Y~X1+X2+X3+X5, data=donbis)\n    rpartiel <- residuals(modbis2, type=\"partial\")\n    noms <- rep(names(donbis)[c(1:3,6)], each=nrow(donbis))\n    X <-  as.vector(data.matrix(donbis[,c(1:3,6)]))\n    donlongbis2 <- cbind.data.frame(noms, X, rpartiel=as.vector(rpartiel))\n    ggplot(donlongbis2, aes(X, rpartiel)) + geom_point() +\n      facet_wrap(vars(noms), scale=\"free\")\n    ```\n    \n    ::: {.cell-output-display}\n    ![](chap3_files/figure-html/unnamed-chunk-9-1.png){width=672}\n    :::\n    :::\n\n\n    Les résidus partiels sont tous à tendance linéaire et donc corrects. La qualité globale analysée par $\\leR$ augmente elle aussi de 0.8106 à 0.9985.\n\n:::\n",
    "supporting": [
      "chap3_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}