{
  "hash": "61198758bd50c720a88207fae8de80b8",
  "result": {
    "markdown": "---\ntitle: \"9 Régression sur composantes : PCR et PLS\"\ntoc: true\n---\n\n\n\n::: {.content-visible when-format=\"html\"}\n::: {.cell}\n\\DeclareMathOperator{\\C}{Cov}\n\\DeclareMathOperator{\\V}{V}\n\\DeclareMathOperator{\\leR}{R^2}\n\\DeclareMathOperator{\\trace}{tr}\n\\DeclareMathOperator{\\tr}{tr}\n\n\\newcommand{\\M}{\\mathcal M}\n\\DeclareMathOperator{\\MCO}{MCO}\n\\DeclareMathOperator{\\SCR}{SCR}\n\\DeclareMathOperator{\\SCT}{SCT}\n\\DeclareMathOperator{\\SCE}{SCE}\n\\DeclareMathOperator{\\EQM}{EQM}\n\\DeclareMathOperator{\\diag}{diag}\n\\DeclareMathOperator{\\ro}{R^2_0}\n\\newcommand{\\SC}{\\text{SC}}\n\\DeclareMathOperator{\\vect}{\\mathsf{vect}}\n\\DeclareMathOperator{\\CME}{CME}\n\\DeclareMathOperator{\\CMA}{CMA}\n\\DeclareMathOperator{\\CMB}{CMB}\n\\DeclareMathOperator{\\CMR}{CMR}\n\\DeclareMathOperator{\\Cp}{C_p}\n\\DeclareMathOperator{\\radeux}{R^2_a}\n\n\\newcommand{\\1}{\\mathbf{1}}\n\\newcommand{\\un}{\\mathbf{1}}\n\n\n\\newcommand{\\prob}{\\mathbf P}\n\\newcommand{\\argmin}{\\mathop{\\mathrm{argmin}}}\n\\newcommand{\\ind}{\\mathbf 1}\n\\newcommand{\\R}{\\mathbb R}\n\\newcommand{\\E}{\\mathbf E}\n\\newcommand{\\var}{\\mathbf V}\n\\newcommand{\\ps}[2]{\\langle #1,#2\\rangle}\n\\newcommand{\\card}[1]{|{#1}|}\n\\newcommand{\\cov}{\\mathbf{cov}}\n\\newcommand{\\corr}{\\text{corr}}\n\\newcommand{\\AUC}{\\text{AUC}}\n\\newcommand{\\logit}{\\text{logit}}\n\\newcommand{\\li}[2]{#1_{#2}}\n\\newcommand{\\ssli}[2]{#1_{(#2)}}\n\\newcommand{\\HH}{{\\mathcal{H}}}\n\\newcommand{\\Hz}{{\\mathrm{H_0}}}\n\\newcommand{\\Hu}{{\\mathrm{H_1}}}\n\\newcommand{\\NO}{{\\mathcal{N}}}\n\\newcommand{\\sfrac}[2]{{#1}/{#2}}\n\n\\usepackage{nicematrix}\n\\usepackage{blkarray}\n:::\n:::\n\n\n::: {#exr-9-1 name=\"Questions de cours\"}\nA, B, C, A, B (les composantes sont orthogonales donc faire des régressions univariées $Y$ contre chaque composante $X^{*}_{j}$ ou faire la régression multiple $Y$ contre toutes les composantes $X^{*}_{j}$ revient au même pour des données centrées), A, C.\n:::\n\n::: {#exr-9-2 name=\"Régression sur composantes\"}\n\n:::\n\n::: {#exr-9-3 name=\"Régression sur composantes\"}\n\n:::\n\n::: {#exr-9-4 name=\"Régression sur composantes\"}\n\n:::\n\n::: {#exr-9-5 name=\"Théorème 9.2\"}\nElle s'effectue par récurrence. Nous allons ajouter à cette propriété un résultat intermédiaire qui constituera la première partie de la propriété:\n$$\nX^{(j)}=X\\prod_{i=1}^{j-1}(I-w^{(i)}({t^{(i)}}'t^{(i)})^{-1}{t^{(i)}}'X).\n$$\nLa seconde partie sera bien sûr de vérifier que $\\tilde{w}^{(j)}$ s'écrit bien sous la forme annoncée.\n\nLa propriété pour $j=1$ : la première partie n'a pas de sens et, concernant $\\tilde{w}^{(j)}$, par construction $X=X^{(1)}$ et donc $\\tilde{w}^{(1)}=w^{(1)}$.\n\nLa propriété pour $j=2$ est-elle vraie ? <br>\nNous savons que par définition $X^{(2)}=P_{{t^{(1)}}^\\perp}X^{(1)}$ et $X^{(1)}=X$ donc\n$$\n\\begin{eqnarray*}\nX^{(2)}&=&P_{{t^{(1)}}^\\perp}X^{(1)}=X-P_{t^{(1)}}X\n=X-t^{(1)}({t^{(1)}}'t^{(1)})^{-1}{t^{(1)}}'X\\\\\n&=&X(I-w^{(1)}({t^{(1)}}'t^{(1)})^{-1}{t^{(1)}}'X),\n\\end{eqnarray*}\n$$\ncar $t^{(1)}=Xw^{(1)}$. Ceci démontre la première partie de la propriété. Ensuite, puisque $t^{(2)}=X^{(2)}w^{(2)}=X\\tilde{w}^{(2)}$, en remplaçant $X^{(2)}$ par $X(I-w^{(1)}({t^{(1)}}'t^{(1)})^{-1}{t^{(1)}}'X)$ nous avons démontré la propriété pour le rang $j=2$.\n\nSupposons maintenant la propriété vraie au rang $(j-1)$. Nous avons par définition :\n$X^{(j)}=P_{{t^{(j-1)}}^\\perp}X^{(j-1)}$ donc\n$X^{(j)}=X^{(j-1)}-P_{t^{(j-1)}}X^{(j-1)}$. Or par construction\nles $\\{t^{(k)}\\}_{k=1}^j$ sont toutes orthogonales donc\n$P_{t^{(j-1)}}X^{(j-1)}=P_{t^{(j-1)}}X$. Nous avons, grâce\nà la propriété vraie pour le rang $(j-1)$, que\n\n$$\n\\begin{split}\nX^{(j)}&=X^{(j-1)}-t^{(j-1)}({t^{(j-1)}}'t^{(j-1)})^{-1}{t^{(j-1)}}'X\\\\\n&=X^{(j-1)}-X^{(j-1)}w^{(j-1)}({t^{(j-1)}}'t^{(j-1)})^{-1}{t^{(j-1)}}'X\\\\\n&=X\\prod_{i=1}^{j-2}(I-w^{(i)}({t^{(i)}}'t^{(i)})^{-1}{t^{(i)}}'X)\n(I-w^{(j-1)}({t^{(j-1)}}'t^{(j-1)})^{-1}{t^{(j-1)}}'X)\n\\end{split}\n$$\ndémontrant la première partie de la proposition. Ensuite, puisque $t^{(j)}=X^{(j)}w^{(j)}=X\\tilde{w}^{(j)}$, en remplaçant $X^{(j)}$ par\n$X\\prod_{i=1}^{j-1}(I-w^{(i)}({t^{(i)}}'t^{(i)})^{-1}{t^{(i)}}'X)$, nous avons démontré la propriété pour le rang $j$.\n:::\n\n::: {#exr-9-6 name=\"Géométrie des estimateurs\"}\n4.  Les quatre premières réponses sont évidentes, les coordonnées de $\\hat Y$ valent $1.5,0.5$ et $0$. Ici $p$ vaut 2 et $B_1$ est\nun cercle de centre $O$ de rayon 1, alors que $B_2$ est un losange.\n\n5.   Intuitivement, l'image d'un cercle par une application linéaire est une ellipse et l'image d'un losange est un parallélogramme.\n\n6.  Le dessin suivant représente les ensembles $C_1$ et $C_2$ et $\\hat Y$ grâce aux ordres **R** suivants :\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    X <- matrix(c(1,0,0,1/sqrt(3),2/sqrt(3),0),3,2)\n    sss <- 1\n    iter <- 1\n    coord <- matrix(0,500,2)\n    for (tt in  seq(-pi,pi,length=500)) {\n       coord[iter,] <- (X%*%as.matrix(sqrt(sss)\n                             *c(cos(tt),sin(tt))))[1:2,]\n       iter <- iter+1\n     }\n    iter <- 1\n    coord2 <- matrix(0,500,2)\n    for (tt in  seq(-1,1,length=250)) {\n       coord2[iter,] <- (X%*%as.matrix(c(tt,1-abs(tt))))[1:2,]\n       coord2[iter+250,] <- (X%*%as.matrix(c(tt,\n                                    abs(tt)-1)))[1:2,]\n       iter <- iter+1\n     }\n    plot(coord,type=\"l\",xlab=\"\",ylab=\"\")\n    lines(coord2)\n    ```\n    :::\n\n\n    ![](FIGURES/exo9-5-1.png){width='75%' fig-align=\"center\"}\n\n7.  Par définition, $X\\hat \\beta_{\\mathrm{ridge}}$ est l'élément de $C_1$ le plus proche de $\\hat Y$. De même, $X\\hat \\beta_{\\mathrm{lasso}}$ est l'élément de $C_2$ le plus proche de $\\hat Y$. Cela donne graphiquement\n    ![](FIGURES/exo9-5-2.png){width='75%' fig-align=\"center\"}\n8.  L'ensemble $C_1$, composé de vecteurs de la forme $u=X_1\\alpha_1+X_2\\alpha_2$ avec la norme du vecteur $\\alpha$ valant 1, peut être vu comme l'ensemble des composantes dans lequel on va choisir la composante PLS. La première composante PLS est le vecteur de $C_1$ dont le produit scalaire avec $Y$ (et donc $\\hat Y$) est le plus grand. Graphiquement, c'est le vecteur de $C_1$ dont l'extrémité sur l'ellipse est le pied de la tangente à l'ellipse perpendiculaire à $O\\hat Y$. La prévision de $Y$ par la régression PLS est la projection de $Y$ et donc de $\\hat Y$ sur la composante PLS.\n\n9.  La calcul donne simplement\n$$\n\\begin{eqnarray*}\nX'X = \\begin{pmatrix}\n1&\\sqrt{3}/3\\\\\n\\sqrt{3}/3&5/3\n\\end{pmatrix}.\n\\end{eqnarray*}\n$$\nLes valeurs propres sont 2 et 2/3.  Le premier axe principal correspond au vecteur propre associé à la valeur 2. Pour trouver la première\ncomposante principale, il faut pré-multiplier ce vecteur par $X$. Cela donne le vecteur de coordonnées $(1,1,0)'$. Les commandes GNU-R sont\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    X <- matrix(c(1,0,0,1/sqrt(3),2/sqrt(3),0),3,2)\n    svd <- eigen(t(X)%*%X)\n    X%*%svd$vect[,1]\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n         [,1]\n    [1,]    1\n    [2,]    1\n    [3,]    0\n    ```\n    :::\n    :::\n\n\n10. La prévision de $Y$ par la régression PCR est la projection de $Y$ (et donc de $\\hat Y$) sur la composante PCR. Dans cet exemple, la projection de $\\hat Y$ sur la composante PCR est un point de l'ellipse, mais cela est uniquement dû aux données de cet exercice.\nLe graphique suivant représente les 4 projections :\n\n    ![](FIGURES/exo9-5-3.png){width='75%' fig-align=\"center\"}\n\n\n:::\n\n::: {#exr-9-7 name=\"Orthonormalisation\"}\n\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}