{
  "hash": "219809069078f6def0df28574e9281ad",
  "result": {
    "markdown": "---\ntitle: \"2 La régression linéaire multiple\"\ntoc: true\n---\n\n\n\n::: {.content-visible when-format=\"html\"}\n::: {.cell}\n\\DeclareMathOperator{\\C}{Cov}\n\\DeclareMathOperator{\\V}{V}\n\\DeclareMathOperator{\\leR}{R^2}\n\\DeclareMathOperator{\\trace}{tr}\n\\DeclareMathOperator{\\tr}{tr}\n\n\\newcommand{\\M}{\\mathcal M}\n\\DeclareMathOperator{\\MCO}{MCO}\n\\DeclareMathOperator{\\SCR}{SCR}\n\\DeclareMathOperator{\\SCT}{SCT}\n\\DeclareMathOperator{\\SCE}{SCE}\n\\DeclareMathOperator{\\EQM}{EQM}\n\\DeclareMathOperator{\\diag}{diag}\n\\DeclareMathOperator{\\ro}{R^2_0}\n\\newcommand{\\SC}{\\text{SC}}\n\\DeclareMathOperator{\\vect}{\\mathsf{vect}}\n\\DeclareMathOperator{\\CME}{CME}\n\\DeclareMathOperator{\\CMA}{CMA}\n\\DeclareMathOperator{\\CMB}{CMB}\n\\DeclareMathOperator{\\CMR}{CMR}\n\n\\newcommand{\\1}{\\mathbf{1}}\n\\newcommand{\\un}{\\mathbf{1}}\n\n\n\\newcommand{\\prob}{\\mathbf P}\n\\newcommand{\\argmin}{\\mathop{\\mathrm{argmin}}}\n\\newcommand{\\ind}{\\mathbf 1}\n\\newcommand{\\R}{\\mathbb R}\n\\newcommand{\\E}{\\mathbf E}\n\\newcommand{\\var}{\\mathbf V}\n\\newcommand{\\ps}[2]{\\langle #1,#2\\rangle}\n\\newcommand{\\card}[1]{|{#1}|}\n\\newcommand{\\cov}{\\mathbf{cov}}\n\\newcommand{\\corr}{\\text{corr}}\n\\newcommand{\\AUC}{\\text{AUC}}\n\\newcommand{\\logit}{\\text{logit}}\n\\newcommand{\\li}[2]{#1_{#2}}\n\\newcommand{\\ssli}[2]{#1_{(#2)}}\n\\newcommand{\\HH}{{\\mathcal{H}}}\n\\newcommand{\\Hz}{{\\mathrm{H_0}}}\n\\newcommand{\\Hu}{{\\mathrm{H_1}}}\n\\newcommand{\\NO}{{\\mathcal{N}}}\n\n\\usepackage{nicematrix}\n\\usepackage{blkarray}\n:::\n:::\n\n\n::: {#exr-2-1 name=\"Questions de cours\"}\nA, A, B, B, B, C.\n:::\n\n::: {#exr-2-2 name=\"Covariance de $\\hat\\varepsilon$ et $\\hat Y$\"}\nNous allons montrer que, pour tout autre estimateur $\\tilde{\\beta}$ de \n$\\beta$ linéaire et sans biais, $\\V (\\tilde{\\beta})  \\geq \\V (\\hat \\beta)$.\nDécomposons la variance de $\\tilde{\\beta}$\n\\begin{eqnarray*}\n\\V (\\tilde{\\beta})  = \\V (\\tilde{\\beta} - \\hat \\beta+\\hat \\beta)\n=\\V (\\tilde{\\beta} - \\hat \\beta)+\\V (\\hat \\beta) -\n2 \\C(\\tilde{\\beta} - \\hat \\beta,\\hat \\beta).\n\\end{eqnarray*}\nLes variances étant définies positives, si nous montrons que \n$\\C(\\tilde{\\beta}- \\hat \\beta,\\hat \\beta)=0$, nous aurons \nfini la démonstration.\\\\\nPuisque $\\tilde{\\beta}$ est linéaire, $\\tilde{\\beta} = A Y$. \nDe plus, nous savons qu'il est sans biais, c'est-à-dire \n$\\E (\\tilde{\\beta}) = \\beta$ pour tout $\\beta$, donc $A X = I$. \nLa covariance devient :\n\\begin{eqnarray*}\n\\C(\\tilde{\\beta} - \\hat \\beta,\\hat \\beta) &=& \n\\C(A Y,(X'X)^{-1}X'Y) - \\V (\\hat \\beta)\\\\\n&=& \\sigma^2 A X (X'X)^{-1} - \\sigma^2 (X'X)^{-1}=0.\n\\end{eqnarray*}\n:::\n\n::: {#exr-2-3 name=\"Théorème de Gauss Markov\"}\nNous devons montrer que, parmi tous les estimateurs linéaires \nsans biais, l'estimateur de MC est celui qui a la plus petite \nvariance. La linéarité de $\\hat \\beta$ est évidente. Calculons sa variance :\n\\begin{eqnarray*}\n\\V (\\hat \\beta) = \\V ((X'X)^{-1}X'Y) = \n(X'X)^{-1}X'\\V(Y)X(X'X)^{-1}=\\sigma^2 (X'X)^{-1}.\n\\end{eqnarray*}\nNous allons montrer que, pour tout autre estimateur $\\tilde{\\beta}$ de \n$\\beta$ linéaire et sans biais, $\\V (\\tilde{\\beta})  \\geq \\V (\\hat \\beta)$.\nDécomposons la variance de $\\tilde{\\beta}$\n\\begin{eqnarray*}\n\\V (\\tilde{\\beta})  = \\V (\\tilde{\\beta} - \\hat \\beta+\\hat \\beta)\n=\\V (\\tilde{\\beta} - \\hat \\beta)+\\V (\\hat \\beta) -\n2 \\C(\\tilde{\\beta} - \\hat \\beta,\\hat \\beta).\n\\end{eqnarray*}\nLes variances étant définies positives, si nous montrons que \n$\\C(\\tilde{\\beta}- \\hat \\beta,\\hat \\beta)=0$, nous aurons \nfini la démonstration.\\\\\nPuisque $\\tilde{\\beta}$ est linéaire, $\\tilde{\\beta} = A Y$. \nDe plus, nous savons qu'il est sans biais, c'est-à-dire \n$\\E (\\tilde{\\beta}) = \\beta$ pour tout $\\beta$, donc $A X = I$. \nLa covariance devient :\n\\begin{eqnarray*}\n\\C(\\tilde{\\beta} - \\hat \\beta,\\hat \\beta) &=& \n\\C(A Y,(X'X)^{-1}X'Y) - \\V (\\hat \\beta)\\\\\n&=& \\sigma^2 A X (X'X)^{-1} - \\sigma^2 (X'X)^{-1}=0.\n\\end{eqnarray*}\n:::\n\n\n::: {#exr-2-4 name=\"Représentation des variables\"}\nNous représentons les données dans $\\R^2$ pour le premier \njeu et dans $\\R^3$ pour le second.\n\n![](FIGURES/exo2-4.png){width='75%' fig-align=\"center\"}\n\nDans le premier modèle, nous projetons $Y$ sur l'espace \nengendré par $X$, soit la droite de vecteur directeur $\\overrightarrow{OX}$.\nNous trouvons par le calcul $\\hat \\beta = 1.47$, résultat \nque nous aurions pu trouver graphiquement car $\\overrightarrow{O \\hat Y}=\n\\hat \\beta . \\overrightarrow{OX}$.\n\nConsidérons $\\R^3$ muni de la base orthonormée\n$(\\vec{i},\\vec{j},\\vec{k})$.  Les vecteurs $\\overrightarrow{OX}$ et\n$\\overrightarrow{OZ}$ engendrent le même plan que celui engendré par\n$(\\vec{i},\\vec{j})$. La projection de $Y$ sur ce plan donne\n$\\overrightarrow{O \\hat Y}$. Il est quasiment impossible de trouver\n$\\hat \\beta$ et $\\hat \\gamma$ graphiquement mais nous trouvons par le\ncalcul $\\hat \\beta = -3.33$ et $\\hat \\gamma =5$.\n\n:::\n\n::: {#exr-2-5 name=\"Modèles emboîtés\"}\nNous obtenons\n\\begin{eqnarray*}\n\\hat Y_p = X \\hat \\beta \\quad  \\hbox{et} \\quad \\hat Y_q= X_q \\hat \\gamma.\n\\end{eqnarray*}\nPar définition du $\\leR$, il faut comparer la norme au carré des\nvecteurs $\\hat Y_p$ et $\\hat Y_q$. Notons les espaces engendrés\npar les colonnes de $X_q$ et $X$, $\\M_{X_q}$ et $\\M_{X}$, nous avons \n$\\M_{X_q} \\subset  \\M_{X}$. Nous obtenons alors \n\\begin{eqnarray*}\n\\hat Y_p = P_{X_p}Y \n= (P_{X_q} + P_{X^{\\perp}_q})P_{X_p}Y &=& P_{X_q}P_{X_p}Y + P_{X^{\\perp}_q}P_{X_p}Y\\\\\n&=& P_{X_q}Y + P_{X^{\\perp}_q \\cap X_p} Y\\\\\n&=& \\hat Y_q + P_{X^{\\perp}_q \\cap X_p} Y.\n\\end{eqnarray*}\nEn utilisant le théorème de Pythagore, nous avons\n\\begin{eqnarray*}\n\\| \\hat Y_p \\|^2 &=& \\|\\hat Y_q \\|^2 + \\| P_{X^{\\perp}_q \\cap X_p} Y \\|^2 \n\\geq \\|\\hat Y_q \\|^2,\n\\end{eqnarray*}\nd'où\n\\begin{eqnarray*}\n\\leR(p)=\\frac{\\| \\hat Y_p \\|^2}{\\| Y \\|^2} \\geq\n\\frac{\\| \\hat Y_q \\|^2}{\\| Y \\|^2} =\\leR(q).\n\\end{eqnarray*}\n\nEn conclusion, lorsque les modèles sont emboîtés $\\M_{X_q} \\subset  \\M_{X}$, \nle $\\leR$ du modèle le plus grand (ayant le plus de variables) sera\ntoujours plus grand que le $\\leR$ du modèle le plus petit.\n:::\n\n::: {#exr-2-6}\nLa matrice $X'X$ est symétrique, $n$ vaut 30 et $\\bar x= \\bar z=0$.\nLe coefficient de corrélation\n\\begin{equation*}\n\\rho_{x,z} = \\frac{\\sum_{i=1}^{30} (x_i -\\bar x)(z_i - \\bar z)}\n{\\sqrt{\\sum_{i=1}^{30} (x_i -\\bar x)^2\\sum_{i=1}^{30} (z_i - \\bar z)^2}}\n=\\frac{\\sum_{i=1}^{30} x_i z_i}\n{\\sqrt{\\sum_{i=1}^{30} x_i^2 \\sum_{i=1}^{30} z_i^2}}\n=\\frac{7}{\\sqrt{150}}=0.57.\n\\end{equation*}\nNous avons \n\\begin{eqnarray*}\ny_i &=& -2 +x_i+z_i+\\hat \\varepsilon_i\n\\end{eqnarray*}\net la moyenne vaut alors\n\\begin{eqnarray*}\n\\bar y &=& -2 + \\bar x +\\bar z + \\frac{1}{n}\\sum_i \\hat \\varepsilon_i.\n\\end{eqnarray*}\nLa constante étant dans le modèle, la somme des résidus est\nnulle car le vecteur $\\hat \\varepsilon$ est orthogonal au \nvecteur $\\1$. Nous obtenons donc que la moyenne de $Y$ vaut 2 \ncar $\\bar x=0$ et $\\bar z=0$.\nNous obtenons en développant\n\\begin{eqnarray*}\n\\|\\hat Y \\|^2 &=& \\sum_{i=1}^{30}(-2+x_i+2z_i)^2\\\\\n&=& 4+10+60+14=88.\n\\end{eqnarray*}\nPar le théorème de Pythagore, nous concluons que \n\\begin{eqnarray*}\n\\SCT=\\SCE+\\SCR=88+12=100.\n\\end{eqnarray*}\n\n:::\n\n::: {#exr-2-7 name=\"Changement d'échelle des variables explicatives\"}\nNous avons l'estimation sur le modèle avec les variables originales qui minimise\n\\begin{align*}\n\\MCO(\\beta)&=\\|Y - \\sum_{j=1}^{p} X_j \\beta_j \\|^{2}\n\\end{align*}\nCette solution est notée $\\hat \\beta$.\n\nNous avons l'estimation sur le modèle avec les variables prémultipliées par $a_{j}$ (changement d'échelle) qui minimise\n\n$$   \n\\begin{align*}\n\\tilde{\\MCO}(\\beta)&=\\|Y - \\sum_{j=1}^{p} \\tilde X_j \\beta_j \\|^{2} = \\|Y - \\sum_{j=1}^{p} a_{j} X_j \\beta_j \\|^{2}\\\\\n&= \\|Y - \\sum_{j=1}^{p} X_j \\gamma_j \\|^{2}=\\MCO(\\gamma),\n\\end{align*}\n$$\n\nen posant en dernière ligne $\\gamma_{j}=a_{j} \\beta_j$. La solution\nde de $\\MCO(\\gamma)$ (ou encore $\\MCO(\\beta)$) est $\\hat \\beta$. La\nsolution de $\\tilde{\\MCO}(\\beta)$ est alors donnée par\n$\\hat \\beta_{j}=a_{j} \\tilde \\beta_j$.\n:::\n\n::: {#exr-2-8 name=\"Différence entre régression multiple et régressions simples\"}\n1.  Calculons l'estimateur des MCO noté traditionnellement $(X'X)^{-1}X'Y$ avec la matrice $X$ qui possède ici deux colonnes (notées ici $X$ et $Z$) et $n$ lignes. On a donc\n    \\begin{align*}\n      (X'X)&=\n             \\begin{pmatrix}\n               \\|X\\|^{2} & <X,Z>\\\\\n               <X,Z> & \\|Z\\|^{2} \\\\\n             \\end{pmatrix}\n    \\end{align*}\n    Son déterminant est $\\Delta= \\|X\\|^{2}\\|Z\\|^{2} - 2 <X,Z>$ et son inverse est\n    \\begin{align*}\n      \\frac{1}{\\Delta}\n      \\begin{pmatrix}\n               \\|Z\\|^{2} & -<X,Z>\\\\\n               -<X,Z> & \\|X\\|^{2} \\\\\n      \\end{pmatrix}\n    \\end{align*}\n    Ensuite $X'Y$ est simplement le vecteur colonne de coordonnées $<X,Y>$ et  $<Z,Y>$. En rassemblant le tout nous avons\n    \\begin{align*}\n      \\hat \\beta_{1}&=\\frac{1}{\\Delta}(\\|Z\\|^{2} <X,Y> - <X,Z><Z,Y>),\\\\\n      \\hat \\beta_{2}&=\\frac{1}{\\Delta}(\\|X\\|^{2} <Z,Y> - <X,Z><X,Y>).\n    \\end{align*}\n    Si $<X,Z>=0$ (les deux vecteurs sont orthogonaux) alors cette écriture se simplifie en\n    $$\n      \\hat \\beta_{1}=\\frac{<X,Y>}{\\|X\\|^{2}},\\quad\n      \\hat \\beta_{2}=\\frac{<Z,Y>}{\\|Z\\|^{2}}.\n    $$\n    \n2.  Calculons l'estimateur des MCO noté traditionnellement $(X'X)^{-1}X'Y$ avec la matrice $X$ qui possède ici une colonne (notée ici $X$) et $n$ lignes. On a donc\n    \\begin{align*}\n      \\hat \\beta_{X} = \\frac{<X,Y>}{\\|X\\|^{2}}.\n    \\end{align*}\n    Passons maintenant à la matrice qui possède ici une colonne (notée ici $Z$) et $n$ lignes. On a donc\n    \\begin{align*}\n      \\hat \\beta_{Z} = \\frac{<Z,Y>}{\\|Z\\|^{2}}.\n    \\end{align*}\n    \n3.  En général les coefficients des régressions simples ne sont pas ceux obtenus par régression multiple sauf si les variables sont orthogonales.\n\n4.  Nous avons ici les résidus de la première régression qui sont\n    \\begin{align}\n      \\hat \\varepsilon = Y - \\hat \\beta_{X} X.\n    \\end{align}\n    La deuxième régression (sur les résidus) donne le coefficient\n    \\begin{align*}\n      \\hat \\beta_{Z} = \\frac{<Z,\\hat \\varepsilon>}{\\|Z\\|^{2}} = \\hat \\beta_{Z} - \\hat \\beta_{X}\\frac{<Z,X>}{\\|Z\\|^{2}}\n    \\end{align*}\nLa régression séquentielle donne des coefficients différents des régressions univariées ou bivariées sauf si les variables sont orthogonales.\n  \\end{enumerate}\n\n:::\n\n::: {#exr-2-9 name=\"TP : différence entre régression multiple et régressions simples\"}\n1.  Simulons 2 variables explicatives avec GNU-R pour $n=100$ individus selon selon deux loi uniforme $[0,1]$ :\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    n <- 100\n    set.seed(4321) # pour fixer les simulations\n    X1 <- runif(n)\n    X2 <- runif(n)\n    ```\n    :::\n\n\n    Ensuite simulons $Y$ selon le modèle avec $\\sigma=0.5$\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    sigma <- 0.5\n    set.seed(321) # pour fixer les simulations\n    Y <- 2 -3*X1 + 4*X2  + rnorm(n, sd=sigma)\n    don <- cbind.data.frame(Y,X1,X2)\n    head(don)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n             Y         X1        X2\n    1 4.213477 0.33477802 0.5913398\n    2 1.602748 0.90913948 0.6715464\n    3 2.958646 0.41152969 0.5830570\n    4 3.215113 0.04384097 0.3516151\n    5 3.367004 0.76350011 0.9298713\n    6 3.555247 0.75043889 0.9181180\n    ```\n    :::\n    :::\n\n\n2.  Le graphique est obtenu avec\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    rgl::plot3d(X1,X2,Y)\n    ```\n    :::\n\n\n3.  On observe grâce au code ci-dessus que les points sont répartis autour d'un plan (d'équation $z=2-3x + 4y$).\n\n4.  Effectuons la régression multiple et stockons $\\hat Y$ dans `Yhat` :\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    (regmult <- lm(Y~1+X1+X2, data=don))\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    \n    Call:\n    lm(formula = Y ~ 1 + X1 + X2, data = don)\n    \n    Coefficients:\n    (Intercept)           X1           X2  \n          2.080       -2.972        3.827  \n    ```\n    :::\n    :::\n\n\n    Nous sommes assez proches des coefficients recherchés (2, -3 et 4)\n5.  Effectuons les régression simples\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    (regX1 <- lm(Y~1+X1, data=don))\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    \n    Call:\n    lm(formula = Y ~ 1 + X1, data = don)\n    \n    Coefficients:\n    (Intercept)           X1  \n          3.803       -2.441  \n    ```\n    :::\n    :::\n\n\n    Nous voyons que le paramètre ne correspond pas à celui de $X_{1}$ dans la régression multiple.\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    (regX2 <- lm(Y~1+X2, data=don))\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    \n    Call:\n    lm(formula = Y ~ 1 + X2, data = don)\n    \n    Coefficients:\n    (Intercept)           X2  \n         0.7221       3.4424  \n    ```\n    :::\n    :::\n\n\n    Nous voyons que le paramètre ne correspond pas à celui de $X_{2}$ dans la régression multiple.\n\n6.  Enchainons après la régression simple sur $X_{1}$ la régression des résidus (ce que $X_{1}$ n'a pas réussi à modéliser) sur $X_{2}$:\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    residX1 <- residuals(regX1)\n    don <- cbind(don, residX1)\n    regX2residX1 <- lm(residX1~1+X2, data=don)\n    regX2residX1\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    \n    Call:\n    lm(formula = residX1 ~ 1 + X2, data = don)\n    \n    Coefficients:\n    (Intercept)           X2  \n         -1.965        3.758  \n    ```\n    :::\n    :::\n\n    \n    Là encore le coefficient sur $X_{2}$ ne correspond pas à celui de $X_{2}$  dans la régression multiple. :La prévision est différente\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    (predict(regX2residX1) + predict(regX1))[1:5]\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n           1        2        3        4        5 \n    3.242781 2.142450 3.024335 3.051894 3.468733 \n    ```\n    :::\n    :::\n\n\n6.  Changeons l'ordre\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    residX2 <- residuals(regX2)\n    don <- cbind(don, residX2)\n    regX1residX2 <- lm(residX2~1+X1, data=don)\n    regX1residX2\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    \n    Call:\n    lm(formula = residX2 ~ 1 + X1, data = don)\n    \n    Coefficients:\n    (Intercept)           X1  \n          1.531       -2.918  \n    ```\n    :::\n    :::\n\n\n    On a là encore des différences\n    \n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    (predict(regX1residX2) + predict(regX2))[1:5]\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n           1        2        3        4        5 \n    3.311895 1.911850 3.059400 3.335703 3.226125 \n    ```\n    :::\n    :::\n\n    \n7.  Envisageons la covariance empirique\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    cov(X1,X2)    \n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    [1] 0.01050429\n    ```\n    :::\n    :::\n\n    \n    et leur produit scalaire\n    \n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    sum(X1*X2)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    [1] 28.47696\n    ```\n    :::\n    :::\n\n   \n    Les deux variables sont loin d'être orthogonales. Centrons les\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    X1c <- X1 - mean(X1)\n    X2c <- X2 - mean(X2)\n    sum(X1c*X2c) \n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    [1] 1.039925\n    ```\n    :::\n    :::\n\n    \n    Refaisons les régressions: la multiple\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    donc <- cbind.data.frame(Y,X1c,X2c)\n    (regmultc <- lm(Y~1+X1c+X2c, data=donc))\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    \n    Call:\n    lm(formula = Y ~ 1 + X1c + X2c, data = donc)\n    \n    Coefficients:\n    (Intercept)          X1c          X2c  \n          2.522       -2.972        3.827  \n    ```\n    :::\n    :::\n\n\n    Nous retrouvons que seul la constante change puis l'enchainement\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    (regX1c <- lm(Y~1+X1c, data=donc))\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    \n    Call:\n    lm(formula = Y ~ 1 + X1c, data = donc)\n    \n    Coefficients:\n    (Intercept)          X1c  \n          2.522       -2.441  \n    ```\n    :::\n    :::\n\n    \n    Ici les variables $X_{1c}$ et $X_{2c}$ étant centrées elles sont orthogonales à la constante. Nous avons donc\n        \\begin{align}\n          \\Im(\\un, X_{1c}, X_{2c}) = \\Im(\\un) \\stackrel{\\perp}{\\oplus} \\Im(X_{1c}, X_{2c})\n        \\end{align}\n    Le modèle précédent nous donne grâce à l'orthogonalité ci-dessus $\\hat Y_{1c} = P_{\\un, X_{1c}}Y=P_{\\un}Y + P_{X_{1c}}Y$. Le modèle complet donne de son côté $\\hat Y = P_{\\un, X_{1c},X_{2c}}Y=P_{\\un}Y + P_{X_{1c},X_{2c}}Y$. Nous retrouvons donc le coefficient constant qui est la coordonnée sur $\\un$ de $P_{\\un}Y$ dans les deux cas.\n      \n        \n    Enchainons sur la régression sur résidus\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    residX1c <- residuals(regX1c)\n    donc <- cbind(donc, residX1c)\n    (regX2residX1c <- lm(residX1c~1+X2c, data=don))\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    \n    Call:\n    lm(formula = residX1c ~ 1 + X2c, data = don)\n    \n    Coefficients:\n    (Intercept)          X2c  \n      1.669e-16    3.758e+00  \n    ```\n    :::\n    :::\n\n\n    Puisque le vecteur \\code{residX1c} est $Y- \\hat Y_{1}=Y-P_{\\un}Y-P_{X_{1c}}Y$ et que $X_{2c}\\perp\\un$ on a la projection des résidus sur $\\Im(\\un, X_{2c})$ qui vaut\n    \\begin{align*}\n      P_{\\un, X_{2c}}(Y- \\hat Y_{1c})&= P_{\\un}(Y- \\hat Y_{1c}) + P_{X_{2c}}(Y- \\hat Y_{1c})\\\\\n      &=P_{\\un}Y - P_{\\un}P_{\\un, X_{1c}}Y + P_{X_{2c}}Y -  P_{X_{2c}}P_{\\un, X_{1c}}Y\\\\\n      &= P_{\\un}Y - P_{\\un}Y - 0 + P_{X_{2c}}Y - 0 - P_{X_{2c}}P_{X_{1c}}Y\\\\\n      &= P_{X_{2c}} Y - P_{X_{2c}}P_{X_{1c}}Y\n    \\end{align*}\n    Cette dernière somme de projection est dans $\\Im(X_{1c}, X_{2c})$ qui est un sous-espace orthogonal à $\\Im(\\un)$, d'où le coefficient 0 pour la constante.\n\n    Pour les prévisions rien ne change :\n    \n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    predict(regmultc)[1:5]\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n           1        2        3        4        5 \n    3.348336 1.948483 3.088559 3.295485 3.369866 \n    ```\n    :::\n    :::\n\n    \n\n\n:::\n\n\n::: {#exr-2-10 name=\"TP : régression multiple et code R\"}\n\n1.  \n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    ozone <- read.table(\"../donnees/ozone_complet.txt\",sep=\";\",header=TRUE)\n    nomvar <- names(ozone)\n    ```\n    :::\n\n\n2.  \n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    (ch <- paste(nomvar[-1],collapse = \"+\"))\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    [1] \"T6+T9+T12+T15+T18+Ne6+Ne9+Ne12+Ne15+Ne18+Vdir6+Vvit6+Vdir9+Vvit9+Vdir12+Vvit12+Vdir15+Vvit15+Vdir18+Vvit18+Vx+maxO3v\"\n    ```\n    :::\n    :::\n\n\n3.  \n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    (ch2 <- paste(\"maxO3~\",ch,sep=\"\"))\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    [1] \"maxO3~T6+T9+T12+T15+T18+Ne6+Ne9+Ne12+Ne15+Ne18+Vdir6+Vvit6+Vdir9+Vvit9+Vdir12+Vvit12+Vdir15+Vvit15+Vdir18+Vvit18+Vx+maxO3v\"\n    ```\n    :::\n    :::\n\n\n4.  \n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    form <- formula(ch2)\n    lm(form,data=ozone)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    \n    Call:\n    lm(formula = form, data = ozone)\n    \n    Coefficients:\n    (Intercept)           T6           T9          T12          T15          T18  \n      31.628418    -1.937194     0.121834     1.522143     0.609047     0.091871  \n            Ne6          Ne9         Ne12         Ne15         Ne18        Vdir6  \n       0.091969    -0.692891    -0.941925    -0.033847    -0.107068    -0.001344  \n          Vvit6        Vdir9        Vvit9       Vdir12       Vvit12       Vdir15  \n       1.586427    -0.009728    -1.163728    -0.007144     0.182232    -0.003468  \n         Vvit15       Vdir18       Vvit18           Vx       maxO3v  \n       0.246885     0.006251     0.560660     0.255474     0.465541  \n    ```\n    :::\n    :::\n\n\n:::\n\n::: {#exr-2-11 name=\"Régression orthogonale\"}\nLes vecteurs étant orthogonaux, nous avons \n$\\M_X = \\M_U \\stackrel{\\perp}{\\oplus} \\M_V$. Nous pouvons \nalors écrire\n\\begin{eqnarray*}\n\\hat Y_X = P_X Y &=& (P_U + P_{U^{\\perp}})P_X Y \\\\\n&=& P_U P_X Y + P_{U^{\\perp}}P_X Y =  P_U Y + P_{U^{\\perp}\\cap X} Y \\\\\n&=& \\hat Y_U + \\hat Y_V.\n\\end{eqnarray*}\nLa suite de l'exercice est identique. En conclusion, effectuer une régression \nmultiple sur des variables orthogonales revient à effectuer $p$ \nrégressions simples.\n:::\n\n::: {#exr-2-12 name=\"Centrage, centrage-réduction et coefficient constant\"}\n1.  Comme la dernière colonne de $X$, notée $X_p$ vaut $\\un_n$ sa moyenne empirique vaut $1$ et la variable centrée issue de $X_p$ est donc $X_p -1\\times\\un_n=\\boldsymbol{0}_n$.\n\n2.  Nous avons le modèle sur variable centrée\n\\begin{eqnarray*}\n\\tilde Y&=&\\tilde X\\tilde \\beta+ \\varepsilon\\\\\nY-\\bar Y\\un_n&=&\\sum_{j=1}^{p-1}{(X_j -\\bar X_j\\un_n)\\tilde\\beta_j}+\\varepsilon\\\\\nY&=&\\sum_{j=1}^{p-1}{\\tilde\\beta_j X_j}+ \\Bigl(\\bar Y -\\sum_{j=1}^{p-1}{\\bar X_j\\tilde\\beta_j}\\Bigr)\\un_n+\\varepsilon.\n\\end{eqnarray*}\nEn identifiant cela donne\n$$\n\\begin{eqnarray}\n\\beta_j&=&\\tilde\\beta_j,\\ \\forall j\\in\\{1,\\dotsc,p-1\\},\\nonumber\\\\\n\\beta_p&=&\\bar Y\\un_n-\\sum_{j=1}^{p-1}{\\bar X_j\\tilde\\beta_j}.\n\\end{eqnarray}\n$${#eq-coefct}\nSi l'on utilise des variables centrées dans le modèle de régression,\non ne met pas de colonne $\\un$ (pas de coefficient constant -\n*intercept*). Les coefficients du modèle sur les variables\noriginales sont égaux à ceux sur les variables centrées et le\ncoefficient constant est donné par la formule (@eq-coefct}).\n\n3.  Maintenant les variables explicatives sont centrées et réduites~:\n\\begin{eqnarray*}\n\\tilde Y&=&\\tilde X\\tilde \\beta+ \\varepsilon\\\\\nY-\\bar Y\\un_n&=&\\sum_{j=1}^{p-1}{\\frac{(X_j -\\bar X_j\\un_n)}{\\hat\\sigma_{X_j}}\\tilde\\beta_j}+\\varepsilon\\\\\nY&=&\\sum_{j=1}^{p-1}{\\frac{\\tilde\\beta_j}{\\hat\\sigma_{X_j}}X_j}+ \\Bigl(\\bar Y-\\sum_{j=1}^{p-1}{\\bar X_j\\frac{\\tilde\\beta_j}{\\hat\\sigma_{X_j}}}\\Bigr)\\un_n+\\varepsilon.\n\\end{eqnarray*}\nEn identifiant cela donne\n\\begin{eqnarray*}\n\\beta_j&=&\\frac{\\tilde\\beta_j}{\\hat\\sigma_{X_j}},\\ \\forall j\\in\\{1,\\dotsc,p-1\\},\\\\\n\\beta_p&=&\\bar Y\\un_n-\\sum_{j=1}^{p-1}{\\bar X_j\\frac{\\tilde\\beta_j}{\\hat\\sigma_{X_j}}}.\n\\end{eqnarray*}\n    Nous obtenons ici que les coefficients du modèle sur les\nvariables originales sont égaux à ceux sur les variables centrées-réduites divisés par l'écart-type empirique des variables explicatives. Plus la variable explicative $X_j$ est dispersée, plus son coefficient $\\beta_j$ sera réduit par rapport à $\\tilde\\beta_j$. Le coefficient constant est donné par la formule ci-dessus.\n\n4.  La variable à expliquer $Y$ est elle aussi centrée-réduite~:\n\\begin{eqnarray*}\n\\tilde Y&=&\\tilde X\\tilde \\beta+ \\tilde\\varepsilon\\\\\n\\frac{Y-\\bar Y\\un_n}{\\hat\\sigma_Y}&=&\\sum_{j=1}^{p-1}{\\frac{(X_j -\\bar X_j\\un_n)}{\\hat\\sigma_{X_j}}\\tilde\\beta_j}+\\tilde\\varepsilon\\\\\nY&=&\\hat\\sigma_Y\\sum_{j=1}^{p-1}{\\frac{\\tilde\\beta_j}{\\hat\\sigma_{X_j}}X_j}+ \\Bigl(\\bar Y-\\hat\\sigma_Y\\sum_{j=1}^{p-1}{\\bar X_j\\frac{\\tilde\\beta_j}{\\hat\\sigma_{X_j}}}\\Bigr)\\un_n+\\hat\\sigma_Y\\tilde\\varepsilon.\n\\end{eqnarray*}\n    En identifiant cela donne\n\\begin{eqnarray*}\n\\beta_j&=&\\hat\\sigma_Y\\frac{\\tilde\\beta_j}{\\hat\\sigma_{X_j}},\\ \\forall j\\in\\{1,\\dotsc,p-1\\},\\\\\n\\beta_p&=&\\bar Y\\un_n-\\hat\\sigma_Y\\sum_{j=1}^{p-1}{\\bar X_j\\frac{\\tilde\\beta_j}{\\hat\\sigma_{X_j}}},\\\\\n\\varepsilon&=&\\hat\\sigma_Y\\tilde\\varepsilon.\n\\end{eqnarray*}\n    L'écart-type empirique de $Y$ entre en jeu et nous constatons que les\nrésidus du modèle \\og centré-réduit\\fg~ sont égaux à ceux initiaux divisés\npar l'écart-type empirique de $Y$.\n\n:::\n\n::: {#exr-2-13 name=\"Moindres carrés contraints\"}\n\n:::\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}