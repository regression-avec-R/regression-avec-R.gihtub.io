[
  {
    "objectID": "correction/chap11.html",
    "href": "correction/chap11.html",
    "title": "11 Régression logistique",
    "section": "",
    "text": "Exercice 1 (Question de cours)  \n\nA\nA\nB\nA\nA\nA\nB\nA\n\n\n\nExercice 2 (Interprétation des coefficients)  \n\nOn génère l’échantillon.\n\nn &lt;- 100\nset.seed(48967365)\nX &lt;- sample(c(\"A\",\"B\",\"C\"),100,replace=TRUE)\nY &lt;- rep(0,n)\nset.seed(487365)\nY[X==\"A\"] &lt;- rbinom(sum(X==\"A\"),size=1,prob=0.95)\nset.seed(4878365)\nY[X==\"B\"] &lt;- rbinom(sum(X==\"B\"),size=1,prob=0.95)\nset.seed(4653965)\nY[X==\"C\"] &lt;- rbinom(sum(X==\"C\"),size=1,prob=0.05)\nY &lt;- factor(Y)\ndonnees&lt;-data.frame(Y,X)\n\nOn ajuste le modèle avec les contraintes par défaut.\n\nmodel1 &lt;- glm(Y~X,data=donnees,family=binomial)\nsummary(model1)\n\n\nCall:\nglm(formula = Y ~ X, family = binomial, data = donnees)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   2.2336     0.6075   3.677 0.000236 ***\nXB            0.6568     0.9470   0.694 0.487977    \nXC           -5.6348     1.1842  -4.758 1.95e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 129.489  on 99  degrees of freedom\nResidual deviance:  44.218  on 97  degrees of freedom\nAIC: 50.218\n\nNumber of Fisher Scoring iterations: 6\n\n\nOn obtient les résultats du test de Wald sur la nullité des paramètres \\(\\beta_0,\\beta_2\\) et \\(\\beta_3\\).\nOn change la modalité de référence.\n\nmodel2 &lt;- glm(Y~C(X,base=3),data=donnees,family=binomial)\nsummary(model2)\n\n\nCall:\nglm(formula = Y ~ C(X, base = 3), family = binomial, data = donnees)\n\nCoefficients:\n                Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)       -3.401      1.017  -3.346  0.00082 ***\nC(X, base = 3)A    5.635      1.184   4.758 1.95e-06 ***\nC(X, base = 3)B    6.292      1.249   5.035 4.77e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 129.489  on 99  degrees of freedom\nResidual deviance:  44.218  on 97  degrees of freedom\nAIC: 50.218\n\nNumber of Fisher Scoring iterations: 6\n\n\nOn obtient les résultats du test de Wald sur la nullité des paramètres \\(\\beta_0,\\beta_1\\) et \\(\\beta_2\\).\nOn remarque que dans model1 on accepte la nullité de \\(\\beta_2\\) alors qu’on la rejette dans model2. Ceci est logique dans la mesure où ces tests dépendent de la contrainte identifiante choisie. Dans model1 le test de nullité de \\(\\beta_2\\) permet de vérifier si \\(B\\) à un effet similaire à \\(A\\) sur \\(Y\\). Dans model2, on compare l’effet de \\(B\\) à celui de \\(C\\). On peut donc conclure \\(A\\) et \\(B\\) ont des effets proches sur \\(Y\\) alors que \\(B\\) et \\(C\\) ont un impact différent. Ceci est logique vu la façon dont les données ont été générées.\nTester l’effet global de \\(X\\) sur \\(Y\\) revient à tester si les coefficients \\(\\beta_1,\\beta_2\\) et \\(\\beta_3\\) sont égaux, ce qui, compte tenu des contraintes revient à considérer les hypothèses nulles :\n\n\\(\\beta_2=\\beta_3=0\\) dans model1 ;\n\\(\\beta_1=\\beta_2=0\\) dans model2.\n\nOn peut effectuer les tests de Wald ou du rapport de vraisemblance. On obtient les résultats du rapport de vraisemblance avec :\n\nlibrary(car)\nAnova(model1,type=3,test.statistic=\"LR\")\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: Y\n  LR Chisq Df Pr(&gt;Chisq)    \nX   85.271  2  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nAnova(model2,type=3,test.statistic=\"LR\")\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: Y\n               LR Chisq Df Pr(&gt;Chisq)    \nC(X, base = 3)   85.271  2  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nOn remarque ici que ces deux tests sont identiques : ils ne dépendent pas de la contrainte identifiante choisie.\n\n\n\nExercice 3 (Séparabilité)  \n\nOn génère l’échantillon demandé.\n\nset.seed(1234)\nX &lt;- c(runif(50,-1,0),runif(50,0,1))\nset.seed(5678)\nY &lt;- c(rep(0,50),rep(1,50))\ndf &lt;- data.frame(X,Y)\n\nLe graphe s’obtient avec :\n\nbeta &lt;- seq(0,100,by=0.01)\nlog_vrais &lt;- function(X,Y,beta){\n  LV &lt;- rep(0,length(beta))\n  for (i in 1:length(beta)){\n    Pbeta &lt;- exp(beta[i]*X)/(1+exp(beta[i]*X))\n    LV[i] &lt;- sum(Y*X*beta[i]-log(1+exp(X*beta[i])))\n#    gradln[i] &lt;- t(Xb)%*%(Yb-Pbeta)\n  }\n  return(LV)\n}\nLL &lt;- log_vrais(X,Y,beta)\nplot(beta,LL,type=\"l\")\n\n\n\n\nOn obtient un avertissement qui nous dit que l’algorithme d’optimisation n’a pas convergé.\n\nmodel &lt;- glm(Y~X-1,data=df,family=\"binomial\")\nmodel$coef\n\n       X \n1999.371 \n\n\nLe changement proposé supprime la séparabilité des données. On obtient bien un maximum fini pour cette nouvelle vraisemblance.\n\nY1 &lt;- Y;Y1[1] &lt;- 1\nLL1 &lt;- log_vrais(X,Y1,beta)\nplot(beta,LL1,type=\"l\")\n\n\n\nmodel1 &lt;- glm(Y1~X-1,family=\"binomial\")\nmodel1$coef\n\n       X \n10.17868 \n\n\n\n\n\nExercice 4 (Matrice hessienne) Le gradient de la log-vraisemblance en \\(\\beta\\) est donné par \\(\\nabla \\mathcal L(Y,\\beta)=X'(Y-P_\\beta)\\). Sa \\(j\\)ème composante vaut \\[\\frac{\\partial\\mathcal L}{\\partial\\beta_j}(\\beta)=\\sum_{i=1}^nx_{ij}(y_i-p_\\beta(x_i)).\\]\nOn peut donc calculer la drivée par rapport à \\(\\beta_\\ell\\) : \\[\\begin{align*}\n\\frac{\\partial\\mathcal L}{\\partial\\beta_j\\partial\\beta_\\ell}(\\beta)= & \\frac{\\partial}{\\partial\\beta_\\ell}\\left[\n\\sum_{i=1}^nx_{ij}\\left(y_i-\\frac{\\exp(x_i'\\beta)}{1+\\exp(x_i'\\beta)}\\right)\\right] \\\\\n=& -\\sum_{i=1}^nx_{ij}x_{i\\ell}\\frac{\\exp(x_i'\\beta)}{[1+\\exp(x_i'\\beta)]^2} \\\\\n=& -\\sum_{i=1}^nx_{ij}x_{i\\ell}p_\\beta(x_i)(1-p_\\beta(x_i)).\n\\end{align*}\\] Matriciellement on déduit donc que la hessienne vaut \\[\\nabla^2\\mathcal L(Y,\\beta)=-X'W_\\beta X,\\] où \\(W_\\beta\\) est la matrice \\(n\\times n\\) diagonale dont le \\(i\\)ème terme de la diagonale vaut \\(p_\\beta(x_i)(1-p_\\beta(x_i))\\). Par ailleurs, comme pour tout \\(i=1,\\dots,n\\), on a \\(p_\\beta(x_i)(1-p_\\beta(x_i))&gt;0\\) et que \\(X\\) est de plein rang, on déduit que \\(X'W_\\beta X\\) est définie positive et par conséquent que la hessienne est définie négative.\n\n\nExercice 5 (Modèles avec R) On importe les données\n\npanne &lt;- read.table(\"panne.txt\",header=T)\nhead(panne)\n\n  etat age marque\n1    0   4      A\n2    0   2      C\n3    0   3      C\n4    0   9      B\n5    0   7      B\n6    0   6      A\n\n\n\nLa commande\n\nmodel &lt;- glm(etat~.,data=panne,family=binomial)\nmodel\n\n\nCall:  glm(formula = etat ~ ., family = binomial, data = panne)\n\nCoefficients:\n(Intercept)          age      marqueB      marqueC  \n    0.47808      0.01388     -0.41941     -1.45608  \n\nDegrees of Freedom: 32 Total (i.e. Null);  29 Residual\nNull Deviance:      45.72 \nResidual Deviance: 43.5     AIC: 51.5\n\n\najuste le modèle \\[\\log\\left(\\frac{p_\\beta(x)}{1-p_\\beta(x)}\\right)=\\beta_0+\\beta_1x_1+\\beta_2\\mathsf{1}_{x_2=B}+\\beta_3\\mathsf{1}_{x_2=C}\\] où \\(x_1\\) et \\(x_2\\) désigne respectivement les variables age et marque. On obtient les estimateurs avec\n\ncoef(model)\n\n(Intercept)         age     marqueB     marqueC \n 0.47808311  0.01388395 -0.41941071 -1.45608147 \n\n\nIl s’agit des tests de Wald pour tester l’effet des variables dans le modèle. Pour l’effet de marque, on va par exemple tester \\[H_0:\\beta_2=\\beta_3=0\\quad\\text{contre}\\quad H_1:\\beta_2\\neq 0\\text{ ou }\\beta_3\\neq 0.\\] Sous \\(H_0\\) la statistique de Wald suit une loi du \\(\\chi^2\\) à 4-2=2 degrés de liberté. Pour le test de la variable age le nombre de degrés de liberté manquant est 1. On retrouve cela dans la sortie\n\nlibrary(car)\nAnova(model,type=3,test.statistic=\"Wald\")\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: etat\n            Df  Chisq Pr(&gt;Chisq)\n(Intercept)  1 0.3294     0.5660\nage          1 0.0218     0.8826\nmarque       2 1.9307     0.3809\n\n\nIl s’agit cette fois du test du rapport de vraisemblance. Les degrés de liberté manquants sont identiques.\n\nAnova(model,type=3,test.statistic=\"LR\")\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: etat\n       LR Chisq Df Pr(&gt;Chisq)\nage     0.02189  1     0.8824\nmarque  2.09562  2     0.3507\n\n\n\nLe modèle s’écrit \\[\\log\\left(\\frac{p_\\beta(x)}{1-p_\\beta(x)}\\right)=\\beta_0+\\beta_1\\mathsf{1}_{x_2=A}+\\beta_2\\mathsf{1}_{x_2=B}.\\]\nLe modèle ajusté ici est \\[\\log\\left(\\frac{p_\\beta(x)}{1-p_\\beta(x)}\\right)=\\gamma_0+\\gamma_1\\mathsf{1}_{x_2=B}+\\gamma_2\\mathsf{1}_{x_2=C}.\\] Par identification on a \\[\\begin{cases}\n\\beta_0+\\beta_1=\\gamma_0 \\\\\n\\beta_0+\\beta_2=\\gamma_0+\\gamma_1 \\\\\n\\beta_0=\\gamma_0+\\gamma_2 \\\\\n\\end{cases}\n\\Longleftrightarrow\n\\begin{cases}\n\\beta_0=\\gamma_0+\\gamma_2 \\\\\n\\beta_1=-\\gamma_2 \\\\\n\\beta_2=\\gamma_1-\\gamma_2 \\\\\n\\end{cases}\n\\Longrightarrow\n\\begin{cases}\n\\widehat\\beta_0=-0.92 \\\\\n\\widehat\\beta_1=1.48 \\\\\n\\widehat\\beta_2=1.05 \\\\\n\\end{cases}\\] On peut retrouver ces résultats avec\n\nglm(etat~C(marque,base=3),data=panne,family=\"binomial\")\n\n\nCall:  glm(formula = etat ~ C(marque, base = 3), family = \"binomial\", \n    data = panne)\n\nCoefficients:\n         (Intercept)  C(marque, base = 3)A  C(marque, base = 3)B  \n             -0.9163                1.4759                1.0498  \n\nDegrees of Freedom: 32 Total (i.e. Null);  30 Residual\nNull Deviance:      45.72 \nResidual Deviance: 43.52    AIC: 49.52\n\n\n\nIl y a interaction si l’age agit différemment sur la panne en fonction de la marque.\nLe modèle ajusté sur R est \\[\\log\\left(\\frac{p_\\beta(x)}{1-p_\\beta(x)}\\right)=\\delta_0+\\delta_1\\mathsf{1}_{x_2=B}+\\delta_2\\mathsf{1}_{x_2=C}+\\delta_3x_1+\\delta_4x_1\\mathsf{1}_{x_2=B}+\\delta_5x_1\\mathsf{1}_{x_2=C}.\\] On obtient ainsi par identification : \\[\\begin{cases}\n\\alpha_0=\\delta_0\\\\\n\\alpha_1=\\delta_3\\\\\n\\beta_0=\\delta_0+\\delta_1\\\\\n\\beta_1=\\delta_3+\\delta_4\\\\\n\\gamma_0=\\delta_0+\\delta_2\\\\\n\\gamma_1=\\delta_3+\\delta_5\n\\end{cases}\\] On peut ainsi en déduire les valeurs des estimateurs que l’on peut retrouver avec la commande :\n\nglm(etat~-1+marque+marque:age,data=panne,family=\"binomial\")\n\n\nCall:  glm(formula = etat ~ -1 + marque + marque:age, family = \"binomial\", \n    data = panne)\n\nCoefficients:\n    marqueA      marqueB      marqueC  marqueA:age  marqueB:age  marqueC:age  \n    0.23512      0.43375     -2.19633      0.05641     -0.05547      0.27228  \n\nDegrees of Freedom: 33 Total (i.e. Null);  27 Residual\nNull Deviance:      45.75 \nResidual Deviance: 42.62    AIC: 54.62\n\n\n\n\n\nExercice 6 (Interprétation)  \n\ndf &lt;- read.csv(\"../donnees/logit_ex6.csv\")\nmod &lt;- glm(Y~.,data=df,family=binomial)\nmod1 &lt;- glm(Y~X1,data=df,family=binomial)\nsummary(mod)\n\n\nCall:\nglm(formula = Y ~ ., family = binomial, data = df)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -0.57866    0.11926  -4.852 1.22e-06 ***\nX1          -0.19471    0.06556  -2.970  0.00298 ** \nX2           0.31899    0.04404   7.244 4.36e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 693.15  on 499  degrees of freedom\nResidual deviance: 618.26  on 497  degrees of freedom\nAIC: 624.26\n\nNumber of Fisher Scoring iterations: 4\n\nsummary(mod1)\n\n\nCall:\nglm(formula = Y ~ X1, family = binomial, data = df)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)  0.001092   0.089499   0.012    0.990\nX1          -0.020467   0.051733  -0.396    0.692\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 693.15  on 499  degrees of freedom\nResidual deviance: 692.99  on 498  degrees of freedom\nAIC: 696.99\n\nNumber of Fisher Scoring iterations: 3\n\n\nOn remarque que la nullité du paramètre associé à X1 est accepté dans le modèle avec uniquement X1 alors qu’elle est refusée lorsqu’on considère X1 et X2 dans le modèle.\n\n\nExercice 7 (Tests à la main)  \n\nLe modèle s’écrit \\[log\\left(\\frac{p_\\beta(x)}{1-p_\\beta(x)}\\right)=\\beta_0+\\beta_1x.\\]\nLa log vraisemblance s’obtient avec\n\np &lt;- c(0.76,0.4,0.6,0.89,0.35)\nY &lt;- c(1,0,0,1,1)\nL1 &lt;- log(prod(p^Y*(1-p)^(1-Y)))\nL1\n\n[1] -2.867909\n\n\n\nOn calcule les écart-type des estimateurs\n\nX1 &lt;- c(0.47,-0.55,-0.01,1.07,-0.71)\nX &lt;- matrix(c(rep(1,5),X1),ncol=2)\nW &lt;- diag(p*(1-p))\nSIG &lt;- solve(t(X)%*%W%*%X)\nsig &lt;- sqrt(diag(SIG))\nsig\n\n[1] 1.023252 1.744935\n\n\nOn en déduit les statistiques de test :\n\nbeta &lt;- c(0.4383,1.5063)\nbeta/sig\n\n[1] 0.4283401 0.8632411\n\n\nOn peut faire le test de Wald et du rapport de vraisemblance.\nLa statistique de test vaut 0.8632411, on obtient donc la probabilité critique\n\n2*(1-pnorm(0.8632411))\n\n[1] 0.3880049\n\n\nOn peut également effectuer un test du rapport de vraisemblance. Le modèle null sans X1 a pour log-vraisemblance\n\np0 &lt;- 3/5\nL0 &lt;- log(prod(p0^Y*(1-p0)^(1-Y)))\nL0\n\n[1] -3.365058\n\n\nLa statistique de test vaut donc\n\n2*(L1-L0)\n\n[1] 0.9942984\n\n\net la probabilité critique vaut\n\n1-pchisq(2*(L1-L0),df=1)\n\n[1] 0.3186941\n\n\nOn peut retrouver (aux arrondis près) les résultats de l’exercice avec\n\nX &lt;- c(0.47,-0.55,-0.01,1.07,-0.71)\nY &lt;- c(1,0,0,1,1)\ndf &lt;- data.frame(X,Y)\nmodel &lt;- glm(Y~X,data=df,family=\"binomial\")\nlogLik(model)\n\n'log Lik.' -2.898765 (df=2)\n\nAnova(model,type=3,test.statistic = \"Wald\")\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: Y\n            Df  Chisq Pr(&gt;Chisq)\n(Intercept)  1 0.1846     0.6675\nX            1 0.7552     0.3848\n\nAnova(model,type=3,test.statistic = \"LR\")\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: Y\n  LR Chisq Df Pr(&gt;Chisq)\nX  0.93259  1     0.3342\n\n\n\n\n\n\nExercice 8 (Vraisemblance du modèle saturé)  \n\nLes variables \\((y_t,t=1,\\dots,y_T)\\) étant indépendantes et de loi binomiales \\(B(n_t,p_t)\\), la log-vraisemblance est donnée par \\[\\begin{align*}\n\\mathcal L_{\\text{sat}}(Y,p)= & \\log\\left(\\prod_{t=1}^T\n\\begin{pmatrix}\nn_t\\\\\n\\tilde y_t\n\\end{pmatrix}\np_t^{\\tilde y_t}(1-p_t)^{n_t-\\tilde y_t}\\right) \\\\\n= &\n\\sum_{t=1}^T\\left(\\log\n\\begin{pmatrix}\nn_t\\\\\n\\tilde y_t\n\\end{pmatrix}\n+\\tilde y_t\\log(p_t)+(n_t-\\tilde y_t)\\log(1-p_t)\\right)\n\\end{align*}\\]\nLa dérivée de la log-vraisemblance par rapport à \\(p_t\\) s’écrit \\[\\frac{\\tilde y_t}{p_t}-\\frac{n_t-\\tilde y_t}{1-p_t}.\\] Cette dérivée s’annule pour \\[\\widehat p_t=\\frac{\\tilde y_t}{n_t}.\\]\nOn note \\(\\widehat \\beta\\) l’EMV du modèle logistique et \\(p_{\\widehat\\beta}\\) le vecteur qui contient les valeurs ajustées \\(p_{\\widehat\\beta}(x_t),t=1,\\dots,T\\). On a pour tout \\(\\beta\\in\\mathbb R^p\\) : \\[\\mathcal L(Y,\\beta)\\leq\\mathcal L(Y,\\widehat\\beta)=\\mathcal L_{\\text{sat}}(Y,p_{\\widehat\\beta})\\leq L_{\\text{sat}}(Y,\\widehat p_t).\\]\n\n\n\nExercice 9 (Intervalle de confiance profilé)  \n\n\nartere &lt;- read.table(\"../donnees/artere.txt\",header=T)\nmodele &lt;- glm(chd~age,data=artere,family=binomial)\nB0 &lt;- coef(modele)\nOriginalDeviance &lt;- modele$deviance\n\n\nalpha &lt;- 0.05    \n\n\nstderr &lt;- summary(modele)$coefficients[, \"Std. Error\"]\ndelta &lt;- sqrt(qchisq((1-alpha/4),df=1))* stderr[2] /5\ngrille &lt;- B0[2]+(-10):10*delta\n\nOn a \\[\\begin{align*}\n\\mathcal D_1&=-2(\\mathcal L(Y,\\hat\\beta)-\\mathcal L_{sat})\n  \\end{align*}\\] Pour celle avec l’offset \\(K_i=x_i\\beta_2^*\\) elle vaut \\[\\begin{align*}\n\\mathcal D_o&=-2(\\mathcal L(Y,K,\\hat\\beta_1)-\\mathcal L_{sat})\n  \\end{align*}\\] où \\(\\hat \\beta_1\\) maximise \\(\\mathcal L(Y,K,\\hat\\beta_1)\\) c’est à dire \\(\\mathcal L(Y,K,\\hat\\beta_1)=l(\\beta_2^*)\\) et nous avons donc \\[\\begin{align*}\n\\mathcal D_o - \\mathcal D_1= 2(\\mathcal L(Y,\\hat\\beta)-\\mathcal L(Y,K,\\beta_1)= 2(\\mathcal L(Y,\\hat\\beta)-l(\\beta_2^*))=P(\\beta_2^*).\n  \\end{align*}\\]\n\nprofil2 &lt;- rep(0,length(grille))\nfor (k in 1:length(grille)) {\n  modeleo &lt;- glm(chd~1,family=binomial,offset=artere[,\"age\"]*grille[k],data=artere)\n  profil2[k] &lt;- modeleo$deviance - OriginalDeviance\n}\n\n\nprofil &lt;- sign(-10:10)*sqrt(profil2)\n\n\nspline(x=profil,y=grille,xout=c(-sqrt(qchisq(1-alpha,1)),sqrt(qchisq(1-alpha,1))))$y\n\n[1] 0.0669275 0.1620014\n\n\n\nconfint(modele)\n\n                  2.5 %     97.5 %\n(Intercept) -7.72587162 -3.2461547\nage          0.06693158  0.1620067"
  },
  {
    "objectID": "code/chap16.html",
    "href": "code/chap16.html",
    "title": "16 Introduction à la régression spline",
    "section": "",
    "text": "ozone &lt;- read.table(\"../donnees/ozone_simple.txt\",header=T,sep=\";\")\n\n\npolyreg &lt;- function(donnee,d=3){\n  sigmax &lt;- sd(donnee[,\"T12\"])\n  grillex &lt;- seq(min(donnee[,\"T12\"])-sigmax,max(donnee[,\"T12\"])+sigmax,length=100)\n  aprevoir &lt;- data.frame(T12=grillex)\n  regpol &lt;- lm(O3~poly(T12,degree=d,raw=TRUE),data=donnee)\n  prev &lt;- predict(regpol,aprevoir)\n  return(list(grillex=grillex,grilley=prev))\n}\n\n\nplot(O3~T12,data=ozone,xlab=\"T12\",ylab=\"O3\")\niter &lt;- 1\nfor(ii in c(1,2,3,9)){\n tmp &lt;- polyreg(ozone,d=ii)\n lines(tmp$grillex,tmp$grilley,col=iter,lty=iter)\n iter &lt;- iter+1\n}\nlegend(15,150,c(\"d=1\",\"d=2\",\"d=3\",\"d=9\"),col=1:4,lty=1:4)\n\n\n\n\n\nind &lt;- which(ozone[,2]&lt;23)\nregd &lt;- lm(O3~T12,data=ozone[ind,])\nregf &lt;- lm(O3~T12,data=ozone[-ind,])\ngxd &lt;- seq(3,23,length=50)\ngyd &lt;- regd$coef[1]+gxd*regd$coef[2]\ngxf &lt;- seq(23,35,length=50)\ngyf &lt;- regf$coef[1]+gxf*regf$coef[2]\nplot(O3~T12,data=ozone)\nlines(gxd,gyd,col=2,lty=1,lwd=2)\nlines(gxf,gyf,col=2,lty=1,lwd=2)\nabline(v=23)\n\n\n\n\n\nlibrary(splines)\nXB &lt;- bs(ozone[,2], knots=c(15,23), degree=2,Boundary.knots=c(5,32))\nregs &lt;- lm(ozone[,\"O3\"] ~ XB)\nregs$coef\n\n(Intercept)         XB1         XB2         XB3         XB4 \n  51.101947   61.543761    5.562286   70.459103  106.711539 \n\n\n\ngrillex &lt;- seq(5,32,length=100)\nbgrillex &lt;- bs(grillex, knots=c(15,23), degree=2,Boundary.knots=c(5,32))\nprev &lt;- bgrillex%*%as.matrix(regs$coeff[-1])+regs$coeff[1]\nplot(O3~T12,data=ozone)\nlines(grillex,prev,col=2)\nabline(v=c(15,23))\n\n\n\n\n\nregssplinel1 &lt;- smooth.spline(ozone[,2],ozone[,1],lambda =100)\nprevl1 &lt;- predict(regssplinel1,grillex)\nplot(O3~T12,data=ozone)\nlines(prevl1$x,prevl1$y,col=2)\n\n\n\n\n\nregsspline &lt;- smooth.spline(ozone[,2],ozone[,1])\nprev &lt;- predict(regsspline,grillex)\nplot(O3~T12,data=ozone)\nlines(prev$x,prev$y,col=2)\n\n\n\n\n\nregsspline\n\nCall:\nsmooth.spline(x = ozone[, 2], y = ozone[, 1])\n\nSmoothing Parameter  spar= 0.9410342  lambda= 0.006833357 (15 iterations)\nEquivalent Degrees of Freedom (Df): 4.156771\nPenalized Criterion (RSS): 11036.88\nGCV: 289.8012"
  },
  {
    "objectID": "code/chap12.html",
    "href": "code/chap12.html",
    "title": "12 Régression de Poisson",
    "section": "",
    "text": "Le modèle de Poisson\n\nMalaria &lt;- read.table(\"../donnees/poissonData3.csv\", sep=\",\", header=T)\nsummary(Malaria)\n\n     Sexe                Age            Altitude     Prevention       \n Length:1627        Min.   :  10.0   Min.   :1129   Length:1627       \n Class :character   1st Qu.: 220.0   1st Qu.:1266   Class :character  \n Mode  :character   Median : 361.0   Median :1298   Mode  :character  \n                    Mean   : 419.4   Mean   :1295                     \n                    3rd Qu.: 555.0   3rd Qu.:1320                     \n                    Max.   :1499.0   Max.   :1515                     \n                                     NA's   :105                      \n     Duree          N.malaria     \n Min.   :   0.0   Min.   : 0.000  \n 1st Qu.: 172.0   1st Qu.: 1.000  \n Median : 721.0   Median : 4.000  \n Mean   : 619.3   Mean   : 4.687  \n 3rd Qu.:1011.0   3rd Qu.: 7.000  \n Max.   :1464.0   Max.   :26.000  \n                                  \n\n\n\nmodP &lt;- glm(N.malaria ~ Duree, data = Malaria, family = poisson)\nmodP\n\n\nCall:  glm(formula = N.malaria ~ Duree, family = poisson, data = Malaria)\n\nCoefficients:\n(Intercept)        Duree  \n   0.429459     0.001508  \n\nDegrees of Freedom: 1626 Total (i.e. Null);  1625 Residual\nNull Deviance:      5710 \nResidual Deviance: 3325     AIC: 8125\n\n\n\nplot(N.malaria ~ Duree, data = Malaria,pch=20,cex=0.5)\nmod.lin &lt;- lm(N.malaria ~ Duree, data = Malaria)\nabline(a=coef(mod.lin)[1],b=coef(mod.lin)[2],lwd=2)\nx &lt;- seq(0,1500,by=1)\ny &lt;- exp(coef(modP)[1]+coef(modP)[2]*x)\nlines(x,y,col=\"red\",lty=2,lwd=2.5)\n\n\n\n\n\nmodP3 &lt;- glm( N.malaria ~ Duree + Sexe + Prevention, \n              data = Malaria,family = poisson )\n\n\n\nTests et intervalles de confiance\n\nMalaria$Prevention &lt;- as.factor(Malaria$Prevention)\nMalaria$Prevention &lt;- relevel(Malaria$Prevention,ref=\"Rien\")\nmodP3 &lt;- glm( N.malaria ~ Duree + Sexe + Prevention, data = Malaria,\n             family = poisson )\nsummary(modP3)\n\n\nCall:\nglm(formula = N.malaria ~ Duree + Sexe + Prevention, family = poisson, \n    data = Malaria)\n\nCoefficients:\n                            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                0.3878929  0.0389800   9.951   &lt;2e-16 ***\nDuree                      0.0015101  0.0000343  44.031   &lt;2e-16 ***\nSexeM                      0.0550890  0.0229690   2.398   0.0165 *  \nPreventionAutre           -0.2255828  0.1781379  -1.266   0.2054    \nPreventionMoustiquaire     0.0176850  0.0255967   0.691   0.4896    \nPreventionSerpentin/Spray  0.0196420  0.0590690   0.333   0.7395    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 5710.4  on 1626  degrees of freedom\nResidual deviance: 3317.3  on 1621  degrees of freedom\nAIC: 8124.6\n\nNumber of Fisher Scoring iterations: 5\n\n\n\nmodP2 &lt;- glm( N.malaria ~ Duree + Sexe, data = Malaria, family = poisson)\n-2*(logLik(modP2)-logLik(modP3))\n\n'log Lik.' 2.448823 (df=3)\n\nqchisq(0.95,df=3)\n\n[1] 7.814728\n\n\n\nanova(modP2,modP3,test=\"LRT\")\n\nAnalysis of Deviance Table\n\nModel 1: N.malaria ~ Duree + Sexe\nModel 2: N.malaria ~ Duree + Sexe + Prevention\n  Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)\n1      1624     3319.8                     \n2      1621     3317.3  3   2.4488   0.4846\n\n\n\nlibrary(car)\nAnova(modP2,test=\"LR\")\n\nAnalysis of Deviance Table (Type II tests)\n\nResponse: N.malaria\n      LR Chisq Df Pr(&gt;Chisq)    \nDuree  2386.56  1    &lt; 2e-16 ***\nSexe      5.45  1    0.01961 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nround(confint.default(modP3),3)\n\n                           2.5 % 97.5 %\n(Intercept)                0.311  0.464\nDuree                      0.001  0.002\nSexeM                      0.010  0.100\nPreventionAutre           -0.575  0.124\nPreventionMoustiquaire    -0.032  0.068\nPreventionSerpentin/Spray -0.096  0.135\n\nround(confint(modP3),3)\n\n                           2.5 % 97.5 %\n(Intercept)                0.311  0.464\nDuree                      0.001  0.002\nSexeM                      0.010  0.100\nPreventionAutre           -0.596  0.105\nPreventionMoustiquaire    -0.032  0.068\nPreventionSerpentin/Spray -0.098  0.134\n\n\n\n\nSélection de variables\n\nMalaria &lt;- read.table(\"../donnees/poissonData.csv\", sep=\",\", header=T)\nMalaria1 &lt;- na.omit(Malaria)\nMalaria1$Prevention &lt;- as.factor(Malaria1$Prevention)\nMalaria1$Sexe &lt;- as.factor(Malaria1$Sexe)\nlibrary(bestglm)\nmod_sel &lt;- bestglm(Malaria1,family=poisson)\nmod_sel$BestModels\n\n   Sexe   Age Altitude Prevention Duree Criterion\n1 FALSE  TRUE     TRUE      FALSE  TRUE  7384.946\n2 FALSE FALSE     TRUE      FALSE  TRUE  7387.814\n3  TRUE  TRUE     TRUE      FALSE  TRUE  7390.053\n4  TRUE FALSE     TRUE      FALSE  TRUE  7393.119\n5 FALSE  TRUE    FALSE      FALSE  TRUE  7401.021"
  },
  {
    "objectID": "code/chap10.html",
    "href": "code/chap10.html",
    "title": "10 Comparaison des différentes méthodes, étude de cas réels",
    "section": "",
    "text": "Importation de l’ozone\n\nozone &lt;- read.table(\"../donnees/ozone_complet.txt\", header = T, sep = \";\")\ndim(ozone)\n\n[1] 1464   23\n\n\nÉlimination des individus avec une valeur manquante\n\nindNA &lt;- which(is.na(ozone), arr.ind = T)[,1]\nozone2 &lt;- ozone[-indNA,]\n\nImportation du fichier d’ozone sans valeurs manquantes avec les projections\n\nozone &lt;- read.table(\"../donnees/ozone_transf.txt\", header = T, sep = \";\")\n\net préparation du data-frame qui contiendra les résultats de chaque méthode\n\nRES &lt;- data.frame(Y = ozone$maxO3)\n\nPour le moment il ne contient qu’une seule colonne avec les données à prévoir."
  },
  {
    "objectID": "code/chap10.html#régression-multiple",
    "href": "code/chap10.html#régression-multiple",
    "title": "10 Comparaison des différentes méthodes, étude de cas réels",
    "section": "Régression multiple",
    "text": "Régression multiple\nChargement du package pour la sélection de variables\n\nlibrary(leaps)\n\nEvaluation de la qualité prédictive de la régression linéaire et de la sélection de variables via BIC (algorithme exhaustif)\n\nfor(i in 1:nbbloc){\n  ###MCO global\n  reg &lt;- lm(maxO3~.,data=ozone[bloc!=i,])\n  RES[bloc==i,\"MCO\"] &lt;- predict(reg,ozone[bloc==i,])\n  ###MCO choix\n  recherche &lt;- regsubsets(maxO3~., int=T, nbest=1, nvmax=22, \n                                        data=ozone[bloc!=i,])\n  resume &lt;- summary(recherche)\n  nomselec &lt;- colnames(resume$which)[\n                       resume$which[which.min(resume$bic),] ][-1]\n  formule &lt;- formula(paste(\"maxO3~\",paste(nomselec,collapse=\"+\")))\n  regbic &lt;- lm(formule,data=ozone[bloc!=i,])\n  RES[bloc==i,\"choix\"] &lt;- predict(regbic,ozone[bloc==i,])\n}"
  },
  {
    "objectID": "code/chap10.html#lasso-ridge-et-elasticnet",
    "href": "code/chap10.html#lasso-ridge-et-elasticnet",
    "title": "10 Comparaison des différentes méthodes, étude de cas réels",
    "section": "Lasso, ridge et elasticnet",
    "text": "Lasso, ridge et elasticnet\nChargement du package pour lasso, ridge et elasticnet et création des matrices nécessaires à son utilisation :\n\nlibrary(glmnet)\nozone.X &lt;- model.matrix(maxO3~.,data=ozone)[,-1]\nozone.Y &lt;- ozone[,\"maxO3\"]\n\nÉvaluation de la qualité prédictive des régressions lasso, ridge et elasticnet:\n\nfor(i in 1:nbbloc){  \n  XA &lt;- ozone.X[bloc!=i,]\n  YA &lt;- ozone.Y[bloc!=i]\n  XT &lt;- ozone.X[bloc==i,]\n  ###ridge\n  tmp &lt;- cv.glmnet(XA,YA,alpha=0)\n  mod &lt;- glmnet(XA,YA,alpha=0,lambda=tmp$lambda.min)\n  RES[bloc==i,\"ridge\"] &lt;- predict(mod,XT)\n  ###lasso\n  tmp &lt;- cv.glmnet(XA,YA,alpha=1)\n  mod &lt;- glmnet(XA,YA,alpha=0,lambda=tmp$lambda.min)\n  RES[bloc==i,\"lasso\"] &lt;- predict(mod,XT)\n  ###elastic\n  tmp &lt;- cv.glmnet(XA,YA,alpha=0.5)\n  mod &lt;- glmnet(XA,YA,alpha=.5,lambda=tmp$lambda.min)\n  RES[bloc==i,\"elastic\"] &lt;- predict(mod,XT)\n}"
  },
  {
    "objectID": "code/chap10.html#régressions-sur-composantes",
    "href": "code/chap10.html#régressions-sur-composantes",
    "title": "10 Comparaison des différentes méthodes, étude de cas réels",
    "section": "Régressions sur composantes",
    "text": "Régressions sur composantes\nChargement du package pour les régressions sur composantes\n\nlibrary(pls)\n\nÉvaluation de la qualité prédictive des régressions PCR et PLS\n\nfor(i in 1:nbbloc){\n   #####PLS\n   tmp &lt;- plsr(maxO3~.,data=ozone[bloc!=i,],ncomp=20,\n                                 validation=\"CV\",scale=TRUE)\n   mse &lt;- MSEP(tmp,estimate=c(\"train\",\"CV\"))\n   npls &lt;- which.min(mse$val[\"CV\",,])-1 \n   mod &lt;- plsr(maxO3~.,ncomp=npls,data=ozone[bloc!=i,],scale=TRUE)\n   RES[bloc==i,\"PLS\"] &lt;- predict(mod,ozone[bloc==i,],ncomp=npls)\n   #####PCR\n   tmp &lt;- pcr(maxO3~.,data=ozone[bloc!=i,],ncomp=20,\n                                    validation=\"CV\",scale=TRUE)\n   mse &lt;- MSEP(tmp,estimate=c(\"train\",\"CV\"))\n   npcr &lt;- which.min(mse$val[\"CV\",,])-1 \n   mod &lt;- pcr(maxO3~.,ncomp=npcr,data=ozone[bloc!=i,],scale=TRUE)\n   RES[bloc==i,\"PCR\"] &lt;- predict(mod,ozone[bloc==i,],ncomp=npcr)\n }\n\nLes résultats :\n\nRES |&gt; \n  dplyr::summarise(across(-Y,~mean((Y-.)^2)))\n\n       MCO    choix    ridge    lasso  elastic      PLS      PCR\n1 187.2726 188.8491 187.8304 187.1023 187.0389 187.2622 187.2685"
  },
  {
    "objectID": "code/chap10.html#régression-linéaire",
    "href": "code/chap10.html#régression-linéaire",
    "title": "10 Comparaison des différentes méthodes, étude de cas réels",
    "section": "Régression linéaire",
    "text": "Régression linéaire\nLa fonction\n\nsse_reg &lt;- function(don,bloc,b) {\n    m_reg &lt;- lm(maxO3~.,data=don[bloc!=b,])\n    previsions &lt;- predict(m_reg,don[bloc==b,])\n    return(sum((don[bloc==b,\"maxO3\"]-previsions)^2))\n}\n\nLa qualité de la modélisation\n\nset.seed(1234)\nssereg  &lt;- rep(0,20)\nfor (r in 1:20) {\n  bloc &lt;- sample(blocseq)\n  for(b in 1:nbbloc){\n    ssereg[r] &lt;- ssereg[r] + sse_reg(ozone,bloc,b)\n  }\n}\nres_rep$MCO &lt;- round(mean(ssereg/nrow(ozone)),2)"
  },
  {
    "objectID": "code/chap10.html#choix-de-variables",
    "href": "code/chap10.html#choix-de-variables",
    "title": "10 Comparaison des différentes méthodes, étude de cas réels",
    "section": "Choix de variables",
    "text": "Choix de variables\nLa fonction\n\nlibrary(leaps)\nsse_regbic &lt;- function(don,bloc,b,nvmax,method) {\n    recherche &lt;- regsubsets(maxO3~., int=T, nbest=1,data=don[bloc!=b,],\n                           nvmax=nvmax,method=method)\n    resume &lt;- summary(recherche)\n    nomselec &lt;- colnames(resume$which)[resume$which[which.min(resume$bic),]][-1]\n    formule &lt;- formula(paste(\"maxO3 ~\", paste(nomselec, collapse = \"+\")))\n    m_reg &lt;- lm(formule,data=don[bloc!=b,])\n    previsions &lt;- predict(m_reg,don[bloc==b,])\n    return(sum((don[bloc==b,\"maxO3\"]-previsions)^2))\n}\n\nLa qualité de la modélisation\n\nset.seed(1234)\nsseregbic &lt;-  rep(0,20)\nfor (r in 1:20) {\n  bloc &lt;- sample(blocseq)\n  for(b in 1:nbbloc){\n    sseregbic[r] &lt;- sseregbic[r] + sse_regbic(ozone,bloc,b,22,\"exhaustive\")\n  }\n}\nres_rep$choix &lt;- mean(sseregbic/nrow(ozone))"
  },
  {
    "objectID": "code/chap10.html#lasso",
    "href": "code/chap10.html#lasso",
    "title": "10 Comparaison des différentes méthodes, étude de cas réels",
    "section": "Lasso",
    "text": "Lasso\nLa fonction\n\nlibrary(glmnet)\nsse_glmnet &lt;- function(X,Y,bloc,b,a) {\n  rech &lt;- cv.glmnet(X[bloc!=b,], Y[bloc!=b,drop=FALSE], alpha=a)\n  prev &lt;- predict(rech, newx=X[bloc==b,], s=rech$lambda.min)\n  return(sum((Y[bloc==b,\"maxO3\"] - as.vector(prev))^2))\n}\n\nLa qualité de la modélisation\n\nX &lt;-  model.matrix(maxO3~.,data=ozone)[,-1]\nY &lt;- data.matrix(ozone[,\"maxO3\",drop=FALSE])\nset.seed(1234)\nsselasso &lt;- rep(0,20)\nfor (r in 1:20) {\n  bloc &lt;- sample(blocseq)\n  for(b in 1:nbbloc){\n      sselasso[r] &lt;- sselasso[r] + sse_glmnet(X,Y,bloc,b,a=1)\n  }\n}\nres_rep$lasso &lt;- round(mean(sselasso/nrow(ozone)),2)"
  },
  {
    "objectID": "code/chap10.html#ridge",
    "href": "code/chap10.html#ridge",
    "title": "10 Comparaison des différentes méthodes, étude de cas réels",
    "section": "RIDGE",
    "text": "RIDGE\nLa qualité de la modélisation\n\nX &lt;-  model.matrix(maxO3~.,data=ozone)[,-1]\nY &lt;- data.matrix(ozone[,\"maxO3\",drop=FALSE])\nset.seed(1234)\nsseridge &lt;- rep(0,20)\nfor (r in 1:20) {\n  bloc &lt;- sample(blocseq)\n  for(b in 1:nbbloc){\n      sseridge[r] &lt;- sseridge[r] + sse_glmnet(X,Y,bloc,b,a=0)\n  }\n}\nres_rep$ridge &lt;- mean(sseridge/nrow(ozone))"
  },
  {
    "objectID": "code/chap10.html#elastic-net",
    "href": "code/chap10.html#elastic-net",
    "title": "10 Comparaison des différentes méthodes, étude de cas réels",
    "section": "Elastic-net",
    "text": "Elastic-net\nLa qualité de la modélisation\n\nX &lt;-  model.matrix(maxO3~.,data=ozone)[,-1]\nY &lt;- data.matrix(ozone[,\"maxO3\",drop=FALSE])\nset.seed(1234)\nsseelasticnet &lt;- rep(0,20)\nfor (r in 1:20) {\n  bloc &lt;- sample(blocseq)\n  for(b in 1:nbbloc){\n      sseelasticnet[r] &lt;- sseelasticnet[r] + sse_glmnet(X,Y,bloc,b,a=0.5)\n  }\n}\nres_rep$elastic &lt;- mean(sseelasticnet/nrow(ozone))\n\nLes résultats\n\nres_rep\n\n     MCO    choix    ridge  lasso  elastic\n1 188.36 189.6025 188.6624 187.92 187.8242"
  },
  {
    "objectID": "code/chap8.html",
    "href": "code/chap8.html",
    "title": "8 Régression sous contraintes de norme : ridge, lasso, elastic net",
    "section": "",
    "text": "ozone &lt;- read.table(\"../donnees/ozone.txt\",header=TRUE,sep=\";\",row.names=1)[,-c(11:12)]\nozone.X &lt;- model.matrix(O3 ~ ., data = ozone)[,-1]\nozone.Y &lt;- ozone$O3\n\n\nlibrary(glmnet)\nridge &lt;- glmnet(ozone.X, ozone.Y, alpha = 0)\nlasso &lt;- glmnet(ozone.X, ozone.Y)#par défaut alpha=1\nen &lt;- glmnet(ozone.X, ozone.Y, alpha = 0.5)\n\n\nplot(ridge,main=\"Ridge\",ylim=c(-2,2))\n\n\n\nplot(ridge,xvar=\"lambda\",main=\"Ridge\",ylim=c(-2,2))\n\n\n\nplot(lasso,main=\"Lasso\",ylim=c(-2,2))\n\n\n\nplot(lasso,xvar=\"lambda\",main=\"Lasso\",ylim=c(-2,2))\n\n\n\nplot(en,main=\"Elastic net\",ylim=c(-2,2))\n\n\n\nplot(en,xvar=\"lambda\",main=\"Elastic net\",ylim=c(-2,2))\n\n\n\n\n\nset.seed(1234)\ncv.ridge &lt;- cv.glmnet(ozone.X, ozone.Y, alpha = 0)\ncv.lasso &lt;- cv.glmnet(ozone.X, ozone.Y) #alpha=1 par défaut\ncv.en &lt;- cv.glmnet(ozone.X, ozone.Y, alpha = 0.5)\n\n\ncv.ridge$lambda.min\n\n[1] 4.619799\n\ncv.ridge$lambda.1se\n\n[1] 29.69641\n\n\n\nplot(cv.ridge, main = \"Ridge\")\n\n\n\nplot(cv.lasso, main = \"Lasso\")\n\n\n\nplot(cv.en, main = \"Elastic net\")\n\n\n\n\n\nxnew &lt;- ozone.X[c(25,30),]\nrownames(xnew) &lt;- NULL\n\n\nxnew\n\n      T12  T15 Ne12 N12 S12 E12 W12   Vx   O3v\n[1,] 13.6 14.4    1   0   0   1   0 3.55  97.8\n[2,] 21.8 23.6    6   4   0   0   0 2.50 112.0\n\npredict(cv.ridge,newx=xnew)\n\n     lambda.1se\n[1,]   90.41122\n[2,]   90.65036\n\n\n\nozone&lt;-read.table(\"../donnees/ozone.txt\",header=TRUE,sep=\";\",row.names = 1)\nozone.X &lt;- model.matrix(O3~.,data=ozone)\nozone.Y &lt;- ozone$O3\ncv.defaut &lt;- cv.glmnet(ozone.X,ozone.Y)\nlassodefaut&lt;-glmnet(ozone.X,ozone.Y,lambda=cv.defaut$lambda.min)\n\n\nozone$vent &lt;- as.factor(ozone$vent)\nozone$vent &lt;- relevel(ozone$vent,ref=\"NORD\")\nozone.X &lt;- model.matrix(O3~.,data=ozone)\ncv.nord &lt;- cv.glmnet(ozone.X,ozone.Y)\nlassonord &lt;- glmnet(ozone.X,ozone.Y,lambda=cv.nord$lambda.min)\n\n\npredict(lassodefaut,ozone.X[1:4,])\n\n               s0\n19960422 73.05686\n19960429 92.67573\n19960506 69.26897\n19960514 80.69977\n\npredict(lassonord,ozone.X[1:4,])\n\n               s0\n19960422 79.43741\n19960429 90.58414\n19960506 74.36556\n19960514 75.06648\n\n\n\nozone.X &lt;- model.matrix(O3~.-vent-nebulosite+C(vent,sum)+\n                          C(nebulosite,sum),data=ozone)\ncv.sum &lt;- cv.glmnet(ozone.X,ozone.Y)\nlassosum &lt;- glmnet(ozone.X,ozone.Y,lambda=cv.sum$lambda.min)\npredict(lassosum,ozone.X[1:4,])\n\n               s0\n19960422 78.05238\n19960429 89.87258\n19960506 75.07092\n19960514 73.93502"
  },
  {
    "objectID": "code/chap6.html",
    "href": "code/chap6.html",
    "title": "6 Variables qualitatives : ANCOVA et ANOVA",
    "section": "",
    "text": "La concentration en ozone\n\nozone &lt;- read.table(\"../donnees/ozone.txt\", header = T, sep = \";\")\nplot(ozone[,\"T12\"], ozone[,\"O3\"],pch=as.numeric(ozone[,\"vent\"]),\n     col = as.numeric(ozone[,\"vent\"]))\na1 &lt;- lm(O3 ~ T12, data = ozone[ozone[,\"vent\"]==\"EST\",])\na2 &lt;- lm(O3 ~ T12, data = ozone[ozone[,\"vent\"]==\"NORD\",])\na3 &lt;- lm(O3 ~ T12, data = ozone[ozone[,\"vent\"]==\"OUEST\",])\na4 &lt;- lm(O3 ~ T12, data = ozone[ozone[,\"vent\"]==\"SUD\",])\nabline(a1, col=1)\nabline(a2, col=2)\nabline(a3, col=3)\nabline(a4, col=4)\n\n\n\n\n\nmod1b &lt;- lm(formula = O3 ~ -1 + vent + T12:vent, data = ozone)\nsummary(mod1b)\n\n\nCall:\nlm(formula = O3 ~ -1 + vent + T12:vent, data = ozone)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-29.903  -9.163   1.153  10.319  32.638 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \nventEST        45.6090    13.9343   3.273 0.002133 ** \nventNORD      106.6345    28.0341   3.804 0.000456 ***\nventOUEST      64.6840    24.6208   2.627 0.011967 *  \nventSUD       -27.0602    26.5389  -1.020 0.313737    \nventEST:T12     2.7480     0.6342   4.333 8.96e-05 ***\nventNORD:T12   -1.6491     1.6058  -1.027 0.310327    \nventOUEST:T12   0.3407     1.2047   0.283 0.778709    \nventSUD:T12     5.3786     1.1497   4.678 3.00e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.71 on 42 degrees of freedom\nMultiple R-squared:  0.9773,    Adjusted R-squared:  0.973 \nF-statistic: 226.1 on 8 and 42 DF,  p-value: &lt; 2.2e-16\n\n\n\nmod1 &lt;- lm(formula = O3 ~ vent + T12:vent, data = ozone)\nsummary(mod1)\n\n\nCall:\nlm(formula = O3 ~ vent + T12:vent, data = ozone)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-29.903  -9.163   1.153  10.319  32.638 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    45.6090    13.9343   3.273  0.00213 ** \nventNORD       61.0255    31.3061   1.949  0.05796 .  \nventOUEST      19.0751    28.2905   0.674  0.50384    \nventSUD       -72.6691    29.9746  -2.424  0.01972 *  \nventEST:T12     2.7480     0.6342   4.333 8.96e-05 ***\nventNORD:T12   -1.6491     1.6058  -1.027  0.31033    \nventOUEST:T12   0.3407     1.2047   0.283  0.77871    \nventSUD:T12     5.3786     1.1497   4.678 3.00e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.71 on 42 degrees of freedom\nMultiple R-squared:  0.6753,    Adjusted R-squared:  0.6212 \nF-statistic: 12.48 on 7 and 42 DF,  p-value: 1.614e-08\n\n\n\nmod2 &lt;- lm(formula = O3 ~ vent + T12, data = ozone)\nmod2b &lt;- lm(formula = O3 ~ -1 + vent + T12, data = ozone)\nmod3 &lt;- lm(formula = O3 ~ vent:T12, data = ozone)\n\n\nanova(mod2,mod1)\n\nAnalysis of Variance Table\n\nModel 1: O3 ~ vent + T12\nModel 2: O3 ~ vent + T12:vent\n  Res.Df     RSS Df Sum of Sq      F   Pr(&gt;F)   \n1     45 12612.0                                \n2     42  9087.4  3    3524.5 5.4298 0.003011 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(mod3,mod1)\n\nAnalysis of Variance Table\n\nModel 1: O3 ~ vent:T12\nModel 2: O3 ~ vent + T12:vent\n  Res.Df     RSS Df Sum of Sq      F  Pr(&gt;F)  \n1     45 11864.1                              \n2     42  9087.4  3    2776.6 4.2776 0.01008 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nplot(rstudent(mod2) ~ fitted(mod2),xlab=\"ychap\",ylab=\"residus\")\n\n\n\n\n\nlibrary(lattice)\nxyplot(rstudent(mod2)~fitted(mod2)|vent,data = ozone, ylab=\"residus\")\n\n\n\n\n\nmod &lt;- lm(formula = O3 ~ vent + T12 + T12:vent, data = ozone)\n\n\nmod0 &lt;- lm(formula = O3 ~ vent +T12 + T12:vent, data = ozone)\nsummary(mod0)\n\n\nCall:\nlm(formula = O3 ~ vent + T12 + T12:vent, data = ozone)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-29.903  -9.163   1.153  10.319  32.638 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    45.6090    13.9343   3.273  0.00213 ** \nventNORD       61.0255    31.3061   1.949  0.05796 .  \nventOUEST      19.0751    28.2905   0.674  0.50384    \nventSUD       -72.6691    29.9746  -2.424  0.01972 *  \nT12             2.7480     0.6342   4.333 8.96e-05 ***\nventNORD:T12   -4.3971     1.7265  -2.547  0.01462 *  \nventOUEST:T12  -2.4073     1.3614  -1.768  0.08429 .  \nventSUD:T12     2.6306     1.3130   2.004  0.05160 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.71 on 42 degrees of freedom\nMultiple R-squared:  0.6753,    Adjusted R-squared:  0.6212 \nF-statistic: 12.48 on 7 and 42 DF,  p-value: 1.614e-08\n\n\n\n\nLa hauteur des eucalyptus\n\neucalypt &lt;- read.table(\"../donnees/eucalyptus.txt\", header = T, sep = \";\")\neucalypt[,\"bloc\"] &lt;- as.factor(eucalypt[,\"bloc\"])\nm.complet &lt;- lm(ht ~ bloc - 1 + bloc:circ, data = eucalypt)\nm.pente &lt;- lm(ht ~ bloc - 1 + circ, data = eucalypt)\nm.ordonne &lt;- lm(ht ~ bloc:circ, data = eucalypt)\nanova(m.pente, m.complet)\n\nAnalysis of Variance Table\n\nModel 1: ht ~ bloc - 1 + circ\nModel 2: ht ~ bloc - 1 + bloc:circ\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1   1425 2005.9                           \n2   1423 2005.0  2   0.84752 0.3007 0.7403\n\n\n\nanova(m.ordonne, m.complet)\n\nAnalysis of Variance Table\n\nModel 1: ht ~ bloc:circ\nModel 2: ht ~ bloc - 1 + bloc:circ\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1   1425 2009.2                           \n2   1423 2005.0  2    4.1649 1.4779 0.2285\n\n\n\nm.simple &lt;- lm(ht ~ circ, data = eucalypt)\nanova(m.simple, m.pente)\n\nAnalysis of Variance Table\n\nModel 1: ht ~ circ\nModel 2: ht ~ bloc - 1 + circ\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1   1427 2052.1                                  \n2   1425 2005.9  2    46.188 16.406 9.031e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nANOVA\n\nmod1 &lt;- lm(O3~vent-1,data=ozone)\nsummary(mod1)\n\n\nCall:\nlm(formula = O3 ~ vent - 1, data = ozone)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-40.250 -13.950  -2.233  14.972  39.857 \n\nCoefficients:\n          Estimate Std. Error t value Pr(&gt;|t|)    \nventEST    103.850      4.963   20.92  &lt; 2e-16 ***\nventNORD    78.289      6.618   11.83 1.49e-15 ***\nventOUEST   71.578      4.680   15.30  &lt; 2e-16 ***\nventSUD     94.343      7.504   12.57  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.85 on 46 degrees of freedom\nMultiple R-squared:  0.9547,    Adjusted R-squared:  0.9508 \nF-statistic: 242.4 on 4 and 46 DF,  p-value: &lt; 2.2e-16\n\n\n\nanova(mod1)\n\nAnalysis of Variance Table\n\nResponse: O3\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nvent       4 382244   95561  242.44 &lt; 2.2e-16 ***\nResiduals 46  18131     394                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nmod2 &lt;- lm(O3 ~ vent, data = ozone)\nanova(mod2)\n\nAnalysis of Variance Table\n\nResponse: O3\n          Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nvent       3  9859.8  3286.6  8.3383 0.0001556 ***\nResiduals 46 18131.4   394.2                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(mod2)\n\n\nCall:\nlm(formula = O3 ~ vent, data = ozone)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-40.250 -13.950  -2.233  14.972  39.857 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  103.850      4.963  20.923  &lt; 2e-16 ***\nventNORD     -25.561      8.272  -3.090  0.00339 ** \nventOUEST    -32.272      6.821  -4.731 2.16e-05 ***\nventSUD       -9.507      8.997  -1.057  0.29616    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.85 on 46 degrees of freedom\nMultiple R-squared:  0.3522,    Adjusted R-squared:   0.31 \nF-statistic: 8.338 on 3 and 46 DF,  p-value: 0.0001556\n\n\n\nlm(O3 ~ C(vent,treatment), data = ozone)\n\n\nCall:\nlm(formula = O3 ~ C(vent, treatment), data = ozone)\n\nCoefficients:\n            (Intercept)   C(vent, treatment)NORD  C(vent, treatment)OUEST  \n                103.850                  -25.561                  -32.272  \n  C(vent, treatment)SUD  \n                 -9.507  \n\n\n\nlm(O3 ~ C(vent,base=2), data = ozone)\n\n\nCall:\nlm(formula = O3 ~ C(vent, base = 2), data = ozone)\n\nCoefficients:\n           (Intercept)    C(vent, base = 2)EST  C(vent, base = 2)OUEST  \n                78.289                  25.561                  -6.711  \n  C(vent, base = 2)SUD  \n                16.054  \n\n\n\nII &lt;- length(levels(as.factor(ozone$vent)))\nnI &lt;- table(ozone$vent)\ncontraste&lt;-matrix(rbind(diag(II-1),-nI[-II]/nI[II]),II,II-1)\nmod3 &lt;- lm(O3 ~ C(vent,contraste), data = ozone)\nanova(mod3)\n\nAnalysis of Variance Table\n\nResponse: O3\n                   Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nC(vent, contraste)  3  9859.8  3286.6  8.3383 0.0001556 ***\nResiduals          46 18131.4   394.2                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(mod3)\n\n\nCall:\nlm(formula = O3 ~ C(vent, contraste), data = ozone)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-40.250 -13.950  -2.233  14.972  39.857 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)           86.300      2.808  30.737  &lt; 2e-16 ***\nC(vent, contraste)1   17.550      4.093   4.288 9.15e-05 ***\nC(vent, contraste)2   -8.011      5.993  -1.337 0.187858    \nC(vent, contraste)3  -14.722      3.744  -3.933 0.000281 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.85 on 46 degrees of freedom\nMultiple R-squared:  0.3522,    Adjusted R-squared:   0.31 \nF-statistic: 8.338 on 3 and 46 DF,  p-value: 0.0001556\n\n\n\nmod4 &lt;- lm(O3 ~ C(vent,sum), data = ozone) \nanova(mod4)\n\nAnalysis of Variance Table\n\nResponse: O3\n             Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nC(vent, sum)  3  9859.8  3286.6  8.3383 0.0001556 ***\nResiduals    46 18131.4   394.2                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(mod4)\n\n\nCall:\nlm(formula = O3 ~ C(vent, sum), data = ozone)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-40.250 -13.950  -2.233  14.972  39.857 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     87.015      3.027  28.743  &lt; 2e-16 ***\nC(vent, sum)1   16.835      4.635   3.632 0.000705 ***\nC(vent, sum)2   -8.726      5.573  -1.566 0.124284    \nC(vent, sum)3  -15.437      4.485  -3.442 0.001240 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.85 on 46 degrees of freedom\nMultiple R-squared:  0.3522,    Adjusted R-squared:   0.31 \nF-statistic: 8.338 on 3 and 46 DF,  p-value: 0.0001556\n\n\n\nresid2 &lt;- resid(mod2)\nozone$vent &lt;- as.factor(ozone$vent)\nplot(resid2~vent,data=ozone, ylab=\"residus\")\n\n\n\nplot(resid2 ~ jitter(fitted(mod2)),xlab=\"ychap\",ylab=\"residus\")\n\n\n\nxyplot(resid2 ~ I(1:50)|vent, data=ozone,\n       xlab=\"index\", ylab=\"residus\")\n\n\n\n\n\npar(mfrow=c(1,2))\nwith(ozone, interaction.plot(vent, nebulosite, O3, col=1:2))\nwith(ozone, interaction.plot(nebulosite, vent, O3, col=1:4))\n\n\n\n\n\nmod1 &lt;- lm(O3 ~ vent + nebulosite + vent:nebulosite, data = ozone)\nmod2 &lt;- lm(O3 ~ vent + nebulosite, data = ozone)\nanova(mod2, mod1)\n\nAnalysis of Variance Table\n\nModel 1: O3 ~ vent + nebulosite\nModel 2: O3 ~ vent + nebulosite + vent:nebulosite\n  Res.Df   RSS Df Sum of Sq     F Pr(&gt;F)\n1     45 11730                          \n2     42 11246  3    483.62 0.602 0.6173\n\n\n\nmod3 &lt;- lm(O3 ~ vent, data = ozone)\nanova(mod3, mod2)\n\nAnalysis of Variance Table\n\nModel 1: O3 ~ vent\nModel 2: O3 ~ vent + nebulosite\n  Res.Df   RSS Df Sum of Sq      F    Pr(&gt;F)    \n1     46 18131                                  \n2     45 11730  1    6401.5 24.558 1.066e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nanova(mod3, mod2, mod1)\n\nAnalysis of Variance Table\n\nModel 1: O3 ~ vent\nModel 2: O3 ~ vent + nebulosite\nModel 3: O3 ~ vent + nebulosite + vent:nebulosite\n  Res.Df   RSS Df Sum of Sq      F    Pr(&gt;F)    \n1     46 18131                                  \n2     45 11730  1    6401.5 23.907 1.523e-05 ***\n3     42 11246  3     483.6  0.602    0.6173    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nanova(mod1)\n\nAnalysis of Variance Table\n\nResponse: O3\n                Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nvent             3  9859.8  3286.6  12.274 6.689e-06 ***\nnebulosite       1  6401.5  6401.5  23.907 1.523e-05 ***\nvent:nebulosite  3   483.6   161.2   0.602    0.6173    \nResiduals       42 11246.2   267.8                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "code/chap3.html",
    "href": "code/chap3.html",
    "title": "3 Validation du modèle",
    "section": "",
    "text": "ozone &lt;- read.table(\"../donnees/ozone_long.txt\", header = T, sep = \";\")\nmod.lin6v &lt;- lm(O3~T6 + T12 + Ne12 + Ne15 + Vx + O3v,data=ozone)\n\n\nplot(rstudent(mod.lin6v), pch = \".\",ylab = \"Résidus studentisés par VC\")\nabline(h = c(-2,2))\nlines(lowess(rstudent(mod.lin6v)))\n\n\n\n\n\nplot(mod.lin6v, which = 2, sub = \"\", main = \"\")\nabline(0,1)\n\n\n\n\n\nplot(cooks.distance(mod.lin6v),type=\"h\",ylab=\"Distance de Cook\")\np &lt;- ncol(ozone) ; n &lt;- nrow(ozone)\nseuil1 &lt;- qf(0.1,p,n-p) ; abline(h=seuil1)\n\n\n\ninfl.ozone &lt;- influence.measures(mod.lin6v)\nplot(infl.ozone$infmat[,\"hat\"],type=\"h\",ylab=\"hii\")\nseuil1 &lt;- 3*p/n ; abline(h=seuil1,col=1,lty=2)\nseuil2 &lt;- 2*p/n ; abline(h=seuil2,col=1,lty=3)\n\n\n\n\n\nresidpartiels &lt;- resid(mod.lin6v, type = \"partial\")\nprov &lt;- loess(residpartiels[,\"O3v\"] ~ ozone$O3v)\nordre &lt;- order(ozone$O3v)\nplot(ozone$O3v, residpartiels[,\"O3v\"], pch=\".\",ylab=\"\",xlab=\"\")\nmatlines(ozone$O3v[ordre], predict(prov)[ordre])\nabline(lsfit(ozone$O3v, residpartiels[,\"O3v\"]), lty = 2)"
  },
  {
    "objectID": "code/chap1.html",
    "href": "code/chap1.html",
    "title": "1 Régression simple",
    "section": "",
    "text": "La concentration en ozone\n\nozone &lt;- read.table(\"../donnees/ozone_simple.txt\",header=TRUE,sep=\";\")\nplot(O3~T12, data=ozone, xlab=\"T12\", ylab=\"O3\")\n\n\n\n\n\nreg &lt;- lm(O3~T12, data=ozone)\nsummary(reg)\n\n\nCall:\nlm(formula = O3 ~ T12, data = ozone)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-45.256 -15.326  -3.461  17.634  40.072 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  31.4150    13.0584   2.406     0.02 *  \nT12           2.7010     0.6266   4.311 8.04e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 20.5 on 48 degrees of freedom\nMultiple R-squared:  0.2791,    Adjusted R-squared:  0.2641 \nF-statistic: 18.58 on 1 and 48 DF,  p-value: 8.041e-05\n\n\n\nplot(O3~T12, data=ozone)\nT12 &lt;- seq(min(ozone[,\"T12\"]), max(ozone[,\"T12\"]), length = 100)\ngrille &lt;- data.frame(T12)\nICdte &lt;- predict(reg, new=grille, interval=\"conf\", level=0.95)\nmatlines(grille$T12, cbind(ICdte), lty = c(1,2,2), col = 1)\n\n\n\n\n\nplot(O3~T12, data = ozone, ylim = c(0,150))\nT12 &lt;- seq(min(ozone[,\"T12\"]), max(ozone[,\"T12\"]), length = 100)\ngrille &lt;- data.frame(T12)\nICdte &lt;- predict(reg, new=grille, interval=\"conf\", level=0.95)\nICprev &lt;- predict(reg, new=grille, interval=\"pred\", level=0.95)\nmatlines(T12, cbind(ICdte,ICprev[,-1]), lty=c(1,2,2,3,3), col=1)\nlegend(\"topleft\", lty=2:3, c(\"Y\",\"E(Y)\"))\n\n\n\n\n\nIC &lt;- confint(reg, level = 0.95)\nIC\n\n               2.5 %   97.5 %\n(Intercept) 5.159232 57.67071\nT12         1.441180  3.96089\n\n\n\nlibrary(ellipse)\nplot(ellipse(reg, level=0.95), type = \"l\", xlab = \"\", ylab = \"\")\npoints(coef(reg)[1], coef(reg)[2], pch = 3)\nlines(IC[1,c(1,1,2,2,1)], IC[2,c(1,2,2,1,1)], lty = 2)\n\n\n\n\n\n\nLa hauteur des eucalyptus\n\neucalypt &lt;- read.table(\"../donnees/eucalyptus.txt\", header = T, sep = \";\")\nplot(ht~circ, data = eucalypt, xlab = \"circ\", ylab = \"ht\")\n\n\n\n\n\nreg &lt;- lm(ht~circ, data = eucalypt)\nsummary(reg)\n\n\nCall:\nlm(formula = ht ~ circ, data = eucalypt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.7659 -0.7802  0.0557  0.8271  3.6913 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 9.037476   0.179802   50.26   &lt;2e-16 ***\ncirc        0.257138   0.003738   68.79   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.199 on 1427 degrees of freedom\nMultiple R-squared:  0.7683,    Adjusted R-squared:  0.7682 \nF-statistic:  4732 on 1 and 1427 DF,  p-value: &lt; 2.2e-16\n\n\n\nplot(ht~circ, data = eucalypt, pch = \"+\", col = \"grey60\")\ngrille &lt;- data.frame(circ = seq(min(eucalypt[,\"circ\"]),\n                                max(eucalypt[,\"circ\"]), length = 100))\nICdte &lt;- predict(reg, new=grille, interval=\"confi\", level=0.95)\nmatlines(grille$circ, ICdte, lty = c(1,2,2), col = 1)\n\n\n\n\n\nplot(ht~circ, data = eucalypt, pch = \"+\", col = \"grey60\")\ncirc &lt;- seq(min(eucalypt[,\"circ\"]),max(eucalypt[,\"circ\"]), len = 100)\ngrille &lt;- data.frame(circ)\nCdte &lt;- predict(reg, new=grille, interval=\"conf\", level=0.95)\nICprev &lt;- predict(reg, new=grille, interval=\"pred\", level=0.95)\nmatlines(circ, cbind(ICdte,ICprev[,-1]),lty=c(1,2,2,3,3), col=1)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "3ème édition",
    "section": "",
    "text": "Les auteurs\n\nPierre-André Cornillon\nNicolas Hengartner\nÉric Matzner-Løber\nLaurent Rouvière\n\n\n\nDescriptif\n\n4e de couverture\nAvant-propos\nSommaire détaillé\n\n\n\nBoutique en ligne\nPar ici"
  },
  {
    "objectID": "donnees.html",
    "href": "donnees.html",
    "title": "Jeux de données",
    "section": "",
    "text": "ad_data.txt\n\nartere.txt\n\neucalyptus.txt\n\nlogit_ex6.csv\nlogit_ridge_lasso.csv\nozone.txt\nozone_complet.txt\nozone_long.txt\nozone_simple.txt\nozone_transf.txt\npanne.txt\npoissonData.csv\npoissonData3.csv"
  },
  {
    "objectID": "donnees.html#liste-des-jeux-de-données-par-ordre-alphabétique",
    "href": "donnees.html#liste-des-jeux-de-données-par-ordre-alphabétique",
    "title": "Jeux de données",
    "section": "",
    "text": "ad_data.txt\n\nartere.txt\n\neucalyptus.txt\n\nlogit_ex6.csv\nlogit_ridge_lasso.csv\nozone.txt\nozone_complet.txt\nozone_long.txt\nozone_simple.txt\nozone_transf.txt\npanne.txt\npoissonData.csv\npoissonData3.csv"
  },
  {
    "objectID": "code/chap2.html",
    "href": "code/chap2.html",
    "title": "2 La régression linéaire multiple",
    "section": "",
    "text": "La concentration en ozone\n\nozone &lt;- read.table(\"../donnees/ozone.txt\", header = T, sep = \";\")\nlibrary(\"scatterplot3d\")\nscatterplot3d(ozone[,\"T12\"],ozone[,\"Vx\"],ozone[,\"O3\"],\n              type=\"h\",pch=16, box=FALSE, xlab=\"T12\", ylab=\"Vx\", zlab=\"O3\")\n\n\n\n\n\nregmulti &lt;- lm(O3~T12+Vx, data = ozone)\nsummary(regmulti)\n\n\nCall:\nlm(formula = O3 ~ T12 + Vx, data = ozone)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-42.984 -10.152  -2.407  11.710  34.494 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  35.4530    10.7446   3.300  0.00185 ** \nT12           2.5380     0.5151   4.927 1.08e-05 ***\nVx            0.8736     0.1772   4.931 1.06e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.82 on 47 degrees of freedom\nMultiple R-squared:  0.5249,    Adjusted R-squared:  0.5047 \nF-statistic: 25.96 on 2 and 47 DF,  p-value: 2.541e-08\n\n\n\n\nLa hauteur des eucalyptus\n\neucalypt &lt;- read.table(\"../donnees/eucalyptus.txt\", header = T, sep = \";\")\nplot(ht~circ, data = eucalypt, xlab = \"circ\", ylab = \"ht\")\n\n\n\n\n\nregmult &lt;- lm(ht ~ circ + I(sqrt(circ)), data = eucalypt)\nresume.mult &lt;- summary(regmult)\nresume.mult\n\n\nCall:\nlm(formula = ht ~ circ + I(sqrt(circ)), data = eucalypt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.1881 -0.6881  0.0427  0.7927  3.7481 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -24.35200    2.61444  -9.314   &lt;2e-16 ***\ncirc           -0.48295    0.05793  -8.336   &lt;2e-16 ***\nI(sqrt(circ))   9.98689    0.78033  12.798   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.136 on 1426 degrees of freedom\nMultiple R-squared:  0.7922,    Adjusted R-squared:  0.7919 \nF-statistic:  2718 on 2 and 1426 DF,  p-value: &lt; 2.2e-16\n\n\n\nplot(ht ~ circ, data = eucalypt, pch = \"+\", col = \"grey60\")\ngrille &lt;- data.frame(circ = seq(min(eucalypt[,\"circ\"]),max(eucalypt[,\"circ\"]), length = 100))\nlines(grille[,\"circ\"], predict(regmult, grille))"
  },
  {
    "objectID": "code/chap5.html",
    "href": "code/chap5.html",
    "title": "5 Inférence dans le modèle Gaussien",
    "section": "",
    "text": "ozone &lt;- read.table(\"../donnees/ozone.txt\", header = T, sep = \";\")\nmodele3 &lt;- lm(O3 ~ T12 + Vx + Ne12, data = ozone)\nresume3 &lt;- summary(modele3)\ncoef3 &lt;- coef(resume3)\nIC3 &lt;- t(confint(modele3, level = 0.95))\nIC3\n\n       (Intercept)       T12        Vx      Ne12\n2.5 %     57.15842 0.3138112 0.1491857 -6.960609\n97.5 %   111.93625 2.3162807 0.8237055 -2.826137\n\n\n\nlibrary(ellipse)\npar(mfrow=c(3,2))\nfor(i in 1:3){\n  for(j in (i+1):4){\n    plot(ellipse(modele3,c(i,j),level=0.95),type=\"l\",\n         xlab=paste(\"beta\",i,sep=\"\"),ylab=paste(\"beta\",j,sep=\"\"))\n    points(coef(modele3)[i], coef(modele3)[j],pch=3)\n    lines(c(IC3[1,i],IC3[1,i],IC3[2,i],IC3[2,i],IC3[1,i]),\n          c(IC3[1,j],IC3[2,j],IC3[2,j],IC3[1,j],IC3[1,j]),lty=2)\n  }\n}\n\n\n\n\n\nc(resume3$sigma^2*modele3$df.res/qchisq(0.975,modele3$df.res),\n  resume3$sigma^2*modele3$df.res/qchisq(0.025,modele3$df.res))\n\n[1] 133.6699 305.3706\n\n\n\nExemple 1 : la concentration en ozone\n\nmodele3 &lt;- lm(O3 ~ T12 + Vx + Ne12, data = ozone)\nresume3 &lt;- summary(modele3)\nresume3\n\n\nCall:\nlm(formula = O3 ~ T12 + Vx + Ne12, data = ozone)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-29.0461  -8.4824   0.7861   7.7024  28.2916 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  84.5473    13.6067   6.214 1.38e-07 ***\nT12           1.3150     0.4974   2.644  0.01117 *  \nVx            0.4864     0.1675   2.903  0.00565 ** \nNe12         -4.8934     1.0270  -4.765 1.93e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.91 on 46 degrees of freedom\nMultiple R-squared:  0.6819,    Adjusted R-squared:  0.6611 \nF-statistic: 32.87 on 3 and 46 DF,  p-value: 1.663e-11\n\n\n\nmodele2 &lt;- lm(O3 ~ T12 + Vx, data = ozone)\nanova(modele2, modele3)\n\nAnalysis of Variance Table\n\nModel 1: O3 ~ T12 + Vx\nModel 2: O3 ~ T12 + Vx + Ne12\n  Res.Df     RSS Df Sum of Sq      F    Pr(&gt;F)    \n1     47 13299.4                                  \n2     46  8904.6  1    4394.8 22.703 1.927e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nExemple 2 : la hauteur des eucalyptus\n\neucalypt &lt;- read.table(\"../donnees/eucalyptus.txt\", header = T, sep = \";\")\nregsimple &lt;- lm(ht ~ circ, data = eucalypt)\nregM &lt;- lm(ht ~ circ + I(sqrt(circ)), data = eucalypt)\nanova(regsimple, regM)\n\nAnalysis of Variance Table\n\nModel 1: ht ~ circ\nModel 2: ht ~ circ + I(sqrt(circ))\n  Res.Df    RSS Df Sum of Sq     F    Pr(&gt;F)    \n1   1427 2052.1                                 \n2   1426 1840.7  1    211.43 163.8 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nsummary(regM)\n\n\nCall:\nlm(formula = ht ~ circ + I(sqrt(circ)), data = eucalypt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.1881 -0.6881  0.0427  0.7927  3.7481 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -24.35200    2.61444  -9.314   &lt;2e-16 ***\ncirc           -0.48295    0.05793  -8.336   &lt;2e-16 ***\nI(sqrt(circ))   9.98689    0.78033  12.798   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.136 on 1426 degrees of freedom\nMultiple R-squared:  0.7922,    Adjusted R-squared:  0.7919 \nF-statistic:  2718 on 2 and 1426 DF,  p-value: &lt; 2.2e-16\n\n\n\ngrille &lt;- data.frame(circ = seq(min(eucalypt[,\"circ\"]),\n                                max(eucalypt[,\"circ\"]), len = 100))\nICdte &lt;- predict(regM,new=grille,interval=\"conf\",level=0.95)\nICpre &lt;- predict(regM,new=grille,interval=\"pred\",level=0.95)\nplot(ht ~ circ, data = eucalypt, pch=\"+\", col=\"grey60\")\nmatlines(grille,cbind(ICdte,ICpre[,-1]),lty=c(1,2,2,3,3),col=1)\nlegend(\"topleft\", lty=2:3, c(\"E(Y)\",\"Y\"))\n\n\n\n\n\n\nIntervalle de confiance bootstrap\n\nmodele3 &lt;- lm(O3 ~ T12 + Vx + Ne12, data = ozone)\n\n\nresume3 &lt;- summary(modele3)\nresume3$coef[,1:2]\n\n              Estimate Std. Error\n(Intercept) 84.5473326 13.6067253\nT12          1.3150459  0.4974102\nVx           0.4864456  0.1675496\nNe12        -4.8933729  1.0269960\n\n\n\nres &lt;- residuals(modele3)\nychap &lt;- predict(modele3)\nCOEFF &lt;- matrix(0, ncol = 4, nrow = 1000)\ncolnames(COEFF) &lt;- names(coef(modele3))\nozone.boot &lt;- ozone\n\n\nfor(i in 1:nrow(COEFF)){\n  resetoile &lt;- sample(res, length(res), replace=T)\n  O3etoile &lt;- ychap + resetoile\n  ozone.boot[,\"O3\"] &lt;- O3etoile\n  regboot &lt;- lm(formula(modele3), data=ozone.boot)\n  COEFF[i,] &lt;- coef(regboot)\n }\n\n\napply(COEFF, 2, quantile, probs = c(0.025,0.975))\n\n      (Intercept)       T12        Vx      Ne12\n2.5%     59.31069 0.3940245 0.2036312 -6.775764\n97.5%   108.67459 2.2270745 0.7974850 -2.900857\n\n\n\nhist(COEFF[,\"T12\"], main = \"\", xlab = \"Coefficient de T12\")"
  },
  {
    "objectID": "code/chap7.html",
    "href": "code/chap7.html",
    "title": "7 Choix de variables",
    "section": "",
    "text": "ozone &lt;- read.table(\"../donnees/ozone.txt\", header = T, sep = \";\")\nlibrary(leaps)\nrecherche &lt;- regsubsets(O3 ~ T12 + T15 + Ne12 + N12 + S12 + E12 + W12 + Vx + O3v, int = T,nbest = 1, nvmax = 10, method = \"exhaustive\", data = ozone)\n\n\nplot(recherche, scale = \"bic\")\n\n\n\nplot(recherche, scale = \"Cp\")\n\n\n\nplot(recherche, scale = \"adjr2\")\n\n\n\nplot(recherche, scale = \"r2\")\n\n\n\n\n\nresume &lt;- summary(recherche)\nnomselec &lt;- colnames(resume$which)[resume$which[which.min(resume$bic),]][-1]\nformule &lt;- formula(paste(\"O3~\",paste(nomselec,collapse=\"+\")))\nmodeleslectionne &lt;- lm(formule, data = ozone)\nmodeleslectionne\n\n\nCall:\nlm(formula = formule, data = ozone)\n\nCoefficients:\n(Intercept)          T15         Ne12           Vx          O3v  \n    61.8252       1.0577      -3.9935       0.3146       0.2629"
  },
  {
    "objectID": "code/chap9.html",
    "href": "code/chap9.html",
    "title": "9 Régression sur composantes : PCR et PLS",
    "section": "",
    "text": "Régression MCO et choix de variables\n\nozone &lt;- read.table(\"../donnees/ozone.txt\",header=TRUE,sep=\";\",row.names=1)\nmodeleinit &lt;- lm(O3 ~ ., data = ozone[,1:10])\nround(coefficients(modeleinit),2)\n\n(Intercept)         T12         T15        Ne12         N12         S12 \n      54.73       -0.35        1.50       -4.19        1.28        3.17 \n        E12         W12          Vx         O3v \n       0.53        2.47        0.61        0.25 \n\nBIC(modeleinit)\n\n[1] 431.8923\n\n\n\nlibrary(leaps)\nchoix &lt;- regsubsets(O3 ~ .,nbest=1,nvmax=10,data=ozone[,1:10])\nresume &lt;- summary(choix)\nindmin &lt;- which.min(resume$bic)\nnomselec &lt;- colnames(resume$which)[resume$which[indmin,]][-1]\nformule &lt;- formula(paste(\"O3~\",paste(nomselec,collapse=\"+\")))\nmodeleBIC &lt;- lm(formule,data=ozone[,1:10])\nround(coefficients(modeleBIC),2)\n\n(Intercept)         T15        Ne12          Vx         O3v \n      61.83        1.06       -3.99        0.31        0.26 \n\nBIC(modeleBIC)\n\n[1] 415.8866\n\n\n\n\nMise en place des données centrées réduites\n\nX &lt;- ozone[,2:10]\nXbar  &lt;- apply(X, 2, mean)\nstdX &lt;- sqrt(apply(X, 2, var))\nXcr &lt;- scale(X, center = Xbar, scale = stdX)\n\n\n\nPCR\n\nlibrary(pls)\nset.seed(87)\ncvseg &lt;- cvsegments(nrow(ozone), k = 4, type = \"random\")\nn.app &lt;- nrow(ozone)\nmodele.pcr &lt;- pcr(O3 ~ ., ncomp=9, data=ozone[,1:10], scale=T,\n                   validation = \"CV\", segments = cvseg)\nmsepcv.pcr &lt;- MSEP(modele.pcr ,estimate=c(\"train\",\"CV\")) \nmsepcv.pcr\n\n       (Intercept)  1 comps  2 comps  3 comps  4 comps  5 comps  6 comps\ntrain        559.8    188.7    186.5    185.2    179.8    158.8    152.0\nCV           582.9    260.4    260.6    278.2    271.3    239.3    248.1\n       7 comps  8 comps  9 comps\ntrain    143.9    142.5    139.7\nCV       242.1    244.0    239.4\n\n\n\nnpcr &lt;- which.min(msepcv.pcr$val[\"CV\",,])-1\nmodele.pcr.fin &lt;- pcr(O3 ~ ., ncomp = npcr, scale = TRUE,data = ozone[,1:10])\n\n\nX &lt;- ozone[,2:10]\nY &lt;- ozone[,1]\nn &lt;- nrow(X)\nXbar  &lt;- apply(X,2,mean)\nstdX &lt;- sqrt(apply(X,2,var)*(n-1)/n)\nYbar &lt;- mean(Y)\nmodele.pcr.fin &lt;- pcr(O3~.,ncomp=1,scale=TRUE,data =ozone[,1:10])\nbetafinpcr &lt;- matrix(coefficients(modele.pcr.fin),ncol=1)/stdX\nmu &lt;- mean(Y)-Xbar%*%betafinpcr\nc(mu,betafinpcr)\n\n [1] 48.3373569  0.7884931  0.8236705 -1.7618991 -0.6760911  0.2126764\n [7]  1.6099503 -1.4694047  0.3118176  0.1662505\n\n\n\n\nPLS\n\nset.seed(87)\ncvseg &lt;- cvsegments(nrow(ozone), k = 4, type = \"random\")\nn.app &lt;- nrow(ozone)\nmodele.pls &lt;- plsr(O3 ~ ., ncomp=9, data = ozone[,1:10], scale=T,validation = \"CV\", segments = cvseg)\nmsepcv.pls &lt;- MSEP(modele.pls ,estimate=c(\"train\",\"CV\")) \nmsepcv.pls\n\n       (Intercept)  1 comps  2 comps  3 comps  4 comps  5 comps  6 comps\ntrain        559.8    173.9    150.9    146.9    144.2    142.1    141.4\nCV           582.9    251.9    248.4    245.3    234.6    241.7    243.6\n       7 comps  8 comps  9 comps\ntrain    140.9    139.8    139.7\nCV       234.7    239.6    239.4\n\n\n\nnpls &lt;- which.min(msepcv.pls$val[\"CV\",,])-1\nmodele.pls.fin &lt;- plsr(O3~ . , ncomp  =npls, scale = TRUE,data = ozone[,1:10])\n\n\nX &lt;- ozone[,2:10]\nY &lt;- ozone[,1]\nn &lt;- nrow(X)\nXbar  &lt;- apply(X,2,mean)\nstdX &lt;- sqrt(apply(X,2,var)*(n-1)/n)\nYbar &lt;- mean(Y)\nmodele.pls.fin &lt;- plsr(O3~.,ncomp=1,scale=TRUE,data =ozone[,1:10])\nbetafinpls &lt;- matrix(coefficients(modele.pls.fin),ncol=1)/stdX\nmu &lt;- mean(Y)-Xbar%*%betafinpls\nc(mu,betafinpls)\n\n [1] 47.1060475  0.8151834  0.8471154 -2.1910455 -0.5175855  0.3563688\n [7]  1.3005636 -1.2552108  0.2805513  0.1920499"
  },
  {
    "objectID": "code/chap11.html",
    "href": "code/chap11.html",
    "title": "11 Régression logistique",
    "section": "",
    "text": "Présentation du modèle\n\nartere &lt;- read.table(\"../donnees/artere.txt\",header=T)\nplot(chd~age,data=artere,pch=16)\n\n\n\n\n\ntab_freq &lt;- table(artere$agrp,artere$chd)\nfreq &lt;- tab_freq[,2]/apply(tab_freq,1,sum)\ncbind(tab_freq,round(freq,3))\n\n   0  1      \n1  9  1 0.100\n2 13  2 0.133\n3  9  3 0.250\n4 10  5 0.333\n5  7  6 0.462\n6  3  5 0.625\n7  4 13 0.765\n8  2  8 0.800\n\nx.age &lt;- c(19,29,34,39,44,49,54,59)\nplot(x.age,c(freq),type=\"s\",xlim=c(18,80),ylim=c(0,1),xlab=\"âge\",ylab=\"freq\")\nlines(c(59,80),rep(freq[length(freq)],2))\nx &lt;- seq(15,80,by=0.01)\ny &lt;- exp(-5.31+0.11*x)/(1+exp(-5.31+0.11*x))\nlines(x,y,lty=3)\n\n\n\n\n\nglm(chd~age,data=artere,family=binomial)\n\n\nCall:  glm(formula = chd ~ age, family = binomial, data = artere)\n\nCoefficients:\n(Intercept)          age  \n    -5.3095       0.1109  \n\nDegrees of Freedom: 99 Total (i.e. Null);  98 Residual\nNull Deviance:      136.7 \nResidual Deviance: 107.4    AIC: 111.4\n\n\n\nset.seed(12345)\nX &lt;- factor(sample(c(\"A\",\"B\",\"C\"),100,replace=T))\n#levels(X) &lt;- c(\"A\",\"B\",\"C\")\nY &lt;- rep(0,100)\nY[X==\"A\"] &lt;- rbinom(sum(X==\"A\"),1,0.9)\nY[X==\"B\"] &lt;- rbinom(sum(X==\"B\"),1,0.1)\nY[X==\"C\"] &lt;- rbinom(sum(X==\"C\"),1,0.9)\ndonnees &lt;- data.frame(X,Y)\nmodel &lt;- glm(Y~.,data=donnees,family=binomial)\ncoef(model)\n\n(Intercept)          XB          XC \n  2.0794415  -3.6549779   0.3772942 \n\nmodel1 &lt;- glm(Y~C(X,sum),data=donnees,family=binomial)\ncoef(model1)\n\n(Intercept)  C(X, sum)1  C(X, sum)2 \n  0.9868803   1.0925612  -2.5624167 \n\n\n\n\nIntervalles de confiance et tests\n\nlibrary(bestglm)\ndata(SAheart)\nnew.SAheart &lt;- SAheart[c(2,408,35),]\nrow.names(new.SAheart) &lt;- NULL\nSAheart &lt;- SAheart[-c(2,408,35),]\nmodel &lt;- glm(chd~.,data=SAheart,family=binomial)\nround(summary(model)$coefficients,4)\n\n               Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)     -6.0837     1.3141 -4.6294   0.0000\nsbp              0.0065     0.0058  1.1268   0.2598\ntobacco          0.0814     0.0269  3.0232   0.0025\nldl              0.1794     0.0600  2.9891   0.0028\nadiposity        0.0184     0.0295  0.6224   0.5337\nfamhistPresent   0.9325     0.2291  4.0694   0.0000\ntypea            0.0392     0.0123  3.1845   0.0015\nobesity         -0.0637     0.0446 -1.4300   0.1527\nalcohol          0.0002     0.0045  0.0346   0.9724\nage              0.0439     0.0122  3.5923   0.0003\n\nconfint.default(model)\n\n                      2.5 %       97.5 %\n(Intercept)    -8.659355064 -3.507983650\nsbp            -0.004797560  0.017773140\ntobacco         0.028628110  0.134174033\nldl             0.061770934  0.297043348\nadiposity      -0.039461469  0.076187468\nfamhistPresent  0.483354369  1.381572764\ntypea           0.015090098  0.063396115\nobesity        -0.151054913  0.023612482\nalcohol        -0.008639577  0.008950096\nage             0.019931097  0.067793230\n\n\n\nn &lt;- 1000\nset.seed(123)\nX1 &lt;- sample(c(\"A\",\"B\",\"C\"),n,replace=TRUE)\nX2 &lt;- rnorm(n)\nX3 &lt;- runif(n)\ncl &lt;- 1+0*(X1==\"A\")+1*(X1==\"B\")-3*(X1==\"C\")+2*X2\nY &lt;- rbinom(n,1,exp(cl)/(1+exp(cl)))\ndonnees &lt;- data.frame(X1,X2,X3,Y)\n\n\nm1 &lt;- glm(Y~.,data=donnees,family=binomial)\nlibrary(car)\nAnova(m1,type=3,test.statistic=\"Wald\")\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: Y\n            Df    Chisq Pr(&gt;Chisq)    \n(Intercept)  1  28.4698  9.517e-08 ***\nX1           2 212.5061  &lt; 2.2e-16 ***\nX2           1 210.3902  &lt; 2.2e-16 ***\nX3           1   0.3096     0.5779    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nAnova(m1,type=3,test.statistic=\"LR\")\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: Y\n   LR Chisq Df Pr(&gt;Chisq)    \nX1   376.74  2     &lt;2e-16 ***\nX2   417.66  1     &lt;2e-16 ***\nX3     0.31  1     0.5778    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nm01 &lt;- glm(Y~X2+X3,data=donnees,family=binomial)\nm02 &lt;- glm(Y~X1+X3,data=donnees,family=binomial)\nm03 &lt;- glm(Y~X1+X2,data=donnees,family=binomial)\nanova(m01,m1,test=\"LRT\")\n\nAnalysis of Deviance Table\n\nModel 1: Y ~ X2 + X3\nModel 2: Y ~ X1 + X2 + X3\n  Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    \n1       997    1109.67                          \n2       995     732.93  2   376.74 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(m02,m1,test=\"LRT\")\n\nAnalysis of Deviance Table\n\nModel 1: Y ~ X1 + X3\nModel 2: Y ~ X1 + X2 + X3\n  Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    \n1       996    1150.59                          \n2       995     732.93  1   417.66 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(m03,m1,test=\"LRT\")\n\nAnalysis of Deviance Table\n\nModel 1: Y ~ X1 + X2\nModel 2: Y ~ X1 + X2 + X3\n  Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)\n1       996     733.24                     \n2       995     732.93  1  0.30976   0.5778\n\nlibrary(aod)\nwald.test(Sigma=vcov(m1),b=coef(m1),Terms=c(2,3))\n\nWald test:\n----------\n\nChi-squared test:\nX2 = 212.5, df = 2, P(&gt; X2) = 0.0\n\n\n\n\nPrévisions\n\nmodel &lt;- glm(chd~.,data=SAheart,family=binomial)\n\n\nnew.SAheart &lt;- SAheart[c(2,408,35),-10]\nrow.names(new.SAheart) &lt;- NULL\nnew.SAheart\n\n  sbp tobacco  ldl adiposity famhist typea obesity alcohol age\n1 118    0.08 3.48     32.28 Present    52   29.14    3.81  46\n2 178   20.00 9.78     33.55  Absent    37   27.29    2.88  62\n3 140    3.90 7.32     25.05  Absent    47   27.36   36.77  32\n\n\n\npredict(model, newdata=new.SAheart)\n\n         1          2          3 \n-0.9599837  1.5028033 -1.5743496 \n\n\n\npredict(model, newdata=new.SAheart,type=\"response\")\n\n        1         2         3 \n0.2768815 0.8179922 0.1715972 \n\n\n\nprev &lt;- predict(model,newdata=new.SAheart,type=\"link\",se.fit = TRUE)\ncl_inf &lt;- prev$fit-qnorm(0.975)*prev$se.fit\ncl_sup &lt;- prev$fit+qnorm(0.975)*prev$se.fit\nbinf &lt;- exp(cl_inf)/(1+exp(cl_inf))\nbsup &lt;- exp(cl_sup)/(1+exp(cl_sup))\ndata.frame(binf,bsup)\n\n       binf      bsup\n1 0.1774323 0.4046504\n2 0.6040315 0.9297800\n3 0.1024782 0.2731474\n\n\n\nunique(artere[,\"age\"])\n\n [1] 20 23 24 25 26 28 29 30 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n[26] 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 69\n\nsature &lt;- aggregate(artere[,\"chd\"],by=list(artere$age),FUN=mean)\nnames(sature) &lt;- c(\"age\",\"p\")\nndesign &lt;- aggregate(artere[,\"chd\"],by=list(artere$age),FUN=length)\nnames(ndesign) &lt;- c(\"age\",\"n\")\nmerge(sature,ndesign,by=\"age\")[1:5,]\n\n  age   p n\n1  20 0.0 1\n2  23 0.0 1\n3  24 0.0 1\n4  25 0.5 2\n5  26 0.0 2\n\nplot(chd~age,data=artere,pch=15+chd,col=chd+1)\nlines(p~age,data=sature)\n\n\n\n\n\nmodel &lt;- glm(chd~.,data=SAheart,family=binomial)\nlibrary(generalhoslem)\nlogitgof(obs= SAheart$chd, exp = fitted(model))\n\n\n    Hosmer and Lemeshow test (binary model)\n\ndata:  SAheart$chd, fitted(model)\nX-squared = 6.6586, df = 8, p-value = 0.5739\n\n\n\nmodel &lt;- glm(chd~.,data=SAheart,family=binomial)\nprev_lin &lt;- predict(model)\nres_P &lt;- residuals(model,type=\"pearson\") #Pearson\nres_PS &lt;- rstandard(model,type=\"pearson\") #Pearson standard\nres_D &lt;- residuals(model,type=\"deviance\")  #Deviance\nres_DS &lt;- rstandard(model,type=\"deviance\") #Deviance standard\n\n\npar(mfrow=c(2,2),pch=20,mai = c(0.1,0.15,0.1,0.1),mar=c(3,3,1,1),cex.axis=0.6,cex.lab=0.7,mgp=c(1.5,0.3,0),oma=c(1,0,0,0),tcl=-0.4)\nplot(res_PS,cex=0.3,xlab=\"index\",ylab=\"Pearson Standard\")\nplot(prev_lin,cex=0.3,res_PS,xlab=\"Prevision lineaire\",ylab=\"Pearson Standard\")\nplot(res_DS,cex=0.3,xlab=\"index\",ylab=\"Deviance Standard\")\nplot(prev_lin,cex=0.3,res_DS,xlab=\"Prevision lineaire\",ylab=\"Deviance Standard\")\n\n\n\n\n\n\nChoix de variables\n\nmodel0 &lt;- glm(chd~sbp+ldl,data=SAheart,family=binomial)\nmodel1 &lt;- glm(chd~sbp+ldl+famhist+alcohol,data=SAheart,family=binomial)\nanova(model0,model1,test=\"LRT\")\n\nAnalysis of Deviance Table\n\nModel 1: chd ~ sbp + ldl\nModel 2: chd ~ sbp + ldl + famhist + alcohol\n  Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    \n1       456     548.18                          \n2       454     522.64  2   25.545 2.838e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\ndata(SAheart)\nmod_sel &lt;- bestglm(SAheart,family=binomial,IC=\"BIC\")\nmod_sel$BestModels\n\n    sbp tobacco   ldl adiposity famhist typea obesity alcohol  age Criterion\n1 FALSE    TRUE  TRUE     FALSE    TRUE  TRUE   FALSE   FALSE TRUE  506.3634\n2 FALSE    TRUE FALSE     FALSE    TRUE  TRUE   FALSE   FALSE TRUE  509.2566\n3 FALSE    TRUE  TRUE     FALSE    TRUE FALSE   FALSE   FALSE TRUE  509.9861\n4 FALSE   FALSE  TRUE     FALSE    TRUE  TRUE   FALSE   FALSE TRUE  510.5745\n5 FALSE    TRUE  TRUE     FALSE    TRUE  TRUE    TRUE   FALSE TRUE  510.7933\n\nmod_sel1 &lt;- bestglm(SAheart,family=binomial,IC=\"AIC\")\nmod_sel1$BestModels\n\n    sbp tobacco  ldl adiposity famhist typea obesity alcohol  age Criterion\n1 FALSE    TRUE TRUE     FALSE    TRUE  TRUE   FALSE   FALSE TRUE  485.6856\n2 FALSE    TRUE TRUE     FALSE    TRUE  TRUE    TRUE   FALSE TRUE  485.9799\n3  TRUE    TRUE TRUE     FALSE    TRUE  TRUE    TRUE   FALSE TRUE  486.5490\n4  TRUE    TRUE TRUE     FALSE    TRUE  TRUE   FALSE   FALSE TRUE  486.6548\n5 FALSE    TRUE TRUE      TRUE    TRUE  TRUE   FALSE   FALSE TRUE  487.4435\n\n\n\n\nScoring\n\nset.seed(1234)\nind.app &lt;- sample(nrow(SAheart),300)\ndapp &lt;- SAheart[ind.app,]\ndval &lt;- SAheart[-ind.app,]\n#Construction des modeles\nmodel1 &lt;- glm(chd~tobacco+famhist,data=dapp,family=binomial)\nmodel2 &lt;- glm(chd~tobacco+famhist+adiposity+alcohol,\n                data=dapp,family=binomial)  \nround(coef(model1),3)\n\n   (Intercept)        tobacco famhistPresent \n        -1.784          0.140          1.095 \n\nround(coef(model2),3)\n\n   (Intercept)        tobacco famhistPresent      adiposity        alcohol \n        -3.180          0.117          1.022          0.059         -0.002 \n\nprev1 &lt;- round(predict(model1,newdata=dval,type=\"response\"))\nprev2 &lt;- round(predict(model2,newdata=dval,type=\"response\"))\nmean(prev1!=dval$chd)\n\n[1] 0.3395062\n\nmean(prev2!=dval$chd)\n\n[1] 0.3395062\n\n\n\nset.seed(1245)\nbloc &lt;- sample(1:10,nrow(SAheart),replace=TRUE)\ntable(bloc)\n\nbloc\n 1  2  3  4  5  6  7  8  9 10 \n52 39 44 62 49 36 47 38 53 42 \n\n\n\nprev &lt;- data.frame(matrix(0,nrow=nrow(SAheart),ncol=2))\nnames(prev) &lt;- c(\"model1\",\"model2\")\nfor (k in 1:10){\n  ind.val &lt;- bloc==k\n  dapp.k &lt;- SAheart[!ind.val,]\n  dval.k &lt;- SAheart[ind.val,]\n  model1 &lt;- glm(chd~tobacco+famhist,data=dapp.k,family=binomial)\n  model2 &lt;- glm(chd~tobacco+famhist+adiposity+alcohol,data=dapp.k,family=binomial)  \n  prev[ind.val,1] &lt;- round(predict(model1,newdata=dval.k,type=\"response\"))\n  prev[ind.val,2] &lt;- round(predict(model2,newdata=dval.k,type=\"response\"))\n}\napply(sweep(prev,1,SAheart$chd,FUN=\"!=\"),2,mean)\n\n   model1    model2 \n0.3203463 0.3073593 \n\n\n\nscore1 &lt;- predict(model1,newdata=dval)\nscore2 &lt;- predict(model2,newdata=dval)\n\n\nlibrary(pROC)\nR1 &lt;- roc(dval$chd,score1)\nR2 &lt;- roc(dval$chd,score2)\nplot(R1,lwd=3,legacy.axes=TRUE)\nplot(R2,lwd=3,col=\"red\",lty=2,legacy.axes=TRUE,add=TRUE)\ncouleur &lt;- c(\"black\",\"red\")\nlegend(\"bottomright\",legend=c(\"score1\",\"score2\"),col=couleur,lty=1:2,lwd=2,cex=0.75)\n\n\n\n\n\nauc(R1)\n\nArea under the curve: 0.7356\n\nauc(R2)\n\nArea under the curve: 0.7372\n\n\n\nscore &lt;- data.frame(matrix(0,nrow=nrow(SAheart),ncol=2))\nnames(score) &lt;- c(\"score1\",\"score2\")\nfor (k in 1:10){\n  ind.val &lt;- bloc==k\n  dapp.k &lt;- SAheart[!ind.val,]\n  dval.k &lt;- SAheart[ind.val,]\n  model1 &lt;- glm(chd~tobacco+famhist,data=dapp.k,family=binomial)\n  model2 &lt;- glm(chd~tobacco+famhist+adiposity+alcohol,data=dapp.k,family=binomial)  \n  score[ind.val,1] &lt;- predict(model1,newdata=dval.k)\n  score[ind.val,2] &lt;- predict(model2,newdata=dval.k)\n}\n\n\nscore$obs &lt;- SAheart$chd\nroc.cv &lt;- roc(obs~score1+score2,data=score)\ncouleur &lt;- c(\"black\",\"red\")\nmapply(plot,roc.cv,col=couleur,lty=1:2,add=c(F,T),lwd=3,legacy.axes=TRUE)\n\n                   score1      score2     \npercent            FALSE       FALSE      \nsensitivities      numeric,356 numeric,463\nspecificities      numeric,356 numeric,463\nthresholds         numeric,356 numeric,463\ndirection          \"&lt;\"         \"&lt;\"        \ncases              numeric,160 numeric,160\ncontrols           numeric,302 numeric,302\nfun.sesp           ?           ?          \nauc                0.7159872   0.7271937  \ncall               expression  expression \noriginal.predictor numeric,462 numeric,462\noriginal.response  integer,462 integer,462\npredictor          numeric,462 numeric,462\nresponse           integer,462 integer,462\nlevels             character,2 character,2\npredictor.name     \"score1\"    \"score2\"   \nresponse.name      \"obs\"       \"obs\"      \n\nlegend(\"bottomright\",legend=c(\"score1\",\"score2\"),col=couleur,lty=1:2,lwd=2,cex=0.75)\n\n\n\n\n\nsort(round(unlist(lapply(roc.cv,auc)),3),decreasing=TRUE)\n\nscore2 score1 \n 0.727  0.716"
  },
  {
    "objectID": "code/chap13.html",
    "href": "code/chap13.html",
    "title": "13 Régularisation de la vraisemblance",
    "section": "",
    "text": "Régressions pénalisées avec glmnet\n\nlibrary(bestglm)\ndata(SAheart)\nSAheart.X &lt;- model.matrix(chd~.,data=SAheart)[,-1]\nSAheart.Y &lt;- SAheart$chd \nlibrary(glmnet)\nridge &lt;- glmnet(SAheart.X,SAheart.Y,family=\"binomial\",alpha=0)\nlasso &lt;- glmnet(SAheart.X,SAheart.Y,family=\"binomial\",alpha=1)\npar(mfrow=c(2,2))\nplot(ridge,ylim=c(-0.1,0.2))\nplot(lasso,ylim=c(-0.1,0.2))\nplot(ridge,ylim=c(-0.1,0.2),xvar=\"lambda\")\nplot(lasso,ylim=c(-0.1,0.2),xvar=\"lambda\")\n\n\n\nplot(ridge,ylim=c(-0.1,0.2),cex.lab=0.5)\nplot(lasso,ylim=c(-0.1,0.2),cex.lab=0.5)\nplot(ridge,ylim=c(-0.1,0.2),xvar=\"lambda\",cex.lab=0.5)\nplot(lasso,ylim=c(-0.1,0.2),xvar=\"lambda\",cex.lab=0.5)\n\n\n\n\n\n\nValidation croisée\n\nset.seed(2398)\nm1.ridge &lt;- cv.glmnet(SAheart.X,SAheart.Y,family=\"binomial\",alpha=0)\nm1.lasso &lt;- cv.glmnet(SAheart.X,SAheart.Y,family=\"binomial\",alpha=1)\nm2.ridge &lt;- cv.glmnet(SAheart.X,SAheart.Y,family=\"binomial\",alpha=0,type.measure=\"class\")\nm2.lasso &lt;- cv.glmnet(SAheart.X,SAheart.Y,family=\"binomial\",alpha=1,type.measure=\"class\")\nm3.ridge &lt;- cv.glmnet(SAheart.X,SAheart.Y,family=\"binomial\",alpha=0,type.measure=\"auc\")\nm3.lasso &lt;- cv.glmnet(SAheart.X,SAheart.Y,family=\"binomial\",alpha=1,type.measure=\"auc\")\n\n\nm1.ridge$lambda.min\n\n[1] 0.01774595\n\nm1.ridge$lambda.1se\n\n[1] 0.2892148\n\n\n\npar(mfrow=c(3,2))\nplot(m1.ridge,main=\"Ridge\")\nplot(m1.lasso,main=\"Lasso\")\nplot(m2.ridge,main=\"Ridge\")\nplot(m2.lasso,main=\"Lasso\")\nplot(m3.ridge,main=\"Ridge\")\nplot(m3.lasso,main=\"Lasso\")\n\n\n\n\n\n\nGroup-lasso et elastic net\n\nlibrary(gglasso)\nX1 &lt;- c(rep(\"A\",60),rep(\"B\",90),rep(\"C\",50))\nX2 &lt;- c(rep(\"E\",40),rep(\"F\",60),rep(\"G\",55),rep(\"H\",45))\nset.seed(1298)\nX_3 &lt;- runif(200)\nset.seed(2381)\nY &lt;- round(runif(200))\ndonnees &lt;- data.frame(X1,X2,X_3,Y)\n\n\nD &lt;- model.matrix(Y~.,data=donnees)[,-1]\nlasso &lt;- glmnet(D,Y,alpha=1,lambda=exp(seq(-3,-5,length=100)))\ngroupe &lt;- c(1,1,2,2,2,3)\nlibrary(gglasso)\nY1 &lt;- 2*Y-1 \ng.lasso &lt;- gglasso(D,Y1,group=groupe,loss=\"logit\",lambda=exp(seq(-4.5,-5.5,length=100)))\nplot(lasso,xvar=\"lambda\",lwd=2,main=\"Lasso\")\n\n\n\nplot(g.lasso,main=\"Group-lasso\")\n\n\n\n\n\nlibrary(caret)\nalpha &lt;- seq(0,1,by=0.1)\nlambda &lt;- exp(seq(-7,2,length=100))\ngrille &lt;- expand.grid(alpha=alpha,lambda=lambda)\nctrl &lt;- trainControl(method=\"cv\")\nSAheart$chd &lt;- as.factor(SAheart$chd)\nset.seed(1234)\nsel &lt;- train(chd~.,data=SAheart,method=\"glmnet\",family=\"binomial\",trControl=ctrl,tuneGrid=grille)\nsel$bestTune\n\n    alpha     lambda\n747   0.7 0.05971442\n\ngetTrainPerf(sel)\n\n  TrainAccuracy TrainKappa method\n1     0.7489362  0.3759712 glmnet\n\n\n\n\nApplication : détection d’images publicitaires\n\nad.data &lt;- read.table(\"../donnees/ad_data.txt\",header=FALSE,sep=\",\",dec=\".\",na.strings = \"?\",strip.white = TRUE)\nnames(ad.data)[ncol(ad.data)] &lt;- \"Y\"\nad.data$Y &lt;- as.factor(ad.data$Y)\n\nad.data1 &lt;- na.omit(ad.data)\ndim(ad.data1)\n\n[1] 2359 1559\n\n\n\nset.seed(1234)\nind.app &lt;- sample(nrow(ad.data1),1800)\ndapp &lt;- ad.data1[ind.app,]\ndtest &lt;- ad.data1[-ind.app,]\n\n\nX.app &lt;- model.matrix(Y~.,data=dapp)[,-1]\nX.test &lt;- model.matrix(Y~.,data=dtest)[,-1]\nY.app &lt;- dapp$Y\nY.test &lt;- dtest$Y\n\n\nlogit &lt;- glm(Y~.,data=dapp,family=\"binomial\") \n\n\nset.seed(123)\nlasso.cv &lt;- cv.glmnet(X.app,Y.app,family=\"binomial\")\nridge.cv &lt;- cv.glmnet(X.app,Y.app,family=\"binomial\",alpha=0,lambda=exp(seq(-8,0,length=100)))\nen.cv &lt;- cv.glmnet(X.app,Y.app,family=\"binomial\",alpha=0.5)\n\n\npar(mfrow=c(1,3))\nplot(lasso.cv,main=\"Lasso\")\nplot(ridge.cv,main=\"Ridge\")\nplot(en.cv,main=\"Elastic net\")\n\n\n\n\n\nscore &lt;- data.frame(obs=dtest$Y,logit=predict(logit,newdata=dtest,type=\"response\"),\n                                lasso=as.vector(predict(lasso.cv,newx = X.test,type=\"response\")),\n                                ridge=as.vector(predict(ridge.cv,newx = X.test,type=\"response\")),\n                                en=as.vector(predict(en.cv,newx = X.test,type=\"response\")))\n\n\nlibrary(pROC)\nroc.ad &lt;- roc(obs~logit+lasso+ridge+en,data=score)\n\ncouleur &lt;- c(\"black\",\"red\",\"blue\",\"green\")\nmapply(plot,roc.ad,col=couleur,lty=1:4,add=c(F,T,T,T),lwd=2,legacy.axes=TRUE)\n\n                   logit       lasso       ridge       en         \npercent            FALSE       FALSE       FALSE       FALSE      \nsensitivities      numeric,329 numeric,276 numeric,515 numeric,266\nspecificities      numeric,329 numeric,276 numeric,515 numeric,266\nthresholds         numeric,329 numeric,276 numeric,515 numeric,266\ndirection          \"&lt;\"         \"&lt;\"         \"&lt;\"         \"&lt;\"        \ncases              numeric,461 numeric,461 numeric,461 numeric,461\ncontrols           numeric,98  numeric,98  numeric,98  numeric,98 \nfun.sesp           ?           ?           ?           ?          \nauc                0.8635619   0.9695648   0.9805879   0.969587   \ncall               expression  expression  expression  expression \noriginal.predictor numeric,559 numeric,559 numeric,559 numeric,559\noriginal.response  factor,559  factor,559  factor,559  factor,559 \npredictor          numeric,559 numeric,559 numeric,559 numeric,559\nresponse           factor,559  factor,559  factor,559  factor,559 \nlevels             character,2 character,2 character,2 character,2\npredictor.name     \"logit\"     \"lasso\"     \"ridge\"     \"en\"       \nresponse.name      \"obs\"       \"obs\"       \"obs\"       \"obs\"      \n\nlegend(\"bottomright\",legend=c(\"logit\",\"lasso\",\"ridge\",\"elastic net\"),col=couleur,lty=1:4,lwd=2,cex=0.65)\n\n\n\n\n\nsort(round(unlist(lapply(roc.ad,auc)),3),decreasing=TRUE)\n\nridge lasso    en logit \n0.981 0.970 0.970 0.864 \n\n\n\nprev1 &lt;- data.frame(apply(round(score[,-1]),2,factor,labels=c(\"ad.\",\"nonad.\")))\nerr &lt;- apply(sweep(prev1,1,dtest$Y,FUN=\"!=\"),2,mean)\nsort(round(err,3))\n\nridge lasso    en logit \n0.030 0.034 0.036 0.077"
  },
  {
    "objectID": "code/chap17.html",
    "href": "code/chap17.html",
    "title": "17 Estimateurs à noyau et \\(k\\) plus proches voisins",
    "section": "",
    "text": "Estimateurs à noyau\n\nozone &lt;- read.table(\"../donnees/ozone.txt\",header=TRUE,sep=\";\")\n\n\nind &lt;- order(ozone[,\"T12\"])\nT12o &lt;- ozone[ind,\"T12\"]\nO3o &lt;- ozone[ind,\"O3\"]\n\n\nreg1 &lt;- lm(O3o~1,weight=c(rep(1,10),rep(0,40)))\nreg2 &lt;- lm(O3o~1,weight=c(rep(0,10),rep(1,10),rep(0,30)))\nreg3 &lt;- lm(O3o~1,weight=c(rep(0,20),rep(1,10),rep(0,20)))\nreg4 &lt;- lm(O3o~1,weight=c(rep(0,30),rep(1,10),rep(0,10)))\nreg5 &lt;- lm(O3o~1,weight=c(rep(0,40),rep(1,10)))\n\n\nplot(T12o,O3o,pch=20,xlab=\"T12\",ylab=\"O3\")\nabline(v=c(14,18),col=\"red\",lwd=2)\nabline(v=c(16),col=\"blue\",lty=2)\npoints(16,mean(O3o[T12o&gt;=14 & T12o&lt;=18]),col=\"blue\",pch=17,cex=1.5)\n\n\n\n\n\nlibrary(ibr)\nx &lt;- seq(7,30,by=0.01)\npar(mfrow=c(1,3))\nh &lt;- c(20,3,0.05)\nfor (i in h){\n  plot(T12o,O3o,pch=20,xlab=\"T12\",ylab=\"O3\")\n  tmp &lt;- npregress(T12o,O3o,bandwidth = i)\n  prev &lt;- predict(tmp,newdata=x)\n  lines(x,prev,col=\"blue\",lwd=2)\n}\n\n\n\n\n\n\nLes \\(k\\) plus proches voisins\n\npar(mfrow=c(1,3))\nlibrary(FNN)\nk &lt;- c(50,10,1)\nfor (i in k){\n  mod &lt;- knn.reg(train=T12o,test=as.matrix(x),y=O3o,k=i)\n  plot(T12o,O3o,pch=20,xlab=\"T12\",ylab=\"O3\")\n  lines(x,mod$pred,col=\"blue\",lwd=2)\n}\n\n\n\n\n\n\nSélection des paramètres\n\nhcv &lt;- npregress(T12o,O3o)$bandwidth\nhcv\n\n[1] 1.688373\n\n\n\nknn.reg(train=T12o,y=O3o,k=10)$PRESS/length(T12o)\n\n[1] 287.6629\n\n\n\nK_cand &lt;- 1:49\nloo &lt;- rep(0,length(K_cand))\nfor (i in 1:length(K_cand)){\n  loo[i] &lt;- knn.reg(train=T12o,y=O3o,k=K_cand[i])$PRESS/length(T12o)\n}\nK_cand[which.min(loo)]\n\n[1] 8\n\n\n\nmod.kppv &lt;- knn.reg(train=T12o,test=as.matrix(x),y=O3o,k=8)\nmod.noyau &lt;- npregress(T12o,O3o,bandwidth = hcv)\nprev.noyau &lt;- predict(mod.noyau,newdata=x)\nplot(T12o,O3o,pch=20,xlab=\"T12\",ylab=\"O3\")\nlines(x,mod.kppv$pred,col=\"blue\",lwd=2)\nlines(x,prev.noyau,col=\"red\",lty=2,lwd=2)\n\n\n\n\n\nmod.noyau$df\n\n[1] 5.339428"
  }
]